{
    "docs": [
        {
            "location": "/", 
            "text": "MixedModels.jl Documentation\n\n\nFitting and examining mixed-effects models\n\n\n\n\nManual Outline\n\n\n\n\nFitting linear mixed-effects models\n\n\nA simple example\n\n\nMore substantial examples\n\n\n\n\n\n\nParametric bootstrap for linear mixed-effects models\n\n\nThe parametric bootstrap\n\n\nUsing the \nbootstrap!\n function\n\n\n\n\n\n\n\n\n\n\nLibrary Outline\n\n\n\n\nPublic Documentation\n\n\nMixedModels\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nMixedModels.FactorReTerm\n\n\nMixedModels.GeneralizedLinearMixedModel\n\n\nMixedModels.LinearMixedModel\n\n\nMixedModels.MatrixTerm\n\n\nMixedModels.OptSummary\n\n\nMixedModels.VarCorr\n\n\nBase.LinAlg.cond\n\n\nBase.std\n\n\nMixedModels.LaplaceDeviance\n\n\nMixedModels.bootstrap\n\n\nMixedModels.bootstrap!\n\n\nMixedModels.condVar\n\n\nMixedModels.fixef\n\n\nMixedModels.get\u03b8\n\n\nMixedModels.glmm\n\n\nMixedModels.lmm\n\n\nMixedModels.lmm\n\n\nMixedModels.lowerbd\n\n\nMixedModels.lowerbd\n\n\nMixedModels.objective\n\n\nMixedModels.pirls!\n\n\nMixedModels.pwrss\n\n\nMixedModels.ranef\n\n\nMixedModels.refit!\n\n\nMixedModels.sdest\n\n\nMixedModels.set\u03b8!\n\n\nMixedModels.simulate!\n\n\nMixedModels.updateL!\n\n\nMixedModels.varest\n\n\nStatsBase.fit!\n\n\nStatsBase.fit!\n\n\nStatsBase.vcov", 
            "title": "Home"
        }, 
        {
            "location": "/#mixedmodelsjl-documentation", 
            "text": "Fitting and examining mixed-effects models", 
            "title": "MixedModels.jl Documentation"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "Fitting linear mixed-effects models  A simple example  More substantial examples    Parametric bootstrap for linear mixed-effects models  The parametric bootstrap  Using the  bootstrap!  function", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/#library-outline", 
            "text": "Public Documentation  MixedModels", 
            "title": "Library Outline"
        }, 
        {
            "location": "/#index", 
            "text": "MixedModels.FactorReTerm  MixedModels.GeneralizedLinearMixedModel  MixedModels.LinearMixedModel  MixedModels.MatrixTerm  MixedModels.OptSummary  MixedModels.VarCorr  Base.LinAlg.cond  Base.std  MixedModels.LaplaceDeviance  MixedModels.bootstrap  MixedModels.bootstrap!  MixedModels.condVar  MixedModels.fixef  MixedModels.get\u03b8  MixedModels.glmm  MixedModels.lmm  MixedModels.lmm  MixedModels.lowerbd  MixedModels.lowerbd  MixedModels.objective  MixedModels.pirls!  MixedModels.pwrss  MixedModels.ranef  MixedModels.refit!  MixedModels.sdest  MixedModels.set\u03b8!  MixedModels.simulate!  MixedModels.updateL!  MixedModels.varest  StatsBase.fit!  StatsBase.fit!  StatsBase.vcov", 
            "title": "Index"
        }, 
        {
            "location": "/man/fitting/", 
            "text": "Fitting linear mixed-effects models\n\n\nThe \nlmm\n function is similar to the \nlmer\n function in the \nlme4\n package for \nR\n.  The first two arguments for in the \nR\n version are \nformula\n and \ndata\n.  The principle method for the \nJulia\n version takes these arguments.\n\n\n\n\nA simple example\n\n\nThe simplest example of a mixed-effects model that we use in the \nlme4 package for R\n is a model fit to the \nDyestuff\n data.\n\n\n str\n(\nDyestuff\n)\n\n\ndata.frame\n:\n   \n30\n obs. of  \n2\n variables\n:\n\n \n$\n Batch\n:\n Factor w\n/\n \n6\n levels \nA\n,\nB\n,\nC\n,\nD\n,\n..\n:\n \n1\n \n1\n \n1\n \n1\n \n1\n \n2\n \n2\n \n2\n \n2\n \n2\n \n...\n\n \n$\n Yield\n:\n num  \n1545\n \n1440\n \n1440\n \n1520\n \n1580\n \n...\n\n\n \n(\nfm1 \n-\n lmer\n(\nYield \n~\n \n1\n|\nBatch\n,\n Dyestuff\n,\n REML\n=\nFALSE\n))\n\nLinear mixed model fit by maximum likelihood \n[\nlmerMod\n]\n\nFormula\n:\n Yield \n~\n \n1\n \n|\n Batch\n   Data\n:\n Dyestuff\n\n      AIC       BIC    logLik  deviance\n \n333.3271\n  \n337.5307\n \n-163.6635\n  \n327.3271\n\n\nRandom effects\n:\n\n Groups   Name        Variance Std.Dev.\n Batch    \n(\nIntercept\n)\n \n1388\n     \n37.26\n\n Residual             \n2451\n     \n49.51\n\n Number of obs\n:\n \n30\n,\n groups\n:\n Batch\n,\n \n6\n\n\nFixed effects\n:\n\n            Estimate Std. Error t value\n\n(\nIntercept\n)\n  \n1527.50\n      \n17.69\n   \n86.33\n\n\n\n\n\n\nThese \nDyestuff\n data are available through \nRCall\n but to run the doctests we use a stored copy of the dataframe.\n\n\njulia\n \nusing\n \nDataFrames\n,\n \nMixedModels\n\n\n\njulia\n \nds\n\n\n30\n\u00d72\n \nDataFrames\n.\nDataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \nYield\n  \n\u2502\n \nBatch\n \n\u2502\n\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n\n\u2502\n \n1\n   \n\u2502\n \n1545.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n1440.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n1440.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n1520.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n1580.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n1540.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n7\n   \n\u2502\n \n1555.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n8\n   \n\u2502\n \n1490.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u22ee\n\n\n\u2502\n \n22\n  \n\u2502\n \n1630.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n23\n  \n\u2502\n \n1515.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n24\n  \n\u2502\n \n1635.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n25\n  \n\u2502\n \n1625.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n26\n  \n\u2502\n \n1520.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n27\n  \n\u2502\n \n1455.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n28\n  \n\u2502\n \n1450.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n29\n  \n\u2502\n \n1480.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n30\n  \n\u2502\n \n1445.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\n\n\n\nlmm\n defaults to maximum likelihood estimation whereas \nlmer\n in \nR\n defaults to REML estimation.\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nFormula\n:\n \nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n)\n\n   \nlogLik\n    \n-\n2\n \nlogLik\n     \nAIC\n        \nBIC\n    \n  \n-\n163.66353\n  \n327.32706\n  \n333.32706\n  \n337.53065\n\n\n\nVariance\n \ncomponents\n:\n\n              \nColumn\n    \nVariance\n  \nStd\n.\nDev\n.\n\n \nBatch\n    \n(\nIntercept\n)\n  \n1388.3332\n \n37.260344\n\n \nResidual\n              \n2451.2500\n \n49.510100\n\n \nNumber\n \nof\n \nobs\n:\n \n30\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n6\n\n\n  \nFixed\n-\neffects\n \nparameters\n:\n\n             \nEstimate\n \nStd\n.\nError\n \nz\n \nvalue\n\n\n(\nIntercept\n)\n    \n1527.5\n   \n17.6946\n  \n86.326\n\n\n\n\n\n\nIn general the model should be fit through an explicit call to the \nfit!\n function, which may take a second argument indicating a verbose fit.\n\n\njulia\n \nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n),\n \ntrue\n);\n\n\nf_1\n:\n \n327.76702\n,\n \n[\n1.0\n]\n\n\nf_2\n:\n \n331.03619\n,\n \n[\n1.75\n]\n\n\nf_3\n:\n \n330.64583\n,\n \n[\n0.25\n]\n\n\nf_4\n:\n \n327.69511\n,\n \n[\n0.97619\n]\n\n\nf_5\n:\n \n327.56631\n,\n \n[\n0.928569\n]\n\n\nf_6\n:\n \n327.3826\n,\n \n[\n0.833327\n]\n\n\nf_7\n:\n \n327.35315\n,\n \n[\n0.807188\n]\n\n\nf_8\n:\n \n327.34663\n,\n \n[\n0.799688\n]\n\n\nf_9\n:\n \n327.341\n,\n \n[\n0.792188\n]\n\n\nf_10\n:\n \n327.33253\n,\n \n[\n0.777188\n]\n\n\nf_11\n:\n \n327.32733\n,\n \n[\n0.747188\n]\n\n\nf_12\n:\n \n327.32862\n,\n \n[\n0.739688\n]\n\n\nf_13\n:\n \n327.32706\n,\n \n[\n0.752777\n]\n\n\nf_14\n:\n \n327.32707\n,\n \n[\n0.753527\n]\n\n\nf_15\n:\n \n327.32706\n,\n \n[\n0.752584\n]\n\n\nf_16\n:\n \n327.32706\n,\n \n[\n0.752509\n]\n\n\nf_17\n:\n \n327.32706\n,\n \n[\n0.752591\n]\n\n\nf_18\n:\n \n327.32706\n,\n \n[\n0.752581\n]\n\n\nFTOL_REACHED\n\n\n\n\n\n\nThe numeric representation of the model has type\n\n\njulia\n \ntypeof\n(\nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n)))\n\n\nMixedModels\n.\nLinearMixedModel\n{\nFloat64\n}\n\n\n\n\n\n\nThose familiar with the \nlme4\n package for \nR\n will see the usual suspects.\n\n\njulia\n \nm\n \n=\n \nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n));\n\n\n\njulia\n \nfixef\n(\nm\n)\n  \n# estimates of the fixed-effects parameters\n\n\n1\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}\n:\n\n \n1527.5\n\n\n\njulia\n \ncoef\n(\nm\n)\n  \n# another name for fixef\n\n\n1\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}\n:\n\n \n1527.5\n\n\n\njulia\n \nranef\n(\nm\n)\n\n\n1\n-\nelement\n \nArray\n{\nArray\n{\nFloat64\n,\n2\n},\n1\n}\n:\n\n \n1\n\u00d76\n \nArray\n{\nFloat64\n,\n2\n}\n:\n\n \n-\n16.6282\n  \n0.369516\n  \n26.9747\n  \n-\n21.8014\n  \n53.5798\n  \n-\n42.4943\n\n\n\njulia\n \nranef\n(\nm\n,\n \ntrue\n)\n  \n# on the u scale\n\n\n1\n-\nelement\n \nArray\n{\nArray\n{\nFloat64\n,\n2\n},\n1\n}\n:\n\n \n1\n\u00d76\n \nArray\n{\nFloat64\n,\n2\n}\n:\n\n \n-\n22.0949\n  \n0.490999\n  \n35.8429\n  \n-\n28.9689\n  \n71.1948\n  \n-\n56.4648\n\n\n\njulia\n \ndeviance\n(\nm\n)\n\n\n327.3270598811394\n\n\n\njulia\n \nobjective\n(\nm\n)\n\n\n327.3270598811394\n\n\n\n\n\n\nWe prefer \nobjective\n to \ndeviance\n because the value returned is \n-2loglikelihood(m)\n, without the correction for the null deviance. It is not clear how the null deviance should be defined for these models.\n\n\n\n\nMore substantial examples\n\n\nFitting a model to the \nDyestuff\n data is trivial.  The \nInstEval\n data in the \nlme4\n package is more of a challenge in that there are nearly 75,000 evaluations by 2972 students on a total of 1128 instructors.\n\n\njulia\n \nhead\n(\ninst\n)\n\n\n6x7\n \nDataFrames\n.\nDataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \ns\n   \n\u2502\n \nd\n      \n\u2502\n \nstudage\n \n\u2502\n \nlectage\n \n\u2502\n \nservice\n \n\u2502\n \ndept\n \n\u2502\n \ny\n \n\u2502\n\n\n\u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2525\n\n\n\u2502\n \n1\n   \n\u2502\n \n1\n \n\u2502\n \n1002\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n0\n     \n\u2502\n \n2\n  \n\u2502\n \n5\n \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n1\n \n\u2502\n \n1050\n \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n1\n     \n\u2502\n \n6\n  \n\u2502\n \n2\n \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n1\n \n\u2502\n \n1582\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n0\n     \n\u2502\n \n2\n  \n\u2502\n \n5\n \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n1\n \n\u2502\n \n2050\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n3\n  \n\u2502\n \n3\n \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n2\n \n\u2502\n \n115\n  \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n0\n     \n\u2502\n \n5\n  \n\u2502\n \n2\n \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n2\n \n\u2502\n \n756\n  \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n0\n     \n\u2502\n \n5\n  \n\u2502\n \n4\n \n\u2502\n\n\n\njulia\n \nm2\n \n=\n \nfit\n!(\nlmm\n(\ny\n \n~\n \n1\n \n+\n \ndept\n*\nservice\n \n+\n \n(\n1\n|\ns\n)\n \n+\n \n(\n1\n|\nd\n),\n \ninst\n))\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nlogLik\n:\n \n-118792\n.\n776708\n,\n \ndeviance\n:\n \n237585\n.\n553415\n,\n \nAIC\n:\n \n237647\n.\n553415\n,\n \nBIC\n:\n \n237932\n.\n876339\n\n\n\nVariance\n \ncomponents\n:\n\n            \nVariance\n   \nStd\n.\nDev\n.\n\n \ns\n        \n0\n.\n105417971\n \n0\n.\n32468134\n\n \nd\n        \n0\n.\n258416394\n \n0\n.\n50834673\n\n \nResidual\n \n1\n.\n384727771\n \n1\n.\n17674456\n\n \nNumber\n \nof\n \nobs\n:\n \n73421\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n2972\n,\n \n1128\n\n\n  \nFixed-effects\n \nparameters\n:\n\n                           \nEstimate\n \nStd\n.\nError\n   \nz\n \nvalue\n\n\n(\nIntercept\n)\n                 \n3\n.\n22961\n  \n0\n.\n064053\n   \n50\n.\n4209\n\n\ndept\n \n-\n \n5\n                   \n0\n.\n129536\n  \n0\n.\n101294\n   \n1\n.\n27882\n\n\ndept\n \n-\n \n10\n                 \n-0\n.\n176751\n \n0\n.\n0881352\n  \n-2\n.\n00545\n\n\ndept\n \n-\n \n12\n                 \n0\n.\n0517102\n \n0\n.\n0817524\n  \n0\n.\n632522\n\n\ndept\n \n-\n \n6\n                  \n0\n.\n0347319\n  \n0\n.\n085621\n  \n0\n.\n405647\n\n\ndept\n \n-\n \n7\n                    \n0\n.\n14594\n \n0\n.\n0997984\n   \n1\n.\n46235\n\n\ndept\n \n-\n \n4\n                   \n0\n.\n151689\n \n0\n.\n0816897\n   \n1\n.\n85689\n\n\ndept\n \n-\n \n8\n                   \n0\n.\n104206\n  \n0\n.\n118751\n  \n0\n.\n877517\n\n\ndept\n \n-\n \n9\n                  \n0\n.\n0440401\n \n0\n.\n0962985\n  \n0\n.\n457329\n\n\ndept\n \n-\n \n14\n                 \n0\n.\n0517546\n \n0\n.\n0986029\n  \n0\n.\n524879\n\n\ndept\n \n-\n \n1\n                  \n0\n.\n0466719\n  \n0\n.\n101942\n  \n0\n.\n457828\n\n\ndept\n \n-\n \n3\n                  \n0\n.\n0563461\n \n0\n.\n0977925\n   \n0\n.\n57618\n\n\ndept\n \n-\n \n11\n                 \n0\n.\n0596536\n  \n0\n.\n100233\n   \n0\n.\n59515\n\n\ndept\n \n-\n \n2\n                 \n0\n.\n00556281\n  \n0\n.\n110867\n \n0\n.\n0501757\n\n\nservice\n \n-\n \n1\n                \n0\n.\n252025\n \n0\n.\n0686507\n   \n3\n.\n67112\n\n\ndept\n \n-\n \n5\n \n \nservice\n \n-\n \n1\n    \n-0\n.\n180757\n  \n0\n.\n123179\n  \n-1\n.\n46744\n\n\ndept\n \n-\n \n10\n \n \nservice\n \n-\n \n1\n   \n0\n.\n0186492\n  \n0\n.\n110017\n  \n0\n.\n169512\n\n\ndept\n \n-\n \n12\n \n \nservice\n \n-\n \n1\n   \n-0\n.\n282269\n \n0\n.\n0792937\n  \n-3\n.\n55979\n\n\ndept\n \n-\n \n6\n \n \nservice\n \n-\n \n1\n    \n-0\n.\n494464\n \n0\n.\n0790278\n  \n-6\n.\n25683\n\n\ndept\n \n-\n \n7\n \n \nservice\n \n-\n \n1\n    \n-0\n.\n392054\n  \n0\n.\n110313\n  \n-3\n.\n55403\n\n\ndept\n \n-\n \n4\n \n \nservice\n \n-\n \n1\n    \n-0\n.\n278547\n \n0\n.\n0823727\n  \n-3\n.\n38154\n\n\ndept\n \n-\n \n8\n \n \nservice\n \n-\n \n1\n    \n-0\n.\n189526\n  \n0\n.\n111449\n  \n-1\n.\n70056\n\n\ndept\n \n-\n \n9\n \n \nservice\n \n-\n \n1\n    \n-0\n.\n499868\n \n0\n.\n0885423\n  \n-5\n.\n64553\n\n\ndept\n \n-\n \n14\n \n \nservice\n \n-\n \n1\n   \n-0\n.\n497162\n \n0\n.\n0917162\n  \n-5\n.\n42065\n\n\ndept\n \n-\n \n1\n \n \nservice\n \n-\n \n1\n     \n-0\n.\n24042\n \n0\n.\n0982071\n   \n-2\n.\n4481\n\n\ndept\n \n-\n \n3\n \n \nservice\n \n-\n \n1\n    \n-0\n.\n223013\n \n0\n.\n0890548\n  \n-2\n.\n50422\n\n\ndept\n \n-\n \n11\n \n \nservice\n \n-\n \n1\n   \n-0\n.\n516997\n \n0\n.\n0809077\n  \n-6\n.\n38997\n\n\ndept\n \n-\n \n2\n \n \nservice\n \n-\n \n1\n    \n-0\n.\n384773\n  \n0\n.\n091843\n  \n-4\n.\n18946\n\n\n\n\n\n\nModels with vector-valued random effects can be fit\n\n\njulia\n \nslp\n\n\n180\n\u00d73\n \nDataFrames\n.\nDataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \nReaction\n \n\u2502\n \nDays\n \n\u2502\n \nSubject\n \n\u2502\n\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n\n\u2502\n \n1\n   \n\u2502\n \n249.56\n   \n\u2502\n \n0\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n258.705\n  \n\u2502\n \n1\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n250.801\n  \n\u2502\n \n2\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n321.44\n   \n\u2502\n \n3\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n356.852\n  \n\u2502\n \n4\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n414.69\n   \n\u2502\n \n5\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n7\n   \n\u2502\n \n382.204\n  \n\u2502\n \n6\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n8\n   \n\u2502\n \n290.149\n  \n\u2502\n \n7\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u22ee\n\n\n\u2502\n \n172\n \n\u2502\n \n273.474\n  \n\u2502\n \n1\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n173\n \n\u2502\n \n297.597\n  \n\u2502\n \n2\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n174\n \n\u2502\n \n310.632\n  \n\u2502\n \n3\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n175\n \n\u2502\n \n287.173\n  \n\u2502\n \n4\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n176\n \n\u2502\n \n329.608\n  \n\u2502\n \n5\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n177\n \n\u2502\n \n334.482\n  \n\u2502\n \n6\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n178\n \n\u2502\n \n343.22\n   \n\u2502\n \n7\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n179\n \n\u2502\n \n369.142\n  \n\u2502\n \n8\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n180\n \n\u2502\n \n364.124\n  \n\u2502\n \n9\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\njulia\n \nfm3\n \n=\n \nfit!\n(\nlmm\n(\nReaction\n \n~\n \n1\n \n+\n \nDays\n \n+\n \n(\n1\n+\nDays\n|\nSubject\n),\n \nslp\n))\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nFormula\n:\n \nReaction\n \n~\n \n1\n \n+\n \nDays\n \n+\n \n((\n1\n \n+\n \nDays\n)\n \n|\n \nSubject\n)\n\n  \nlogLik\n    \n-\n2\n \nlogLik\n     \nAIC\n        \nBIC\n\n \n-\n875.96967\n \n1751.93934\n \n1763.93934\n \n1783.09709\n\n\n\nVariance\n \ncomponents\n:\n\n              \nColumn\n    \nVariance\n  \nStd\n.\nDev\n.\n   \nCorr\n.\n\n \nSubject\n  \n(\nIntercept\n)\n  \n565.51066\n \n23.780468\n\n          \nDays\n          \n32.68212\n  \n5.716828\n  \n0.08\n\n \nResidual\n \nDays\n         \n654.94145\n \n25.591824\n\n \nNumber\n \nof\n \nobs\n:\n \n180\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n18\n\n\n  \nFixed\n-\neffects\n \nparameters\n:\n\n             \nEstimate\n \nStd\n.\nError\n \nz\n \nvalue\n \nP\n(\n|\nz\n|\n)\n\n\n(\nIntercept\n)\n   \n251.405\n   \n6.63226\n \n37.9064\n  \n1e-99\n\n\nDays\n          \n10.4673\n   \n1.50224\n \n6.96781\n  \n1e-11", 
            "title": "Fitting"
        }, 
        {
            "location": "/man/fitting/#fitting-linear-mixed-effects-models", 
            "text": "The  lmm  function is similar to the  lmer  function in the  lme4  package for  R .  The first two arguments for in the  R  version are  formula  and  data .  The principle method for the  Julia  version takes these arguments.", 
            "title": "Fitting linear mixed-effects models"
        }, 
        {
            "location": "/man/fitting/#a-simple-example", 
            "text": "The simplest example of a mixed-effects model that we use in the  lme4 package for R  is a model fit to the  Dyestuff  data.   str ( Dyestuff )  data.frame :     30  obs. of   2  variables : \n  $  Batch :  Factor w /   6  levels  A , B , C , D , .. :   1   1   1   1   1   2   2   2   2   2   ... \n  $  Yield :  num   1545   1440   1440   1520   1580   ...    ( fm1  -  lmer ( Yield  ~   1 | Batch ,  Dyestuff ,  REML = FALSE )) \nLinear mixed model fit by maximum likelihood  [ lmerMod ] \nFormula :  Yield  ~   1   |  Batch\n   Data :  Dyestuff\n\n      AIC       BIC    logLik  deviance\n  333.3271    337.5307   -163.6635    327.3271 \n\nRandom effects : \n Groups   Name        Variance Std.Dev.\n Batch     ( Intercept )   1388       37.26 \n Residual              2451       49.51 \n Number of obs :   30 ,  groups :  Batch ,   6 \n\nFixed effects : \n            Estimate Std. Error t value ( Intercept )    1527.50        17.69     86.33   These  Dyestuff  data are available through  RCall  but to run the doctests we use a stored copy of the dataframe.  julia   using   DataFrames ,   MixedModels  julia   ds  30 \u00d72   DataFrames . DataFrame  \u2502   Row   \u2502   Yield    \u2502   Batch   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   1     \u2502   1545.0   \u2502   A     \u2502  \u2502   2     \u2502   1440.0   \u2502   A     \u2502  \u2502   3     \u2502   1440.0   \u2502   A     \u2502  \u2502   4     \u2502   1520.0   \u2502   A     \u2502  \u2502   5     \u2502   1580.0   \u2502   A     \u2502  \u2502   6     \u2502   1540.0   \u2502   B     \u2502  \u2502   7     \u2502   1555.0   \u2502   B     \u2502  \u2502   8     \u2502   1490.0   \u2502   B     \u2502  \u22ee  \u2502   22    \u2502   1630.0   \u2502   E     \u2502  \u2502   23    \u2502   1515.0   \u2502   E     \u2502  \u2502   24    \u2502   1635.0   \u2502   E     \u2502  \u2502   25    \u2502   1625.0   \u2502   E     \u2502  \u2502   26    \u2502   1520.0   \u2502   F     \u2502  \u2502   27    \u2502   1455.0   \u2502   F     \u2502  \u2502   28    \u2502   1450.0   \u2502   F     \u2502  \u2502   29    \u2502   1480.0   \u2502   F     \u2502  \u2502   30    \u2502   1445.0   \u2502   F     \u2502   lmm  defaults to maximum likelihood estimation whereas  lmer  in  R  defaults to REML estimation.  Linear   mixed   model   fit   by   maximum   likelihood \n  Formula :   Yield   ~   1   +   ( 1   |   Batch ) \n    logLik      - 2   logLik       AIC          BIC     \n   - 163.66353    327.32706    333.32706    337.53065  Variance   components : \n               Column      Variance    Std . Dev . \n  Batch      ( Intercept )    1388.3332   37.260344 \n  Residual                2451.2500   49.510100 \n  Number   of   obs :   30 ;   levels   of   grouping   factors :   6 \n\n   Fixed - effects   parameters : \n              Estimate   Std . Error   z   value  ( Intercept )      1527.5     17.6946    86.326   In general the model should be fit through an explicit call to the  fit!  function, which may take a second argument indicating a verbose fit.  julia   fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds ),   true );  f_1 :   327.76702 ,   [ 1.0 ]  f_2 :   331.03619 ,   [ 1.75 ]  f_3 :   330.64583 ,   [ 0.25 ]  f_4 :   327.69511 ,   [ 0.97619 ]  f_5 :   327.56631 ,   [ 0.928569 ]  f_6 :   327.3826 ,   [ 0.833327 ]  f_7 :   327.35315 ,   [ 0.807188 ]  f_8 :   327.34663 ,   [ 0.799688 ]  f_9 :   327.341 ,   [ 0.792188 ]  f_10 :   327.33253 ,   [ 0.777188 ]  f_11 :   327.32733 ,   [ 0.747188 ]  f_12 :   327.32862 ,   [ 0.739688 ]  f_13 :   327.32706 ,   [ 0.752777 ]  f_14 :   327.32707 ,   [ 0.753527 ]  f_15 :   327.32706 ,   [ 0.752584 ]  f_16 :   327.32706 ,   [ 0.752509 ]  f_17 :   327.32706 ,   [ 0.752591 ]  f_18 :   327.32706 ,   [ 0.752581 ]  FTOL_REACHED   The numeric representation of the model has type  julia   typeof ( fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds )))  MixedModels . LinearMixedModel { Float64 }   Those familiar with the  lme4  package for  R  will see the usual suspects.  julia   m   =   fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds ));  julia   fixef ( m )    # estimates of the fixed-effects parameters  1 - element   Array { Float64 , 1 } : \n  1527.5  julia   coef ( m )    # another name for fixef  1 - element   Array { Float64 , 1 } : \n  1527.5  julia   ranef ( m )  1 - element   Array { Array { Float64 , 2 }, 1 } : \n  1 \u00d76   Array { Float64 , 2 } : \n  - 16.6282    0.369516    26.9747    - 21.8014    53.5798    - 42.4943  julia   ranef ( m ,   true )    # on the u scale  1 - element   Array { Array { Float64 , 2 }, 1 } : \n  1 \u00d76   Array { Float64 , 2 } : \n  - 22.0949    0.490999    35.8429    - 28.9689    71.1948    - 56.4648  julia   deviance ( m )  327.3270598811394  julia   objective ( m )  327.3270598811394   We prefer  objective  to  deviance  because the value returned is  -2loglikelihood(m) , without the correction for the null deviance. It is not clear how the null deviance should be defined for these models.", 
            "title": "A simple example"
        }, 
        {
            "location": "/man/fitting/#more-substantial-examples", 
            "text": "Fitting a model to the  Dyestuff  data is trivial.  The  InstEval  data in the  lme4  package is more of a challenge in that there are nearly 75,000 evaluations by 2972 students on a total of 1128 instructors.  julia   head ( inst )  6x7   DataFrames . DataFrame  \u2502   Row   \u2502   s     \u2502   d        \u2502   studage   \u2502   lectage   \u2502   service   \u2502   dept   \u2502   y   \u2502  \u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2525  \u2502   1     \u2502   1   \u2502   1002   \u2502   2       \u2502   2       \u2502   0       \u2502   2    \u2502   5   \u2502  \u2502   2     \u2502   1   \u2502   1050   \u2502   2       \u2502   1       \u2502   1       \u2502   6    \u2502   2   \u2502  \u2502   3     \u2502   1   \u2502   1582   \u2502   2       \u2502   2       \u2502   0       \u2502   2    \u2502   5   \u2502  \u2502   4     \u2502   1   \u2502   2050   \u2502   2       \u2502   2       \u2502   1       \u2502   3    \u2502   3   \u2502  \u2502   5     \u2502   2   \u2502   115    \u2502   2       \u2502   1       \u2502   0       \u2502   5    \u2502   2   \u2502  \u2502   6     \u2502   2   \u2502   756    \u2502   2       \u2502   1       \u2502   0       \u2502   5    \u2502   4   \u2502  julia   m2   =   fit !( lmm ( y   ~   1   +   dept * service   +   ( 1 | s )   +   ( 1 | d ),   inst ))  Linear   mixed   model   fit   by   maximum   likelihood \n  logLik :   -118792 . 776708 ,   deviance :   237585 . 553415 ,   AIC :   237647 . 553415 ,   BIC :   237932 . 876339  Variance   components : \n             Variance     Std . Dev . \n  s          0 . 105417971   0 . 32468134 \n  d          0 . 258416394   0 . 50834673 \n  Residual   1 . 384727771   1 . 17674456 \n  Number   of   obs :   73421 ;   levels   of   grouping   factors :   2972 ,   1128 \n\n   Fixed-effects   parameters : \n                            Estimate   Std . Error     z   value  ( Intercept )                   3 . 22961    0 . 064053     50 . 4209  dept   -   5                     0 . 129536    0 . 101294     1 . 27882  dept   -   10                   -0 . 176751   0 . 0881352    -2 . 00545  dept   -   12                   0 . 0517102   0 . 0817524    0 . 632522  dept   -   6                    0 . 0347319    0 . 085621    0 . 405647  dept   -   7                      0 . 14594   0 . 0997984     1 . 46235  dept   -   4                     0 . 151689   0 . 0816897     1 . 85689  dept   -   8                     0 . 104206    0 . 118751    0 . 877517  dept   -   9                    0 . 0440401   0 . 0962985    0 . 457329  dept   -   14                   0 . 0517546   0 . 0986029    0 . 524879  dept   -   1                    0 . 0466719    0 . 101942    0 . 457828  dept   -   3                    0 . 0563461   0 . 0977925     0 . 57618  dept   -   11                   0 . 0596536    0 . 100233     0 . 59515  dept   -   2                   0 . 00556281    0 . 110867   0 . 0501757  service   -   1                  0 . 252025   0 . 0686507     3 . 67112  dept   -   5     service   -   1      -0 . 180757    0 . 123179    -1 . 46744  dept   -   10     service   -   1     0 . 0186492    0 . 110017    0 . 169512  dept   -   12     service   -   1     -0 . 282269   0 . 0792937    -3 . 55979  dept   -   6     service   -   1      -0 . 494464   0 . 0790278    -6 . 25683  dept   -   7     service   -   1      -0 . 392054    0 . 110313    -3 . 55403  dept   -   4     service   -   1      -0 . 278547   0 . 0823727    -3 . 38154  dept   -   8     service   -   1      -0 . 189526    0 . 111449    -1 . 70056  dept   -   9     service   -   1      -0 . 499868   0 . 0885423    -5 . 64553  dept   -   14     service   -   1     -0 . 497162   0 . 0917162    -5 . 42065  dept   -   1     service   -   1       -0 . 24042   0 . 0982071     -2 . 4481  dept   -   3     service   -   1      -0 . 223013   0 . 0890548    -2 . 50422  dept   -   11     service   -   1     -0 . 516997   0 . 0809077    -6 . 38997  dept   -   2     service   -   1      -0 . 384773    0 . 091843    -4 . 18946   Models with vector-valued random effects can be fit  julia   slp  180 \u00d73   DataFrames . DataFrame  \u2502   Row   \u2502   Reaction   \u2502   Days   \u2502   Subject   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   1     \u2502   249.56     \u2502   0      \u2502   1         \u2502  \u2502   2     \u2502   258.705    \u2502   1      \u2502   1         \u2502  \u2502   3     \u2502   250.801    \u2502   2      \u2502   1         \u2502  \u2502   4     \u2502   321.44     \u2502   3      \u2502   1         \u2502  \u2502   5     \u2502   356.852    \u2502   4      \u2502   1         \u2502  \u2502   6     \u2502   414.69     \u2502   5      \u2502   1         \u2502  \u2502   7     \u2502   382.204    \u2502   6      \u2502   1         \u2502  \u2502   8     \u2502   290.149    \u2502   7      \u2502   1         \u2502  \u22ee  \u2502   172   \u2502   273.474    \u2502   1      \u2502   18        \u2502  \u2502   173   \u2502   297.597    \u2502   2      \u2502   18        \u2502  \u2502   174   \u2502   310.632    \u2502   3      \u2502   18        \u2502  \u2502   175   \u2502   287.173    \u2502   4      \u2502   18        \u2502  \u2502   176   \u2502   329.608    \u2502   5      \u2502   18        \u2502  \u2502   177   \u2502   334.482    \u2502   6      \u2502   18        \u2502  \u2502   178   \u2502   343.22     \u2502   7      \u2502   18        \u2502  \u2502   179   \u2502   369.142    \u2502   8      \u2502   18        \u2502  \u2502   180   \u2502   364.124    \u2502   9      \u2502   18        \u2502  julia   fm3   =   fit! ( lmm ( Reaction   ~   1   +   Days   +   ( 1 + Days | Subject ),   slp ))  Linear   mixed   model   fit   by   maximum   likelihood \n  Formula :   Reaction   ~   1   +   Days   +   (( 1   +   Days )   |   Subject ) \n   logLik      - 2   logLik       AIC          BIC \n  - 875.96967   1751.93934   1763.93934   1783.09709  Variance   components : \n               Column      Variance    Std . Dev .     Corr . \n  Subject    ( Intercept )    565.51066   23.780468 \n           Days            32.68212    5.716828    0.08 \n  Residual   Days           654.94145   25.591824 \n  Number   of   obs :   180 ;   levels   of   grouping   factors :   18 \n\n   Fixed - effects   parameters : \n              Estimate   Std . Error   z   value   P ( | z | )  ( Intercept )     251.405     6.63226   37.9064    1e-99  Days            10.4673     1.50224   6.96781    1e-11", 
            "title": "More substantial examples"
        }, 
        {
            "location": "/man/bootstrap/", 
            "text": "Parametric bootstrap for linear mixed-effects models\n\n\nJulia is well-suited to implementing bootstrapping and other simulation-based methods for statistical models. The \nbootstrap!\n function in the \nMixedModels package\n provides an efficient parametric bootstrap for linear mixed-effects models, assuming that the results of interest from each simulated response vector can be incorporated into a vector of floating-point values.\n\n\n\n\nThe parametric bootstrap\n\n\nBootstrapping\n is a family of procedures for generating sample values of a statistic, allowing for visualization of the distribution of the statistic or for inference from this sample of values.\n\n\nA \nparametric bootstrap\n is used with a parametric model, \nm\n, that has been fitted to data. The procedure is to simulate \nn\n response vectors from \nm\n using the estimated parameter values and refit \nm\n to these responses in turn, accumulating the statistics of interest at each iteration.\n\n\nThe parameters of a linear mixed-effects model as fit by the \nlmm\n function are the fixed-effects parameters, \n\u03b2\n, the standard deviation, \n\u03c3\n, of the per-observation noise, and the covariance parameter, \n\u03b8\n, that defines the variance-covariance matrices of the random effects.\n\n\nFor example, a simple linear mixed-effects model for the \nDyestuff\n data in the \nlme4\n package for \nR\n is fit by\n\n\njulia\n \nusing\n \nDataFrames\n,\n \nGadfly\n,\n \nMixedModels\n\n\n\n\n\n\njulia\n \nshow\n(\nds\n)\n   \n# Dyestuff data set30\u00d72 DataFrames.DataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \nYield\n  \n\u2502\n \nBatch\n \n\u2502\n\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n\n\u2502\n \n1\n   \n\u2502\n \n1545.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n1440.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n1440.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n1520.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n1580.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n1540.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n7\n   \n\u2502\n \n1555.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n8\n   \n\u2502\n \n1490.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n9\n   \n\u2502\n \n1560.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n10\n  \n\u2502\n \n1495.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n11\n  \n\u2502\n \n1595.0\n \n\u2502\n \nC\n   \n\u2502\n\n\n\u2502\n \n12\n  \n\u2502\n \n1550.0\n \n\u2502\n \nC\n   \n\u2502\n\n\n\u2502\n \n13\n  \n\u2502\n \n1605.0\n \n\u2502\n \nC\n   \n\u2502\n\n\n\u2502\n \n14\n  \n\u2502\n \n1510.0\n \n\u2502\n \nC\n   \n\u2502\n\n\n\u2502\n \n15\n  \n\u2502\n \n1560.0\n \n\u2502\n \nC\n   \n\u2502\n\n\n\u2502\n \n16\n  \n\u2502\n \n1445.0\n \n\u2502\n \nD\n   \n\u2502\n\n\n\u2502\n \n17\n  \n\u2502\n \n1440.0\n \n\u2502\n \nD\n   \n\u2502\n\n\n\u2502\n \n18\n  \n\u2502\n \n1595.0\n \n\u2502\n \nD\n   \n\u2502\n\n\n\u2502\n \n19\n  \n\u2502\n \n1465.0\n \n\u2502\n \nD\n   \n\u2502\n\n\n\u2502\n \n20\n  \n\u2502\n \n1545.0\n \n\u2502\n \nD\n   \n\u2502\n\n\n\u2502\n \n21\n  \n\u2502\n \n1595.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n22\n  \n\u2502\n \n1630.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n23\n  \n\u2502\n \n1515.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n24\n  \n\u2502\n \n1635.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n25\n  \n\u2502\n \n1625.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n26\n  \n\u2502\n \n1520.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n27\n  \n\u2502\n \n1455.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n28\n  \n\u2502\n \n1450.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n29\n  \n\u2502\n \n1480.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n30\n  \n\u2502\n \n1445.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\n\n\n\njulia\n \nm1\n \n=\n \nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n))\n  \n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nFormula\n:\n \nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n)\n\n  \nlogLik\n    \n-\n2\n \nlogLik\n     \nAIC\n        \nBIC\n    \n \n-\n163.66353\n  \n327.32706\n  \n333.32706\n  \n337.53065\n\n\n\nVariance\n \ncomponents\n:\n\n              \nColumn\n    \nVariance\n  \nStd\n.\nDev\n.\n \n \nBatch\n    \n(\nIntercept\n)\n  \n1388.3334\n \n37.260346\n\n \nResidual\n              \n2451.2500\n \n49.510100\n\n \nNumber\n \nof\n \nobs\n:\n \n30\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n6\n\n\n  \nFixed\n-\neffects\n \nparameters\n:\n\n             \nEstimate\n \nStd\n.\nError\n \nz\n \nvalue\n \nP\n(\n|\nz\n|\n)\n\n\n(\nIntercept\n)\n    \n1527.5\n   \n17.6946\n  \n86.326\n  \n1e-99\n\n\n\n\n\n\n\n\nUsing the \nbootstrap!\n function\n\n\nThis quick explanation is provided for those who only wish to use the \nbootstrap!\n method and do not need detailed explanations of how it works. The three arguments to \nbootstrap!\n are the matrix that will be overwritten with the results, the model to bootstrap, and a function that overwrites a vector with the results of interest from the model.\n\n\nSuppose the objective is to obtain 100,000 parametric bootstrap samples of the estimates of the \"variance components\", \n\u03c3\u00b2\n and \n\u03c3\u2081\u00b2\n, in this model.  In many implementations of mixed-effects models the estimate of \n\u03c3\u2081\u00b2\n, the variance of the scalar random effects, is reported along with a standard error, as if the estimator could be assumed to have a Gaussian distribution. Is this a reasonable assumption?\n\n\nA suitable function to save the results is\n\n\njulia\n \nfunction\n \nsaveresults!\n(\nv\n,\n \nm\n)\n\n    \nv\n[\n1\n]\n \n=\n \nvarest\n(\nm\n)\n\n    \nv\n[\n2\n]\n \n=\n \nabs2\n(\nget\u03b8\n(\nm\n)[\n1\n])\n \n*\n \nv\n[\n1\n]\n\n\nend\n\n\nsaveresults!\n \n(\ngeneric\n \nfunction\n \nwith\n \n1\n \nmethod\n)\n\n\n\n\n\n\nThe \nvarest\n extractor function returns the estimate of \n\u03c3\u00b2\n.  As seen above, the estimate of the \n\u03c3\u2081\n is the product of \n\u0398\n and the estimate of \n\u03c3\n.  The expression \nabs2(get\u0398(m)[1])\n evaluates to \n\u0398\u00b2\n. The \n[1]\n is necessary because the value returned by \nget\u03b8\n is a vector and a scalar is needed here.\n\n\nAs with any simulation-based method, it is advisable to set the random number seed before calling \nbootstrap!\n for reproducibility.\n\n\njulia\n \nsrand\n(\n1234321\n);\n\n\nMersenneTwister\n(\nBase\n.\ndSFMT\n.\nDSFMT_state\n(\nInt32\n[\n-\n1066020669\n,\n1073631810\n,\n397127531\n,\n1072701603\n,\n-\n312796895\n,\n1073626997\n,\n1020815149\n,\n1073320576\n,\n650048908\n,\n1073512247\n  \n\u2026\n  \n-\n352178910\n,\n1073735534\n,\n1816227101\n,\n1072823316\n,\n-\n1468787611\n,\n-\n2121692099\n,\n358864500\n,\n-\n310934288\n,\n382\n,\n0\n]),[\n2.11393e-315\n,\n2.11394e-315\n,\n2.11394e-315\n,\n0.0\n,\nNaN\n,\n2.11399e-315\n,\n2.11332e-315\n,\n4.24399e-314\n,\n2.11278e-315\n,\n6.36599e-314\n  \n\u2026\n  \n3.95253e-323\n,\n5.06417e-321\n,\n0.0\n,\n0.0\n,\n2.11329e-315\n,\n0.0\n,\n7.90505e-323\n,\n5.06911e-321\n,\n0.0\n,\n0.0\n],\n382\n,\nUInt32\n[\n0x0012d591\n])\n\n\n\n\n\n\njulia\n \nresults\n \n=\n \nbootstrap!\n(\nzeros\n(\n2\n,\n \n100000\n),\n \nm1\n,\n \nsaveresults!\n);\n\n\n2\nx100000\n \nArray\n{\nFloat64\n,\n2\n}\n:\n\n \n4547.01\n   \n2302.38\n   \n2513.48\n   \n2832.77\n  \n2051.86\n   \n\u2026\n  \n2721.9\n    \n3735.86\n  \n1617.55\n  \n2624.33\n   \n1473.15\n\n  \n204.834\n   \n653.688\n   \n473.595\n  \n1685.59\n   \n367.881\n      \n564.686\n     \n0.0\n   \n1324.83\n   \n287.775\n  \n1826.86\n\n\n\n\n\n\nThe results for each bootstrap replication are stored in the columns of the matrix passed in as the first argument.  A density plot of the first row using the \nGadfly\n package is created as\n\n\nplot\n(\nx\n \n=\n \nsub\n(\nresults\n,\n \n1\n,\n \n:\n),\n \nGeom\n.\ndensity\n(),\n \nGuide\n.\nxlabel\n(\nParametric bootstrap estimates of \u03c3\u00b2\n))\n\n\n\n\n\n\n\n\n\n\nThe distribution of the bootstrap samples of \n\u03c3\u00b2\n is a bit skewed but not terribly so.  However, the distribution of the bootstrap samples of the estimate of \n\u03c3\u2081\u00b2\n is highly skewed and has a spike at zero.", 
            "title": "Bootstrap"
        }, 
        {
            "location": "/man/bootstrap/#parametric-bootstrap-for-linear-mixed-effects-models", 
            "text": "Julia is well-suited to implementing bootstrapping and other simulation-based methods for statistical models. The  bootstrap!  function in the  MixedModels package  provides an efficient parametric bootstrap for linear mixed-effects models, assuming that the results of interest from each simulated response vector can be incorporated into a vector of floating-point values.", 
            "title": "Parametric bootstrap for linear mixed-effects models"
        }, 
        {
            "location": "/man/bootstrap/#the-parametric-bootstrap", 
            "text": "Bootstrapping  is a family of procedures for generating sample values of a statistic, allowing for visualization of the distribution of the statistic or for inference from this sample of values.  A  parametric bootstrap  is used with a parametric model,  m , that has been fitted to data. The procedure is to simulate  n  response vectors from  m  using the estimated parameter values and refit  m  to these responses in turn, accumulating the statistics of interest at each iteration.  The parameters of a linear mixed-effects model as fit by the  lmm  function are the fixed-effects parameters,  \u03b2 , the standard deviation,  \u03c3 , of the per-observation noise, and the covariance parameter,  \u03b8 , that defines the variance-covariance matrices of the random effects.  For example, a simple linear mixed-effects model for the  Dyestuff  data in the  lme4  package for  R  is fit by  julia   using   DataFrames ,   Gadfly ,   MixedModels   julia   show ( ds )     # Dyestuff data set30\u00d72 DataFrames.DataFrame  \u2502   Row   \u2502   Yield    \u2502   Batch   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   1     \u2502   1545.0   \u2502   A     \u2502  \u2502   2     \u2502   1440.0   \u2502   A     \u2502  \u2502   3     \u2502   1440.0   \u2502   A     \u2502  \u2502   4     \u2502   1520.0   \u2502   A     \u2502  \u2502   5     \u2502   1580.0   \u2502   A     \u2502  \u2502   6     \u2502   1540.0   \u2502   B     \u2502  \u2502   7     \u2502   1555.0   \u2502   B     \u2502  \u2502   8     \u2502   1490.0   \u2502   B     \u2502  \u2502   9     \u2502   1560.0   \u2502   B     \u2502  \u2502   10    \u2502   1495.0   \u2502   B     \u2502  \u2502   11    \u2502   1595.0   \u2502   C     \u2502  \u2502   12    \u2502   1550.0   \u2502   C     \u2502  \u2502   13    \u2502   1605.0   \u2502   C     \u2502  \u2502   14    \u2502   1510.0   \u2502   C     \u2502  \u2502   15    \u2502   1560.0   \u2502   C     \u2502  \u2502   16    \u2502   1445.0   \u2502   D     \u2502  \u2502   17    \u2502   1440.0   \u2502   D     \u2502  \u2502   18    \u2502   1595.0   \u2502   D     \u2502  \u2502   19    \u2502   1465.0   \u2502   D     \u2502  \u2502   20    \u2502   1545.0   \u2502   D     \u2502  \u2502   21    \u2502   1595.0   \u2502   E     \u2502  \u2502   22    \u2502   1630.0   \u2502   E     \u2502  \u2502   23    \u2502   1515.0   \u2502   E     \u2502  \u2502   24    \u2502   1635.0   \u2502   E     \u2502  \u2502   25    \u2502   1625.0   \u2502   E     \u2502  \u2502   26    \u2502   1520.0   \u2502   F     \u2502  \u2502   27    \u2502   1455.0   \u2502   F     \u2502  \u2502   28    \u2502   1450.0   \u2502   F     \u2502  \u2502   29    \u2502   1480.0   \u2502   F     \u2502  \u2502   30    \u2502   1445.0   \u2502   F     \u2502   julia   m1   =   fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds ))    Linear   mixed   model   fit   by   maximum   likelihood \n  Formula :   Yield   ~   1   +   ( 1   |   Batch ) \n   logLik      - 2   logLik       AIC          BIC     \n  - 163.66353    327.32706    333.32706    337.53065  Variance   components : \n               Column      Variance    Std . Dev .  \n  Batch      ( Intercept )    1388.3334   37.260346 \n  Residual                2451.2500   49.510100 \n  Number   of   obs :   30 ;   levels   of   grouping   factors :   6 \n\n   Fixed - effects   parameters : \n              Estimate   Std . Error   z   value   P ( | z | )  ( Intercept )      1527.5     17.6946    86.326    1e-99", 
            "title": "The parametric bootstrap"
        }, 
        {
            "location": "/man/bootstrap/#using-the-bootstrap-function", 
            "text": "This quick explanation is provided for those who only wish to use the  bootstrap!  method and do not need detailed explanations of how it works. The three arguments to  bootstrap!  are the matrix that will be overwritten with the results, the model to bootstrap, and a function that overwrites a vector with the results of interest from the model.  Suppose the objective is to obtain 100,000 parametric bootstrap samples of the estimates of the \"variance components\",  \u03c3\u00b2  and  \u03c3\u2081\u00b2 , in this model.  In many implementations of mixed-effects models the estimate of  \u03c3\u2081\u00b2 , the variance of the scalar random effects, is reported along with a standard error, as if the estimator could be assumed to have a Gaussian distribution. Is this a reasonable assumption?  A suitable function to save the results is  julia   function   saveresults! ( v ,   m ) \n     v [ 1 ]   =   varest ( m ) \n     v [ 2 ]   =   abs2 ( get\u03b8 ( m )[ 1 ])   *   v [ 1 ]  end  saveresults!   ( generic   function   with   1   method )   The  varest  extractor function returns the estimate of  \u03c3\u00b2 .  As seen above, the estimate of the  \u03c3\u2081  is the product of  \u0398  and the estimate of  \u03c3 .  The expression  abs2(get\u0398(m)[1])  evaluates to  \u0398\u00b2 . The  [1]  is necessary because the value returned by  get\u03b8  is a vector and a scalar is needed here.  As with any simulation-based method, it is advisable to set the random number seed before calling  bootstrap!  for reproducibility.  julia   srand ( 1234321 );  MersenneTwister ( Base . dSFMT . DSFMT_state ( Int32 [ - 1066020669 , 1073631810 , 397127531 , 1072701603 , - 312796895 , 1073626997 , 1020815149 , 1073320576 , 650048908 , 1073512247    \u2026    - 352178910 , 1073735534 , 1816227101 , 1072823316 , - 1468787611 , - 2121692099 , 358864500 , - 310934288 , 382 , 0 ]),[ 2.11393e-315 , 2.11394e-315 , 2.11394e-315 , 0.0 , NaN , 2.11399e-315 , 2.11332e-315 , 4.24399e-314 , 2.11278e-315 , 6.36599e-314    \u2026    3.95253e-323 , 5.06417e-321 , 0.0 , 0.0 , 2.11329e-315 , 0.0 , 7.90505e-323 , 5.06911e-321 , 0.0 , 0.0 ], 382 , UInt32 [ 0x0012d591 ])   julia   results   =   bootstrap! ( zeros ( 2 ,   100000 ),   m1 ,   saveresults! );  2 x100000   Array { Float64 , 2 } : \n  4547.01     2302.38     2513.48     2832.77    2051.86     \u2026    2721.9      3735.86    1617.55    2624.33     1473.15 \n   204.834     653.688     473.595    1685.59     367.881        564.686       0.0     1324.83     287.775    1826.86   The results for each bootstrap replication are stored in the columns of the matrix passed in as the first argument.  A density plot of the first row using the  Gadfly  package is created as  plot ( x   =   sub ( results ,   1 ,   : ),   Geom . density (),   Guide . xlabel ( Parametric bootstrap estimates of \u03c3\u00b2 ))     The distribution of the bootstrap samples of  \u03c3\u00b2  is a bit skewed but not terribly so.  However, the distribution of the bootstrap samples of the estimate of  \u03c3\u2081\u00b2  is highly skewed and has a spike at zero.", 
            "title": "Using the bootstrap! function"
        }, 
        {
            "location": "/lib/public/", 
            "text": "Public Documentation\n\n\nDocumentation for \nMixedModels.jl\n's public interface.\n\n\n\n\nMixedModels\n\n\n#\n\n\nMixedModels.FactorReTerm\n \n \nType\n.\n\n\nFactorReTerm\n\n\n\n\n\nRandom-effects term from a grouping factor, model matrix and block pattern\n\n\nMembers\n\n\n\n\nf\n: the grouping factor as an \nAbstractFactor\n\n\nz\n: the transposed raw random-effects model matrix\n\n\nwtz\n: a weighted copy of \nz\n\n\nfnm\n: the name of the grouping factor as a \nSymbol\n\n\ncnms\n: a \nVector\n of column names (row names after transposition) of \nz\n\n\nblks\n: a \nVector{Int}\n of block sizes within \n\u039b\n\n\n\u039b\n: the relative covariance factor\n\n\ninds\n: linear indices of \u03b8 elements in the relative covariance factor\n\n\n\n\nsource\n\n\n#\n\n\nMixedModels.GeneralizedLinearMixedModel\n \n \nType\n.\n\n\nGeneralizedLinearMixedModel\n\n\n\n\n\nGeneralized linear mixed-effects model representation\n\n\nMembers:\n\n\n\n\nLMM\n: a \nLinearMixedModel\n - the local approximation to the GLMM.\n\n\n\u03b2\n: the fixed-effects vector\n\n\n\u03b2\u2080\n: similar to \n\u03b2\n. Used in the PIRLS algorithm if step-halving is needed.\n\n\n\u03b8\n: covariance parameter vector\n\n\nb\n: similar to \nu\n, equivalent to \nbroadcast!(*, b, LMM.\u039b, u)\n\n\nu\n: a vector of matrices of random effects\n\n\nu\u2080\n: similar to \nu\n.  Used in the PIRLS algorithm if step-halving is needed.\n\n\nresp\n: a \nGlmResp\n object\n\n\n\u03b7\n: the linear predictor\n\n\nwt\n: vector of prior case weights, a value of \nT[]\n indicates equal weights.\n\n\n\n\nsource\n\n\n#\n\n\nMixedModels.LinearMixedModel\n \n \nType\n.\n\n\nLinearMixedModel\n\n\n\n\n\nLinear mixed-effects model representation\n\n\nMembers\n\n\n\n\nformula\n: the formula for the model\n\n\ntrms\n: a \nVector{AbstractTerm}\n representing the model.  The last element is the response.\n\n\nsqrtwts\n: vector of square roots of the case weights.  Allowed to be size 0\n\n\nA\n: an \nnt \u00d7 nt\n symmetric matrix of matrices representing \nhcat(Z,X,y)'hcat(Z,X,y)\n\n\nL\n: a \nnt \u00d7 nt\n matrix of matrices - the lower Cholesky factor of \n\u039b'A\u039b+I\n\n\noptsum\n: an \nOptSummary\n object\n\n\n\n\nsource\n\n\n#\n\n\nMixedModels.MatrixTerm\n \n \nType\n.\n\n\nMatrixTerm\n\n\n\n\n\nTerm with an explicit, constant matrix representation\n\n\nMembers\n\n\n\n\nx\n: matrix\n\n\nwtx\n: weighted matrix\n\n\ncnames\n: vector of column names\n\n\n\n\nsource\n\n\n#\n\n\nMixedModels.OptSummary\n \n \nType\n.\n\n\nOptSummary\n\n\n\n\n\nSummary of an \nNLopt\n optimization\n\n\nMembers\n\n\n\n\ninitial\n: a copy of the initial parameter values in the optimization\n\n\nlowerbd\n: lower bounds on the parameter values\n\n\nftol_rel\n: as in NLopt\n\n\nftol_abs\n: as in NLopt\n\n\nxtol_rel\n: as in NLopt\n\n\nxtol_abs\n: as in NLopt\n\n\ninitial_step\n: as in NLopt\n\n\nmaxfeval\n: as in NLopt\n\n\nfinal\n: a copy of the final parameter values from the optimization\n\n\nfmin\n: the final value of the objective\n\n\nfeval\n: the number of function evaluations\n\n\noptimizer\n: the name of the optimizer used, as a \nSymbol\n\n\nreturnvalue\n: the return value, as a \nSymbol\n\n\n\n\nsource\n\n\n#\n\n\nMixedModels.VarCorr\n \n \nType\n.\n\n\nVarCorr\n\n\n\n\n\nAn encapsulation of information on the fitted random-effects variance-covariance matrices.\n\n\nMembers\n\n\n\n\n\u03c3\n: a \nVector{Vector{T}}\n of unscaled standard deviations\n\n\n\u03c1\n: a \nVector{Matrix{T}}\n of correlation matrices\n\n\nfnms\n: a \nVector{Symbol}\n of grouping factor names\n\n\ncnms\n: a \nVector{Vector{String}}\n of column names\n\n\ns\n: the estimate of \u03c3, the standard deviation of the per-observation noise.  When there\n\n\n\n\nis no scaling factor this value is \nNaN\n\n\nThe main purpose of defining this type is to isolate the logic in the show method.\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.cond\n \n \nMethod\n.\n\n\ncond(m::MixedModel)\n\n\n\n\n\nReturns the vector of the condition numbers of the blocks of \nm.\u039b\n\n\nsource\n\n\n#\n\n\nBase.std\n \n \nMethod\n.\n\n\nstd{T}(m::MixedModel{T})\n\n\n\n\n\nThe estimated standard deviations of the variance components as a \nVector{Vector{T}}\n.\n\n\nsource\n\n\n#\n\n\nMixedModels.LaplaceDeviance\n \n \nMethod\n.\n\n\nLaplaceDeviance{T}(m::GeneralizedLinearMixedModel{T})::T\n\n\n\n\n\nReturn the Laplace approximation to the deviance of \nm\n.\n\n\nIf the distribution \nD\n does not have a scale parameter the Laplace approximation is defined as the squared length of the conditional modes, \nu\n, plus the determinant of \n\u039b'Z'Z\u039b + 1\n, plus the sum of the squared deviance residuals.\n\n\nsource\n\n\n#\n\n\nMixedModels.bootstrap!\n \n \nMethod\n.\n\n\nbootstrap\n!\n{\nT\n}\n(\nr\n::\nMatrix\n{\nT\n}\n,\n \nm\n::\nLinearMixedModel\n{\nT\n}\n,\n \nf\n!\n::\nFunction\n;\n\n    \n\u03b2\n=\nfixef\n(\nm\n),\n \n\u03c3\n=\nsdest\n(\nm\n),\n \n\u03b8\n=\nget\n\u03b8\n(\nm\n))\n\n\n\n\n\n\nOverwrite columns of \nr\n with the results of applying the mutating extractor \nf!\n to parametric bootstrap replications of model \nm\n.\n\n\nThe signature of \nf!\n should be     f!{T}(v::AbstractVector{T}, m::LinearMixedModel{T})\n\n\nNamed Arguments\n\n\n\u03b2::Vector{T}\n, \n\u03c3::T\n, and \n\u03b8::Vector{T}\n are the values of the parameters in \nm\n for simulation of the responses.\n\n\nsource\n\n\n#\n\n\nMixedModels.bootstrap\n \n \nMethod\n.\n\n\nbootstrap{T}(N, m::LinearMixedModel{T},\n    \u03b2::Vector{T}=fixef(m), \u03c3::T=sdest(m), \u03b8::Vector{T}=get\u03b8(m))\n\n\n\n\n\nPerform \nN\n parametric bootstrap replication fits of \nm\n, returning a data frame with column names \n:obj\n, the objective function at convergence, \n:\u03c3\n, the estimated standard deviation of the residuals, \n\u03b2\u1d62, i = 1,...,p\n, the fixed-effects coefficients, and \n\u03b8\u1d62, i = 1,...,k\n the covariance parameters.\n\n\nNamed Arguments\n\n\n\u03b2::Vector{T}\n, \n\u03c3::T\n, and \n\u03b8::Vector{T}\n are the values of the parameters in \nm\n for simulation of the responses.\n\n\nsource\n\n\n#\n\n\nMixedModels.condVar\n \n \nMethod\n.\n\n\ncondVar(m::MixedModel)\n\n\n\n\n\nReturn the conditional variances matrices of the random effects.\n\n\nThe random effects are returned by \nranef\n as a vector of length \nk\n, where \nk\n is the number of random effects terms.  The \ni\nth element is a matrix of size \nv\u1d62 \u00d7 \u2113\u1d62\n  where \nv\u1d62\n is the size of the vector-valued random effects for each of the \n\u2113\u1d62\n levels of the grouping factor.  Technically those values are the modes of the conditional distribution of the random effects given the observed data.\n\n\nThis function returns an array of \nk\n three dimensional arrays, where the \ni\nth array is of size \nv\u1d62 \u00d7 v\u1d62 \u00d7 \u2113\u1d62\n.  These are the diagonal blocks from the conditional variance-covariance matrix,\n\n\ns\u00b2 \u039b(\u039b\nZ\nZ\u039b + I)\u207b\u00b9\u039b\n\n\n\n\n\n\nsource\n\n\n#\n\n\nMixedModels.fixef\n \n \nMethod\n.\n\n\nfixef(m::MixedModel)\n\n\n\n\n\nReturns the fixed-effects parameter vector estimate.\n\n\nsource\n\n\n#\n\n\nMixedModels.get\u03b8\n \n \nFunction\n.\n\n\nget\u03b8(A::FactorReTerm)\n\n\n\n\n\nReturn a vector of the elements of the lower triangle blocks in \nA.\u039b\n (column-major ordering)\n\n\nsource\n\n\n#\n\n\nMixedModels.glmm\n \n \nMethod\n.\n\n\nglmm(f::Formula, fr::ModelFrame, d::Distribution[, l::GLM.Link])\n\n\n\n\n\nReturn a \nGeneralizedLinearMixedModel\n object.\n\n\nThe value is ready to be \nfit!\n but has not yet been fit.\n\n\nsource\n\n\n#\n\n\nMixedModels.lmm\n \n \nFunction\n.\n\n\nlmm(m::MixedModel)\n\n\n\n\n\nReturn the \nLinearMixedModel\n from a \nMixedModel\n.\n\n\nIf \nm\n is a \nLinearMixedModel\n return \nm\n. If \nm\n is a \nGeneralizedLinearMixedModel\n return \nm.LMM\n.\n\n\nsource\n\n\n#\n\n\nMixedModels.lmm\n \n \nMethod\n.\n\n\nlmm\n(\nf\n::\nDataFrames\n.\nFormula\n,\n \nfr\n::\nDataFrames\n.\nDataFrame\n;\n \nweights\n \n=\n \n[]\n)\n\n\n\n\n\n\nCreate a \nLinearMixedModel\n from \nf\n, which contains both fixed-effects terms and random effects, and \nfr\n.\n\n\nThe return value is ready to be \nfit!\n but has not yet been fit.\n\n\nsource\n\n\n#\n\n\nMixedModels.lowerbd\n \n \nFunction\n.\n\n\nlowerbd{T}(A::FactorReTerm{T})\nlowerbd{T}(A::MatrixTerm{T})\nlowerbd{T}(v::Vector{AbstractTerm{T}})\n\n\n\n\n\nReturn the vector of lower bounds on the parameters, \n\u03b8\n.\n\n\nThese are the elements in the lower triangle in column-major ordering. Diagonals have a lower bound of \n0\n.  Off-diagonals have a lower-bound of \n-Inf\n.\n\n\nsource\n\n\n#\n\n\nMixedModels.lowerbd\n \n \nMethod\n.\n\n\nlowerbd(m::LinearMixedModel)\n\n\n\n\n\nReturn the vector of lower bounds on the covariance parameter vector \n\u03b8\n\n\nsource\n\n\n#\n\n\nMixedModels.objective\n \n \nMethod\n.\n\n\nobjective(m::LinearMixedModel)\n\n\n\n\n\nReturn negative twice the log-likelihood of model \nm\n\n\nsource\n\n\n#\n\n\nMixedModels.pirls!\n \n \nFunction\n.\n\n\npirls!(m::GeneralizedLinearMixedModel)\n\n\n\n\n\nUse Penalized Iteratively Reweighted Least Squares (PIRLS) to determine the conditional modes of the random effects.\n\n\nWhen \nvary\u03b2\n is true both \nu\n and \n\u03b2\n are optimized with PIRLS.  Otherwise only \nu\n is optimized and \n\u03b2\n is held fixed.\n\n\nPassing \nverbose = true\n provides verbose output of the iterations.\n\n\nsource\n\n\n#\n\n\nMixedModels.pwrss\n \n \nMethod\n.\n\n\npwrss(m::LinearMixedModel)\n\n\n\n\n\nThe penalized residual sum-of-squares.\n\n\nsource\n\n\n#\n\n\nMixedModels.ranef\n \n \nMethod\n.\n\n\nranef{T}(m::MixedModel{T}, uscale=false)\n\n\n\n\n\nReturns, as a \nOrderedDict{}\n, the conditional modes of the random effects in model \nm\n.\n\n\nIf \nuscale\n is \ntrue\n the random effects are on the spherical (i.e. \nu\n) scale, otherwise on the original scale.\n\n\nsource\n\n\n#\n\n\nMixedModels.refit!\n \n \nMethod\n.\n\n\nrefit!{T}(m::LinearMixedModel{T}[, y::Vector{T}])\n\n\n\n\n\nRefit the model \nm\n after installing response \ny\n.\n\n\nIf \ny\n is omitted the current response vector is used.\n\n\nsource\n\n\n#\n\n\nMixedModels.sdest\n \n \nMethod\n.\n\n\nsdest(m::LinearMixedModel)\n\n\n\n\n\nReturn the estimate of \u03c3, the standard deviation of the per-observation noise.\n\n\nsource\n\n\n#\n\n\nMixedModels.set\u03b8!\n \n \nMethod\n.\n\n\nset\u03b8!{T}(m::LinearMixedModel{T}, v::Vector{T})\n\n\n\n\n\nInstall \nv\n as the \u03b8 parameters in \nm\n.\n\n\nsource\n\n\n#\n\n\nMixedModels.simulate!\n \n \nMethod\n.\n\n\nsimulate\n!(\nm\n::\nLinearMixedModel\n;\n \n\u03b2\n=\nfixef\n(\nm\n),\n \n\u03c3\n=\nsdest\n(\nm\n),\n \n\u03b8\n=\nget\n\u03b8\n(\nm\n))\n\n\n\n\n\n\nOverwrite the response (i.e. \nm.trms[end]\n) with a simulated response vector from model \nm\n.\n\n\nsource\n\n\n#\n\n\nMixedModels.updateL!\n \n \nMethod\n.\n\n\nupdateL!(m::LinearMixedModel)\n\n\n\n\n\nUpdate the blocked lower Cholesky factor, \nm.L\n, from \nm.A\n and \nm.trms\n (used for \u039b only)\n\n\nThis is the crucial step in evaluating the objective, given a new parameter value.\n\n\nsource\n\n\n#\n\n\nMixedModels.varest\n \n \nMethod\n.\n\n\nvarest(m::LinearMixedModel)\n\n\n\n\n\nReturns the estimate of \u03c3\u00b2, the variance of the conditional distribution of Y given B.\n\n\nsource\n\n\n#\n\n\nStatsBase.fit!\n \n \nFunction\n.\n\n\nfit!(m::LinearMixedModel[, verbose::Bool=false])\n\n\n\n\n\nOptimize the objective of a \nLinearMixedModel\n.  When \nverbose\n is \ntrue\n the values of the objective and the parameters are printed on STDOUT at each function evaluation.\n\n\nsource\n\n\n#\n\n\nStatsBase.fit!\n \n \nMethod\n.\n\n\nfit!(m::GeneralizedLinearMixedModel[, verbose = false, fast = false])\n\n\n\n\n\nOptimize the objective function for \nm\n.\n\n\nWhen \nfast\n is \ntrue\n a potentially much faster but slightly less accurate algorithm, in which \npirls!\n optimizes both the random effects and the fixed-effects parameters, is used.\n\n\nsource\n\n\n#\n\n\nStatsBase.vcov\n \n \nMethod\n.\n\n\n vcov(m::MixedModel)\n\n\n\n\n\nReturns the estimated covariance matrix of the fixed-effects estimator.\n\n\nsource", 
            "title": "Public"
        }, 
        {
            "location": "/lib/public/#public-documentation", 
            "text": "Documentation for  MixedModels.jl 's public interface.", 
            "title": "Public Documentation"
        }, 
        {
            "location": "/lib/public/#mixedmodels", 
            "text": "#  MixedModels.FactorReTerm     Type .  FactorReTerm  Random-effects term from a grouping factor, model matrix and block pattern  Members   f : the grouping factor as an  AbstractFactor  z : the transposed raw random-effects model matrix  wtz : a weighted copy of  z  fnm : the name of the grouping factor as a  Symbol  cnms : a  Vector  of column names (row names after transposition) of  z  blks : a  Vector{Int}  of block sizes within  \u039b  \u039b : the relative covariance factor  inds : linear indices of \u03b8 elements in the relative covariance factor   source  #  MixedModels.GeneralizedLinearMixedModel     Type .  GeneralizedLinearMixedModel  Generalized linear mixed-effects model representation  Members:   LMM : a  LinearMixedModel  - the local approximation to the GLMM.  \u03b2 : the fixed-effects vector  \u03b2\u2080 : similar to  \u03b2 . Used in the PIRLS algorithm if step-halving is needed.  \u03b8 : covariance parameter vector  b : similar to  u , equivalent to  broadcast!(*, b, LMM.\u039b, u)  u : a vector of matrices of random effects  u\u2080 : similar to  u .  Used in the PIRLS algorithm if step-halving is needed.  resp : a  GlmResp  object  \u03b7 : the linear predictor  wt : vector of prior case weights, a value of  T[]  indicates equal weights.   source  #  MixedModels.LinearMixedModel     Type .  LinearMixedModel  Linear mixed-effects model representation  Members   formula : the formula for the model  trms : a  Vector{AbstractTerm}  representing the model.  The last element is the response.  sqrtwts : vector of square roots of the case weights.  Allowed to be size 0  A : an  nt \u00d7 nt  symmetric matrix of matrices representing  hcat(Z,X,y)'hcat(Z,X,y)  L : a  nt \u00d7 nt  matrix of matrices - the lower Cholesky factor of  \u039b'A\u039b+I  optsum : an  OptSummary  object   source  #  MixedModels.MatrixTerm     Type .  MatrixTerm  Term with an explicit, constant matrix representation", 
            "title": "MixedModels"
        }, 
        {
            "location": "/lib/public/#members", 
            "text": "x : matrix  wtx : weighted matrix  cnames : vector of column names   source  #  MixedModels.OptSummary     Type .  OptSummary  Summary of an  NLopt  optimization  Members   initial : a copy of the initial parameter values in the optimization  lowerbd : lower bounds on the parameter values  ftol_rel : as in NLopt  ftol_abs : as in NLopt  xtol_rel : as in NLopt  xtol_abs : as in NLopt  initial_step : as in NLopt  maxfeval : as in NLopt  final : a copy of the final parameter values from the optimization  fmin : the final value of the objective  feval : the number of function evaluations  optimizer : the name of the optimizer used, as a  Symbol  returnvalue : the return value, as a  Symbol   source  #  MixedModels.VarCorr     Type .  VarCorr  An encapsulation of information on the fitted random-effects variance-covariance matrices.  Members   \u03c3 : a  Vector{Vector{T}}  of unscaled standard deviations  \u03c1 : a  Vector{Matrix{T}}  of correlation matrices  fnms : a  Vector{Symbol}  of grouping factor names  cnms : a  Vector{Vector{String}}  of column names  s : the estimate of \u03c3, the standard deviation of the per-observation noise.  When there   is no scaling factor this value is  NaN  The main purpose of defining this type is to isolate the logic in the show method.  source  #  Base.LinAlg.cond     Method .  cond(m::MixedModel)  Returns the vector of the condition numbers of the blocks of  m.\u039b  source  #  Base.std     Method .  std{T}(m::MixedModel{T})  The estimated standard deviations of the variance components as a  Vector{Vector{T}} .  source  #  MixedModels.LaplaceDeviance     Method .  LaplaceDeviance{T}(m::GeneralizedLinearMixedModel{T})::T  Return the Laplace approximation to the deviance of  m .  If the distribution  D  does not have a scale parameter the Laplace approximation is defined as the squared length of the conditional modes,  u , plus the determinant of  \u039b'Z'Z\u039b + 1 , plus the sum of the squared deviance residuals.  source  #  MixedModels.bootstrap!     Method .  bootstrap ! { T } ( r :: Matrix { T } ,   m :: LinearMixedModel { T } ,   f ! :: Function ; \n     \u03b2 = fixef ( m ),   \u03c3 = sdest ( m ),   \u03b8 = get \u03b8 ( m ))   Overwrite columns of  r  with the results of applying the mutating extractor  f!  to parametric bootstrap replications of model  m .  The signature of  f!  should be     f!{T}(v::AbstractVector{T}, m::LinearMixedModel{T})  Named Arguments  \u03b2::Vector{T} ,  \u03c3::T , and  \u03b8::Vector{T}  are the values of the parameters in  m  for simulation of the responses.  source  #  MixedModels.bootstrap     Method .  bootstrap{T}(N, m::LinearMixedModel{T},\n    \u03b2::Vector{T}=fixef(m), \u03c3::T=sdest(m), \u03b8::Vector{T}=get\u03b8(m))  Perform  N  parametric bootstrap replication fits of  m , returning a data frame with column names  :obj , the objective function at convergence,  :\u03c3 , the estimated standard deviation of the residuals,  \u03b2\u1d62, i = 1,...,p , the fixed-effects coefficients, and  \u03b8\u1d62, i = 1,...,k  the covariance parameters.  Named Arguments  \u03b2::Vector{T} ,  \u03c3::T , and  \u03b8::Vector{T}  are the values of the parameters in  m  for simulation of the responses.  source  #  MixedModels.condVar     Method .  condVar(m::MixedModel)  Return the conditional variances matrices of the random effects.  The random effects are returned by  ranef  as a vector of length  k , where  k  is the number of random effects terms.  The  i th element is a matrix of size  v\u1d62 \u00d7 \u2113\u1d62   where  v\u1d62  is the size of the vector-valued random effects for each of the  \u2113\u1d62  levels of the grouping factor.  Technically those values are the modes of the conditional distribution of the random effects given the observed data.  This function returns an array of  k  three dimensional arrays, where the  i th array is of size  v\u1d62 \u00d7 v\u1d62 \u00d7 \u2113\u1d62 .  These are the diagonal blocks from the conditional variance-covariance matrix,  s\u00b2 \u039b(\u039b Z Z\u039b + I)\u207b\u00b9\u039b   source  #  MixedModels.fixef     Method .  fixef(m::MixedModel)  Returns the fixed-effects parameter vector estimate.  source  #  MixedModels.get\u03b8     Function .  get\u03b8(A::FactorReTerm)  Return a vector of the elements of the lower triangle blocks in  A.\u039b  (column-major ordering)  source  #  MixedModels.glmm     Method .  glmm(f::Formula, fr::ModelFrame, d::Distribution[, l::GLM.Link])  Return a  GeneralizedLinearMixedModel  object.  The value is ready to be  fit!  but has not yet been fit.  source  #  MixedModels.lmm     Function .  lmm(m::MixedModel)  Return the  LinearMixedModel  from a  MixedModel .  If  m  is a  LinearMixedModel  return  m . If  m  is a  GeneralizedLinearMixedModel  return  m.LMM .  source  #  MixedModels.lmm     Method .  lmm ( f :: DataFrames . Formula ,   fr :: DataFrames . DataFrame ;   weights   =   [] )   Create a  LinearMixedModel  from  f , which contains both fixed-effects terms and random effects, and  fr .  The return value is ready to be  fit!  but has not yet been fit.  source  #  MixedModels.lowerbd     Function .  lowerbd{T}(A::FactorReTerm{T})\nlowerbd{T}(A::MatrixTerm{T})\nlowerbd{T}(v::Vector{AbstractTerm{T}})  Return the vector of lower bounds on the parameters,  \u03b8 .  These are the elements in the lower triangle in column-major ordering. Diagonals have a lower bound of  0 .  Off-diagonals have a lower-bound of  -Inf .  source  #  MixedModels.lowerbd     Method .  lowerbd(m::LinearMixedModel)  Return the vector of lower bounds on the covariance parameter vector  \u03b8  source  #  MixedModels.objective     Method .  objective(m::LinearMixedModel)  Return negative twice the log-likelihood of model  m  source  #  MixedModels.pirls!     Function .  pirls!(m::GeneralizedLinearMixedModel)  Use Penalized Iteratively Reweighted Least Squares (PIRLS) to determine the conditional modes of the random effects.  When  vary\u03b2  is true both  u  and  \u03b2  are optimized with PIRLS.  Otherwise only  u  is optimized and  \u03b2  is held fixed.  Passing  verbose = true  provides verbose output of the iterations.  source  #  MixedModels.pwrss     Method .  pwrss(m::LinearMixedModel)  The penalized residual sum-of-squares.  source  #  MixedModels.ranef     Method .  ranef{T}(m::MixedModel{T}, uscale=false)  Returns, as a  OrderedDict{} , the conditional modes of the random effects in model  m .  If  uscale  is  true  the random effects are on the spherical (i.e.  u ) scale, otherwise on the original scale.  source  #  MixedModels.refit!     Method .  refit!{T}(m::LinearMixedModel{T}[, y::Vector{T}])  Refit the model  m  after installing response  y .  If  y  is omitted the current response vector is used.  source  #  MixedModels.sdest     Method .  sdest(m::LinearMixedModel)  Return the estimate of \u03c3, the standard deviation of the per-observation noise.  source  #  MixedModels.set\u03b8!     Method .  set\u03b8!{T}(m::LinearMixedModel{T}, v::Vector{T})  Install  v  as the \u03b8 parameters in  m .  source  #  MixedModels.simulate!     Method .  simulate !( m :: LinearMixedModel ;   \u03b2 = fixef ( m ),   \u03c3 = sdest ( m ),   \u03b8 = get \u03b8 ( m ))   Overwrite the response (i.e.  m.trms[end] ) with a simulated response vector from model  m .  source  #  MixedModels.updateL!     Method .  updateL!(m::LinearMixedModel)  Update the blocked lower Cholesky factor,  m.L , from  m.A  and  m.trms  (used for \u039b only)  This is the crucial step in evaluating the objective, given a new parameter value.  source  #  MixedModels.varest     Method .  varest(m::LinearMixedModel)  Returns the estimate of \u03c3\u00b2, the variance of the conditional distribution of Y given B.  source  #  StatsBase.fit!     Function .  fit!(m::LinearMixedModel[, verbose::Bool=false])  Optimize the objective of a  LinearMixedModel .  When  verbose  is  true  the values of the objective and the parameters are printed on STDOUT at each function evaluation.  source  #  StatsBase.fit!     Method .  fit!(m::GeneralizedLinearMixedModel[, verbose = false, fast = false])  Optimize the objective function for  m .  When  fast  is  true  a potentially much faster but slightly less accurate algorithm, in which  pirls!  optimizes both the random effects and the fixed-effects parameters, is used.  source  #  StatsBase.vcov     Method .   vcov(m::MixedModel)  Returns the estimated covariance matrix of the fixed-effects estimator.  source", 
            "title": "Members"
        }
    ]
}