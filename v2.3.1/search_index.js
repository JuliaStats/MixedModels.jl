var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "MixedModels.jl Documentation",
    "title": "MixedModels.jl Documentation",
    "category": "page",
    "text": ""
},

{
    "location": "#MixedModels.jl-Documentation-1",
    "page": "MixedModels.jl Documentation",
    "title": "MixedModels.jl Documentation",
    "category": "section",
    "text": "CurrentModule = MixedModelsMixedModels.jl is a Julia package providing capabilities for fitting and examining linear and generalized linear mixed-effect models. It is similar in scope to the lme4 package for R.Pages = [\n        \"constructors.md\",\n        \"optimization.md\",\n        \"GaussHermite.md\",\n        \"bootstrap.md\",\n]\nDepth = 2"
},

{
    "location": "constructors/#",
    "page": "Model constructors",
    "title": "Model constructors",
    "category": "page",
    "text": ""
},

{
    "location": "constructors/#MixedModels.LinearMixedModel",
    "page": "Model constructors",
    "title": "MixedModels.LinearMixedModel",
    "category": "type",
    "text": "LinearMixedModel\n\nLinear mixed-effects model representation\n\nFields\n\nformula: the formula for the model\nallterms: a vector of random-effects terms, the fixed-effects terms and the response\nsqrtwts: vector of square roots of the case weights.  Can be empty.\nA: an nt × nt symmetric BlockMatrix of matrices representing hcat(Z,X,y)\'hcat(Z,X,y)\nL: a nt × nt BlockMatrix - the lower Cholesky factor of Λ\'AΛ+I\noptsum: an OptSummary object\n\nProperties\n\nθ or theta: the covariance parameter vector used to form λ\nβ or beta: the fixed-effects coefficient vector\nλ or lambda: a vector of lower triangular matrices repeated on the diagonal blocks of Λ\nσ or sigma: current value of the standard deviation of the per-observation noise\nb: random effects on the original scale, as a vector of matrices\nreterms: a Vector{ReMat{T}} of random-effects terms.\nfeterms: a Vector{FeMat{T}} of the fixed-effects model matrix and the response\nu: random effects on the orthogonal scale, as a vector of matrices\nlowerbd: lower bounds on the elements of θ\nX: the fixed-effects model matrix\ny: the response vector\n\n\n\n\n\n"
},

{
    "location": "constructors/#Model-constructors-1",
    "page": "Model constructors",
    "title": "Model constructors",
    "category": "section",
    "text": "The LinearMixedModel type represents a linear mixed-effects model. Typically it is constructed from a Formula and an appropriate data type, usually a DataFrame.LinearMixedModel"
},

{
    "location": "constructors/#Examples-of-linear-mixed-effects-model-fits-1",
    "page": "Model constructors",
    "title": "Examples of linear mixed-effects model fits",
    "category": "section",
    "text": "For illustration, several data sets from the lme4 package for R are made available in .rda format in this package. These include the Dyestuff and Dyestuff2 data sets.julia> using DataFrames, MixedModels, RData, StatsBase\n\njulia> datf = joinpath(dirname(pathof(MixedModels)),\"..\",\"test\",\"dat.rda\")\n\"/home/travis/build/JuliaStats/MixedModels.jl/src/../test/dat.rda\"\n\njulia> const dat = Dict(Symbol(k)=>v for (k,v) in load(datf))\nDict{Symbol,DataFrames.DataFrame} with 62 entries:\n  :bs10          => 1104×6 DataFrame…\n  :Genetics      => 60×5 DataFrame…\n  :Contraception => 1934×6 DataFrame…\n  :Mmmec         => 354×6 DataFrame…\n  :kb07          => 1790×10 DataFrame. Omitted printing of 3 columns…\n  :Rail          => 18×2 DataFrame…\n  :KKL           => 53765×24 DataFrame. Omitted printing of 16 columns…\n  :Bond          => 21×3 DataFrame…\n  :VerbAgg       => 7584×9 DataFrame. Omitted printing of 1 columns…\n  :ml1m          => 1000209×3 DataFrame…\n  :ergoStool     => 36×3 DataFrame…\n  :s3bbx         => 2449×6 DataFrame…\n  :cake          => 270×5 DataFrame…\n  :Cultivation   => 24×4 DataFrame…\n  :Pastes        => 60×4 DataFrame…\n  :Exam          => 4059×5 DataFrame…\n  :Socatt        => 1056×9 DataFrame. Omitted printing of 2 columns…\n  :WWheat        => 60×3 DataFrame…\n  :Pixel         => 102×5 DataFrame…\n  ⋮              => ⋮\n\njulia> describe(dat[:Dyestuff])\n2×8 DataFrame. Omitted printing of 1 columns\n│ Row │ variable │ mean   │ min    │ median │ max    │ nunique │ nmissing │\n│     │ Symbol   │ Union… │ Any    │ Union… │ Any    │ Union…  │ Nothing  │\n├─────┼──────────┼────────┼────────┼────────┼────────┼─────────┼──────────┤\n│ 1   │ G        │        │ A      │        │ F      │ 6       │          │\n│ 2   │ Y        │ 1527.5 │ 1440.0 │ 1530.0 │ 1635.0 │         │          │\nThe columns in these data sets have been renamed for convenience. The response is always named Y. Potential grouping factors for random-effects terms are named G, H, etc. Numeric covariates are named starting with U. Categorical covariates not suitable as grouping factors are named starting with A."
},

{
    "location": "constructors/#Models-with-simple,-scalar-random-effects-1",
    "page": "Model constructors",
    "title": "Models with simple, scalar random effects",
    "category": "section",
    "text": "The formula language in Julia is similar to that in R except that the formula must be enclosed in a call to the @formula macro. A basic model with simple, scalar random effects for the levels of G (the batch of an intermediate product, in this case) is declared and fit asjulia> fm1 = fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G)),\n    dat[:Dyestuff]))\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n            Column    Variance  Std.Dev. \nG        (Intercept)  1388.3333 37.260345\nResidual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)    1527.5    17.6946   86.326   <1e-99\n──────────────────────────────────────────────────\nAn alternative expression isjulia> fm1 = fit(MixedModel, @formula(Y ~ 1 + (1|G)), dat[:Dyestuff])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n            Column    Variance  Std.Dev. \nG        (Intercept)  1388.3333 37.260345\nResidual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)    1527.5    17.6946   86.326   <1e-99\n──────────────────────────────────────────────────\n(If you are new to Julia you may find that this first fit takes an unexpectedly long time, due to Just-In-Time (JIT) compilation of the code. The second and subsequent calls to such functions are much faster.)julia> @time fit(MixedModel, @formula(Y ~ 1 + (1|G)), dat[:Dyestuff2]);\n  0.466837 seconds (815.17 k allocations: 42.930 MiB)\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -81.436518 162.873037 168.873037 173.076629\n\nVariance components:\n            Column    Variance  Std.Dev. \nG        (Intercept)   0.000000 0.0000000\nResidual              13.346099 3.6532314\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)    5.6656   0.666986  8.49433   <1e-16\n──────────────────────────────────────────────────\nBy default, the model fit is by maximum likelihood.  To use the REML criterion instead, add the optional named argument REML = true to the call to fit!julia> fm1R = fit(MixedModel, @formula(Y ~ 1 + (1|G)),\n    dat[:Dyestuff], REML=true)\nLinear mixed model fit by REML\n Y ~ 1 + (1 | G)\n REML criterion at convergence: 319.6542768422538\n\nVariance components:\n            Column    Variance  Std.Dev. \nG        (Intercept)  1764.0506 42.000602\nResidual              2451.2499 49.510099\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)    1527.5    19.3834  78.8045   <1e-99\n──────────────────────────────────────────────────\n"
},

{
    "location": "constructors/#Simple,-scalar-random-effects-1",
    "page": "Model constructors",
    "title": "Simple, scalar random effects",
    "category": "section",
    "text": "A simple, scalar random effects term in a mixed-effects model formula is of the form (1|G). All random effects terms end with |G where G is the grouping factor for the random effect. The name or, more generally, the expression G should evaluate to a categorical array that has a distinct set of levels. The random effects are associated with the levels of the grouping factor.A scalar random effect is, as the name implies, one scalar value for each level of the grouping factor. A simple, scalar random effects term is of the form, (1|G). It corresponds to a shift in the intercept for each level of the grouping factor."
},

{
    "location": "constructors/#Models-with-vector-valued-random-effects-1",
    "page": "Model constructors",
    "title": "Models with vector-valued random effects",
    "category": "section",
    "text": "The sleepstudy data are observations of reaction time, Y, on several subjects, G, after 0 to 9 days of sleep deprivation, U. A model with random intercepts and random slopes for each subject, allowing for within-subject correlation of the slope and intercept, is fit asjulia> fm2 = fit(MixedModel, @formula(Y ~ 1+U + (1+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n            Column    Variance   Std.Dev.    Corr.\nG        (Intercept)  565.510678 23.7804684\n         U             32.682124  5.7168282  0.08\nResidual              654.941447 25.5918238\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.63226  37.9064    <1e-99\nU             10.4673    1.50224   6.96781   <1e-11\n───────────────────────────────────────────────────\n"
},

{
    "location": "constructors/#Models-with-multiple,-scalar-random-effects-terms-1",
    "page": "Model constructors",
    "title": "Models with multiple, scalar random-effects terms",
    "category": "section",
    "text": "A model for the Penicillin data incorporates random effects for the plate, G, and for the sample, H. As every sample is used on every plate these two factors are crossed.julia> fm4 = fit(MixedModel,@formula(Y ~ 1+(1|G)+(1|H)),dat[:Penicillin])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + (1 | G) + (1 | H)\n   logLik   -2 logLik     AIC        BIC    \n -166.09417  332.18835  340.18835  352.06760\n\nVariance components:\n            Column    Variance   Std.Dev. \nG        (Intercept)  0.71497950 0.8455646\nH        (Intercept)  3.13519287 1.7706476\nResidual              0.30242640 0.5499331\n Number of obs: 144; levels of grouping factors: 24, 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)   22.9722   0.744596  30.8519   <1e-99\n──────────────────────────────────────────────────\nIn contrast the sample, G, grouping factor is nested within the batch, H, grouping factor in the Pastes data. That is, each level of G occurs in conjunction with only one level of H.julia> fm5 = fit(MixedModel, @formula(Y ~ 1 + (1|G) + (1|H)), dat[:Pastes])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + (1 | G) + (1 | H)\n   logLik   -2 logLik     AIC        BIC    \n -123.99723  247.99447  255.99447  264.37184\n\nVariance components:\n            Column    Variance   Std.Dev.  \nG        (Intercept)  8.43361634 2.90406893\nH        (Intercept)  1.19918042 1.09507097\nResidual              0.67800208 0.82340882\n Number of obs: 60; levels of grouping factors: 30, 10\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)   60.0533   0.642136  93.5212   <1e-99\n──────────────────────────────────────────────────\nIn observational studies it is common to encounter partially crossed grouping factors. For example, the InstEval data are course evaluations by students, G, of instructors, H. Additional covariates include the academic department, I, in which the course was given and A, whether or not it was a service course.julia> fm6 = fit(MixedModel,@formula(Y ~ 1+ A*I +(1|G)+(1|H)),dat[:InstEval])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + A + I + A & I + (1 | G) + (1 | H)\n     logLik        -2 logLik          AIC             BIC       \n -1.18792777×10⁵  2.37585553×10⁵  2.37647553×10⁵  2.37932876×10⁵\n\nVariance components:\n            Column    Variance   Std.Dev.  \nG        (Intercept)  0.10541797 0.32468133\nH        (Intercept)  0.25841639 0.50834672\nResidual              1.38472777 1.17674457\n Number of obs: 73421; levels of grouping factors: 2972, 1128\n\n  Fixed-effects parameters:\n─────────────────────────────────────────────────────────\n                 Estimate  Std.Error     z value  P(>|z|)\n─────────────────────────────────────────────────────────\n(Intercept)    3.22961     0.064053   50.4209      <1e-99\nA: 1           0.252025    0.0686507   3.67112     0.0002\nI: 5           0.129536    0.101294    1.27882     0.2010\nI: 10         -0.176751    0.0881352  -2.00545     0.0449\nI: 12          0.0517102   0.0817524   0.632522    0.5270\nI: 6           0.0347319   0.085621    0.405647    0.6850\nI: 7           0.14594     0.0997984   1.46235     0.1436\nI: 4           0.151689    0.0816897   1.85689     0.0633\nI: 8           0.104206    0.118751    0.877517    0.3802\nI: 9           0.0440401   0.0962985   0.457329    0.6474\nI: 14          0.0517546   0.0986029   0.524879    0.5997\nI: 1           0.0466719   0.101942    0.457828    0.6471\nI: 3           0.0563461   0.0977925   0.57618     0.5645\nI: 11          0.0596536   0.100233    0.595151    0.5517\nI: 2           0.00556281  0.110867    0.0501757   0.9600\nA: 1 & I: 5   -0.180757    0.123179   -1.46744     0.1423\nA: 1 & I: 10   0.0186492   0.110017    0.169512    0.8654\nA: 1 & I: 12  -0.282269    0.0792937  -3.55979     0.0004\nA: 1 & I: 6   -0.494464    0.0790278  -6.25683     <1e-9\nA: 1 & I: 7   -0.392054    0.110313   -3.55403     0.0004\nA: 1 & I: 4   -0.278547    0.0823727  -3.38154     0.0007\nA: 1 & I: 8   -0.189526    0.111449   -1.70056     0.0890\nA: 1 & I: 9   -0.499868    0.0885423  -5.64553     <1e-7\nA: 1 & I: 14  -0.497162    0.0917162  -5.42065     <1e-7\nA: 1 & I: 1   -0.24042     0.0982071  -2.4481      0.0144\nA: 1 & I: 3   -0.223013    0.0890548  -2.50422     0.0123\nA: 1 & I: 11  -0.516997    0.0809077  -6.38997     <1e-9\nA: 1 & I: 2   -0.384773    0.091843   -4.18946     <1e-4\n─────────────────────────────────────────────────────────\n"
},

{
    "location": "constructors/#MixedModels.zerocorr!",
    "page": "Model constructors",
    "title": "MixedModels.zerocorr!",
    "category": "function",
    "text": "zerocorr!(m::LinearMixedModel[, trmnms::Vector{Symbol}])\n\nRewrite the random effects specification for the grouping factors in trmnms to zero correlation parameter.\n\nThe default for trmnms is all the names of random-effects terms.\n\nA random effects term is in the zero correlation parameter configuration when the off-diagonal elements of λ are all zero - hence there are no correlation parameters in that term being estimated.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.zerocorr",
    "page": "Model constructors",
    "title": "MixedModels.zerocorr",
    "category": "function",
    "text": "zerocorr(term::RandomEffectsTerm)\n\nRemove correlations between random effects in term.\n\n\n\n\n\n"
},

{
    "location": "constructors/#Simplifying-the-random-effect-correlation-structure-1",
    "page": "Model constructors",
    "title": "Simplifying the random effect correlation structure",
    "category": "section",
    "text": "MixedEffects.jl estimates not only the variance of the effects for each random effect level, but also the correlation between the random effects for different predictors. So, for the model of the sleepstudy data above, one of the parameters that is estimated is the correlation between each subject\'s random intercept (i.e., their baseline reaction time) and slope (i.e., their particular change in reaction time over days of sleep deprivation). In some cases, you may wish to simplify the random effects structure by removing these correlation parameters. This often arises when there are many random effects you want to estimate (as is common in psychological experiments with many conditions and covariates), since the number of random effects parameters increases as the square of the number of predictors, making these models difficult to estimate from limited data.A model with uncorrelated random effects for the intercept and slope by subject is fit asjulia> fm3 = fit!(zerocorr!(LinearMixedModel(@formula(Y ~ 1+U+(1+U|G)),\n    dat[:sleepstudy])))\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -876.00163 1752.00326 1762.00326 1777.96804\n\nVariance components:\n            Column    Variance  Std.Dev.   Corr.\nG        (Intercept)  584.258971 24.17145\n         U             33.632805  5.79938   .  \nResidual              653.115782 25.55613\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.70771  37.48      <1e-99\nU             10.4673    1.51931   6.88951   <1e-11\n───────────────────────────────────────────────────\nNote that the use of zerocorr! requires the model to be constructed, then altered to eliminate the correlation of the random effects, then fit with a call to the mutating function, fit!.zerocorr!The special syntax zerocorr can be applied to individual random effects terms inside the @formula:zerocorrjulia> fit(MixedModel, @formula(Y ~ 1 + U + zerocorr(1+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + MixedModels.ZeroCorr((1 + U | G))\n   logLik   -2 logLik     AIC        BIC    \n -876.00163 1752.00326 1762.00326 1777.96804\n\nVariance components:\n            Column    Variance  Std.Dev.   Corr.\nG        (Intercept)  584.258971 24.17145\n         U             33.632805  5.79938   .  \nResidual              653.115782 25.55613\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.70771  37.48      <1e-99\nU             10.4673    1.51931   6.88951   <1e-11\n───────────────────────────────────────────────────\nAlternatively, correlations between parameters can be removed by including them as separate random effects terms:julia> fit(MixedModel, @formula(Y ~ 1+U+(1|G)+(0+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 | G) + (0 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -876.00163 1752.00326 1762.00326 1777.96804\n\nVariance components:\n            Column    Variance  Std.Dev.   Corr.\nG        (Intercept)  584.258971 24.17145\n         U             33.632805  5.79938   .  \nResidual              653.115782 25.55613\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.70771  37.48      <1e-99\nU             10.4673    1.51931   6.88951   <1e-11\n───────────────────────────────────────────────────\nNote that it is necessary to explicitly block the inclusion of an intercept term by adding 0 in the random-effects term (0+U|G).Finally, for predictors that are categorical, MixedModels.jl will estimate correlations between each level. Notice the large number of correlation parameters if we treat U as a categorical variable by giving it contrasts:julia> fit(MixedModel, @formula(Y ~ 1+U+(1+U|G)), dat[:sleepstudy],\n    contrasts=Dict(:U=>DummyCoding()))\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n  -805.3996  1610.7992  1742.7992  1953.5344\n\nVariance components:\n            Column     Variance   Std.Dev.   Corr.\nG        (Intercept)   956.231148 30.922987\n         U: 1.0        496.640794 22.285439 -0.30\n         U: 2.0        914.755039 30.244918 -0.57  0.75\n         U: 3.0       1264.283956 35.556771 -0.37  0.72  0.87\n         U: 4.0       1480.122126 38.472355 -0.32  0.58  0.67  0.91\n         U: 5.0       2288.746783 47.840848 -0.25  0.46  0.45  0.70  0.85\n         U: 6.0       3842.541380 61.988236 -0.27  0.30  0.48  0.70  0.77  0.75\n         U: 7.0       1805.494419 42.491110 -0.16  0.22  0.47  0.50  0.62  0.64  0.71\n         U: 8.0       3147.273280 56.100564 -0.20  0.28  0.36  0.56  0.73  0.90  0.73  0.74\n         U: 9.0       3068.800196 55.396753  0.05  0.25  0.16  0.38  0.58  0.78  0.38  0.53  0.85\nResidual                18.991546  4.357929\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n────────────────────────────────────────────────────\n              Estimate  Std.Error   z value  P(>|z|)\n────────────────────────────────────────────────────\n(Intercept)  256.652      7.36064  34.8681    <1e-99\nU: 1.0         7.84395    5.44989   1.43929   0.1501\nU: 2.0         8.71009    7.27529   1.19722   0.2312\nU: 3.0        26.3402     8.50577   3.09674   0.0020\nU: 4.0        31.9976     9.18364   3.4842    0.0005\nU: 5.0        51.8667    11.3694    4.56196   <1e-5\nU: 6.0        55.5264    14.6828    3.78173   0.0002\nU: 7.0        62.0988    10.1201    6.13621   <1e-9\nU: 8.0        79.9777    13.3026    6.01219   <1e-8\nU: 9.0        94.1994    13.1377    7.17016   <1e-12\n────────────────────────────────────────────────────\nSeparating the 1 and U random effects into separate terms removes the correlations between the intercept and the levels of U, but not between the levels themselves:julia> fit(MixedModel, @formula(Y ~ 1+U+(1|G)+(0+U|G)), dat[:sleepstudy],\n    contrasts=Dict(:U=>DummyCoding()))\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 | G) + (0 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n  -805.3993  1610.7986  1744.7986  1958.7267\n\nVariance components:\n            Column     Variance    Std.Dev.    Corr.\nG        (Intercept)   254.8329728 15.9634887\n         U: 0.0        715.0961607 26.7412820   .  \n         U: 1.0        796.0939722 28.2151373   .    0.65\n         U: 2.0        561.6178331 23.6984774   .    0.26  0.69\n         U: 3.0       1167.4873418 34.1685139   .    0.32  0.68  0.85\n         U: 4.0       1449.4232150 38.0712912   .    0.32  0.58  0.63  0.90\n         U: 5.0       2270.1876345 47.6464861   .    0.26  0.45  0.40  0.69  0.84\n         U: 6.0       3508.7575522 59.2347664   .    0.11  0.22  0.39  0.65  0.73  0.72\n         U: 7.0       2106.4678189 45.8962724   .    0.40  0.38  0.52  0.54  0.65  0.65  0.68\n         U: 8.0       3160.4372900 56.2177667   .    0.23  0.31  0.32  0.55  0.72  0.89  0.71  0.74\n         U: 9.0       3973.1028107 63.0325536   .    0.47  0.51  0.36  0.53  0.70  0.83  0.42  0.63  0.87\nResidual                 4.5694002  2.1376155\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n────────────────────────────────────────────────────\n              Estimate  Std.Error   z value  P(>|z|)\n────────────────────────────────────────────────────\n(Intercept)  256.652      7.35791  34.8811    <1e-99\nU: 1.0         7.84395    5.45552   1.4378    0.1505\nU: 2.0         8.71009    7.28243   1.19604   0.2317\nU: 3.0        26.3402     8.52339   3.09035   0.0020\nU: 4.0        31.9976     9.20582   3.4758    0.0005\nU: 5.0        51.8667    11.3981    4.55046   <1e-5\nU: 6.0        55.5265    14.7068    3.77555   0.0002\nU: 7.0        62.0988    10.1208    6.13575   <1e-9\nU: 8.0        79.9777    13.3242    6.00244   <1e-8\nU: 9.0        94.1994    13.1547    7.1609    <1e-12\n────────────────────────────────────────────────────\nBut using zerocorr on the individual terms (or zerocorr! on the constructed model object as above) does remove the correlations between the levels:julia> fit(MixedModel, @formula(Y ~ 1+U+zerocorr(1+U|G)), dat[:sleepstudy],\n    contrasts=Dict(:U=>DummyCoding()))\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + MixedModels.ZeroCorr((1 + U | G))\n   logLik   -2 logLik     AIC        BIC    \n  -882.9138  1765.8276  1807.8276  1874.8797\n\nVariance components:\n            Column    Variance   Std.Dev.   Corr.\nG        (Intercept)   958.51186 30.959843\n         U: 1.0          0.00000  0.000000   .  \n         U: 2.0          0.00000  0.000000   .     .  \n         U: 3.0          0.00000  0.000000   .     .     .  \n         U: 4.0          0.00000  0.000000   .     .     .     .  \n         U: 5.0        519.82399 22.799649   .     .     .     .     .  \n         U: 6.0       1703.51659 41.273679   .     .     .     .     .     .  \n         U: 7.0        609.09627 24.679876   .     .     .     .     .     .     .  \n         U: 8.0       1273.52942 35.686544   .     .     .     .     .     .     .     .  \n         U: 9.0       1753.95967 41.880302   .     .     .     .     .     .     .     .     .  \nResidual               434.82413 20.852437\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n────────────────────────────────────────────────────\n              Estimate  Std.Error   z value  P(>|z|)\n────────────────────────────────────────────────────\n(Intercept)  256.652      8.79816  29.1711    <1e-99\nU: 1.0         7.84395    6.95081   1.12849   0.2591\nU: 2.0         8.71009    6.95081   1.2531    0.2102\nU: 3.0        26.3402     6.95081   3.78951   0.0002\nU: 4.0        31.9976     6.95081   4.60344   <1e-5\nU: 5.0        51.8666     8.78595   5.90336   <1e-8\nU: 6.0        55.5264    11.9563    4.64411   <1e-5\nU: 7.0        62.0988     9.0638    6.8513    <1e-11\nU: 8.0        79.9777    10.9117    7.32953   <1e-12\nU: 9.0        94.1994    12.0729    7.80252   <1e-14\n────────────────────────────────────────────────────\n\njulia> fit(MixedModel, @formula(Y ~ 1+U+(1|G)+zerocorr(0+U|G)),dat[:sleepstudy],\n    contrasts=Dict(:U=>DummyCoding()))\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 | G) + MixedModels.ZeroCorr((0 + U | G))\n   logLik   -2 logLik     AIC        BIC    \n  -878.9843  1757.9686  1801.9686  1872.2137\n\nVariance components:\n            Column      Variance    Std.Dev.    Corr.\nG        (Intercept)  1135.34423225 33.6948695\n         U: 0.0        776.23600556 27.8610123   .  \n         U: 1.0        358.01572563 18.9213035   .     .  \n         U: 2.0        221.49645908 14.8827571   .     .     .  \n         U: 3.0          0.38236102  0.6183535   .     .     .     .  \n         U: 4.0         44.71468158  6.6869037   .     .     .     .     .  \n         U: 5.0        670.50512881 25.8941138   .     .     .     .     .     .  \n         U: 6.0       1740.07339494 41.7141870   .     .     .     .     .     .     .  \n         U: 7.0        908.93184419 30.1484965   .     .     .     .     .     .     .     .  \n         U: 8.0       1458.06668413 38.1846394   .     .     .     .     .     .     .     .     .  \n         U: 9.0       2028.19601076 45.0354972   .     .     .     .     .     .     .     .     .     .  \nResidual               180.63063839 13.4398898\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n────────────────────────────────────────────────────\n              Estimate  Std.Error   z value  P(>|z|)\n────────────────────────────────────────────────────\n(Intercept)  256.652     10.7812   23.8055    <1e-99\nU: 1.0         7.84395    9.11505   0.86055   0.3895\nU: 2.0         8.71009    8.68905   1.00242   0.3161\nU: 3.0        26.3402     7.95082   3.31289   0.0009\nU: 4.0        31.9976     8.10422   3.94826   <1e-4\nU: 5.0        51.8666    10.0222    5.17517   <1e-6\nU: 6.0        55.5264    12.6438    4.3916    <1e-4\nU: 7.0        62.0988    10.6626    5.82399   <1e-8\nU: 8.0        79.9777    12.0082    6.66023   <1e-10\nU: 9.0        94.1994    13.2617    7.10313   <1e-11\n────────────────────────────────────────────────────\n"
},

{
    "location": "constructors/#MixedModels.GeneralizedLinearMixedModel",
    "page": "Model constructors",
    "title": "MixedModels.GeneralizedLinearMixedModel",
    "category": "type",
    "text": "GeneralizedLinearMixedModel\n\nGeneralized linear mixed-effects model representation\n\nFields\n\nLMM: a LinearMixedModel - the local approximation to the GLMM.\nβ: the pivoted and possibly truncated fixed-effects vector\nβ₀: similar to β. Used in the PIRLS algorithm if step-halving is needed.\nθ: covariance parameter vector\nb: similar to u, equivalent to broadcast!(*, b, LMM.Λ, u)\nu: a vector of matrices of random effects\nu₀: similar to u.  Used in the PIRLS algorithm if step-halving is needed.\nresp: a GlmResp object\nη: the linear predictor\nwt: vector of prior case weights, a value of T[] indicates equal weights.\n\nThe following fields are used in adaptive Gauss-Hermite quadrature, which applies only to models with a single random-effects term, in which case their lengths are the number of levels in the grouping factor for that term.  Otherwise they are zero-length vectors.\n\ndevc: vector of deviance components\ndevc0: vector of deviance components at offset of zero\nsd: approximate standard deviation of the conditional density\nmult: multiplier\n\nProperties\n\nIn addition to the fieldnames, the following names are also accessible through the . extractor\n\ntheta: synonym for θ\nbeta: synonym for β\nσ or sigma: common scale parameter (value is NaN for distributions without a scale parameter)\nlowerbd: vector of lower bounds on the combined elements of β and θ\nformula, trms, A, L, and optsum: fields of the LMM field\nX: fixed-effects model matrix\ny: response vector\n\n\n\n\n\n"
},

{
    "location": "constructors/#Fitting-generalized-linear-mixed-models-1",
    "page": "Model constructors",
    "title": "Fitting generalized linear mixed models",
    "category": "section",
    "text": "To create a GLMM representationGeneralizedLinearMixedModelthe distribution family for the response, and possibly the link function, must be specified.julia> verbaggform = @formula(r2 ~ 1+a+g+b+s+m+(1|id)+(1|item));\nFormulaTerm\nResponse:\n  r2(unknown)\nPredictors:\n  1\n  a(unknown)\n  g(unknown)\n  b(unknown)\n  s(unknown)\n  m(unknown)\n  (id)->1 | id\n  (item)->1 | item\n\njulia> gm1 = fit(MixedModel, verbaggform, dat[:VerbAgg], Bernoulli())\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + m + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8135.8329\n\nVariance components:\n        Column    Variance   Std.Dev. \nid   (Intercept)  1.64359543 1.2820279\nitem (Intercept)  0.10735333 0.3276482\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n──────────────────────────────────────────────────────\n               Estimate  Std.Error    z value  P(>|z|)\n──────────────────────────────────────────────────────\n(Intercept)   0.553417   0.368907     1.50015   0.1336\na             0.0574209  0.0160374    3.58043   0.0003\ng: M          0.320824   0.183043     1.75273   0.0796\nb: scold     -1.05983    0.176291    -6.0118    <1e-8\nb: shout     -2.10385    0.178549   -11.783     <1e-31\ns: self      -1.05431    0.144735    -7.2844    <1e-12\nm: do        -0.707033   0.144556    -4.89106   <1e-5\n──────────────────────────────────────────────────────\nThe canonical link, which is GLM.LogitLink for the Bernoulli distribution, is used if no explicit link is specified.Note that, in keeping with convention in the GLM package, the distribution family for a binary (i.e. 0/1) response is the Bernoulli distribution. The Binomial distribution is only used when the response is the fraction of trials returning a positive, in which case the number of trials must be specified as the case weights."
},

{
    "location": "constructors/#Optional-arguments-to-fit!-1",
    "page": "Model constructors",
    "title": "Optional arguments to fit!",
    "category": "section",
    "text": "An alternative approach is to create the GeneralizedLinearMixedModel object then call fit! on it. In this form optional arguments fast and/or nAGQ can be passed to the optimization process.As the name implies, fast=true, provides a faster but somewhat less accurate fit. These fits may suffice for model comparisons.julia> gm1a = fit!(GeneralizedLinearMixedModel(verbaggform, dat[:VerbAgg],\n    Bernoulli()), fast=true)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + m + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8136.1709\n\nVariance components:\n        Column     Variance   Std.Dev.  \nid   (Intercept)  1.636122425 1.27911001\nitem (Intercept)  0.108383392 0.32921633\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n──────────────────────────────────────────────────────\n               Estimate  Std.Error    z value  P(>|z|)\n──────────────────────────────────────────────────────\n(Intercept)   0.548543   0.368446     1.4888    0.1365\na             0.0543802  0.0159982    3.39915   0.0007\ng: M          0.304244   0.182603     1.66614   0.0957\nb: scold     -1.01749    0.176943    -5.75038   <1e-8\nb: shout     -2.02067    0.179146   -11.2795    <1e-28\ns: self      -1.01255    0.145248    -6.97114   <1e-11\nm: do        -0.679102   0.145074    -4.68108   <1e-5\n──────────────────────────────────────────────────────\n\njulia> deviance(gm1a) - deviance(gm1)\n0.3380158951767953\n\njulia> @time fit!(GeneralizedLinearMixedModel(verbaggform, dat[:VerbAgg],\n    Bernoulli()));\n  3.959293 seconds (315.81 k allocations: 22.019 MiB)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + m + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8135.8329\n\nVariance components:\n        Column    Variance   Std.Dev. \nid   (Intercept)  1.64359543 1.2820279\nitem (Intercept)  0.10735333 0.3276482\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n──────────────────────────────────────────────────────\n               Estimate  Std.Error    z value  P(>|z|)\n──────────────────────────────────────────────────────\n(Intercept)   0.553417   0.368907     1.50015   0.1336\na             0.0574209  0.0160374    3.58043   0.0003\ng: M          0.320824   0.183043     1.75273   0.0796\nb: scold     -1.05983    0.176291    -6.0118    <1e-8\nb: shout     -2.10385    0.178549   -11.783     <1e-31\ns: self      -1.05431    0.144735    -7.2844    <1e-12\nm: do        -0.707033   0.144556    -4.89106   <1e-5\n──────────────────────────────────────────────────────\n\njulia> @time fit!(GeneralizedLinearMixedModel(verbaggform, dat[:VerbAgg],\n    Bernoulli()), fast=true);\n  0.573215 seconds (49.02 k allocations: 9.615 MiB)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + m + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8136.1709\n\nVariance components:\n        Column     Variance   Std.Dev.  \nid   (Intercept)  1.636122425 1.27911001\nitem (Intercept)  0.108383392 0.32921633\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n──────────────────────────────────────────────────────\n               Estimate  Std.Error    z value  P(>|z|)\n──────────────────────────────────────────────────────\n(Intercept)   0.548543   0.368446     1.4888    0.1365\na             0.0543802  0.0159982    3.39915   0.0007\ng: M          0.304244   0.182603     1.66614   0.0957\nb: scold     -1.01749    0.176943    -5.75038   <1e-8\nb: shout     -2.02067    0.179146   -11.2795    <1e-28\ns: self      -1.01255    0.145248    -6.97114   <1e-11\nm: do        -0.679102   0.145074    -4.68108   <1e-5\n──────────────────────────────────────────────────────\nThe optional argument nAGQ=k causes evaluation of the deviance function to use a k point adaptive Gauss-Hermite quadrature rule. This method only applies to models with a single, simple, scalar random-effects term, such asjulia> contraform = @formula(use ~ 1+a+abs2(a)+l+urb+(1|d));\nFormulaTerm\nResponse:\n  use(unknown)\nPredictors:\n  1\n  a(unknown)\n  (a)->abs2(a)\n  l(unknown)\n  urb(unknown)\n  (d)->1 | d\n\njulia> @time gm2 = fit!(GeneralizedLinearMixedModel(contraform,\n    dat[:Contraception], Bernoulli()), nAGQ=9)\n  2.816569 seconds (4.04 M allocations: 212.625 MiB, 2.22% gc time)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 9)\n  use ~ 1 + a + :(abs2(a)) + l + urb + (1 | d)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 2372.4589\n\nVariance components:\n     Column    Variance  Std.Dev. \nd (Intercept)  0.2222174 0.4713994\n\n Number of obs: 1934; levels of grouping factors: 60\n\nFixed-effects parameters:\n─────────────────────────────────────────────────────────\n                Estimate    Std.Error    z value  P(>|z|)\n─────────────────────────────────────────────────────────\n(Intercept)  -1.03543     0.171933     -6.0223     <1e-8\na             0.00353312  0.00909425    0.388501   0.6976\nabs2(a)      -0.00456293  0.000714432  -6.3868     <1e-9\nl: 1          0.81519     0.159786      5.10175    <1e-6\nl: 2          0.916514    0.182361      5.02584    <1e-6\nl: 3+         0.915436    0.183022      5.00179    <1e-6\nurb: Y        0.6967      0.118142      5.89713    <1e-8\n─────────────────────────────────────────────────────────\n\njulia> @time deviance(fit!(GeneralizedLinearMixedModel(contraform,\n    dat[:Contraception], Bernoulli()), nAGQ=9, fast=true))\n  0.064895 seconds (21.82 k allocations: 2.447 MiB)\n2372.513592622964\n\njulia> @time deviance(fit!(GeneralizedLinearMixedModel(contraform,\n    dat[:Contraception], Bernoulli())))\n  0.266326 seconds (74.01 k allocations: 5.337 MiB)\n2372.7285823935636\n\njulia> @time deviance(fit!(GeneralizedLinearMixedModel(contraform,\n    dat[:Contraception], Bernoulli()), fast=true))\n  0.037536 seconds (11.26 k allocations: 1.814 MiB)\n2372.7844291358947\n"
},

{
    "location": "constructors/#Extractor-functions-1",
    "page": "Model constructors",
    "title": "Extractor functions",
    "category": "section",
    "text": "LinearMixedModel and GeneralizedLinearMixedModel are subtypes of StatsBase.RegressionModel which, in turn, is a subtype of StatsBase.StatisticalModel. Many of the generic extractors defined in the StatsBase package have methods for these models."
},

{
    "location": "constructors/#StatsBase.loglikelihood-Tuple{StatisticalModel}",
    "page": "Model constructors",
    "title": "StatsBase.loglikelihood",
    "category": "method",
    "text": "loglikelihood(obj::StatisticalModel)\n\nReturn the log-likelihood of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.aic",
    "page": "Model constructors",
    "title": "StatsBase.aic",
    "category": "function",
    "text": "aic(obj::StatisticalModel)\n\nAkaike\'s Information Criterion, defined as -2 log L + 2k, with L the likelihood of the model, and k its number of consumed degrees of freedom (as returned by dof).\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.bic",
    "page": "Model constructors",
    "title": "StatsBase.bic",
    "category": "function",
    "text": "bic(obj::StatisticalModel)\n\nBayesian Information Criterion, defined as -2 log L + k log n, with L the likelihood of the model,  k its number of consumed degrees of freedom (as returned by dof), and n the number of observations (as returned by nobs).\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.dof-Tuple{StatisticalModel}",
    "page": "Model constructors",
    "title": "StatsBase.dof",
    "category": "method",
    "text": "dof(obj::StatisticalModel)\n\nReturn the number of degrees of freedom consumed in the model, including when applicable the intercept and the distribution\'s dispersion parameter.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.nobs-Tuple{StatisticalModel}",
    "page": "Model constructors",
    "title": "StatsBase.nobs",
    "category": "method",
    "text": "nobs(obj::StatisticalModel)\n\nReturn the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.deviance-Tuple{StatisticalModel}",
    "page": "Model constructors",
    "title": "StatsBase.deviance",
    "category": "method",
    "text": "deviance(obj::StatisticalModel)\n\nReturn the deviance of the model relative to a reference, which is usually when applicable the saturated model. It is equal, up to a constant, to -2 log L, with L the likelihood of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.objective",
    "page": "Model constructors",
    "title": "MixedModels.objective",
    "category": "function",
    "text": "objective(m::LinearMixedModel)\n\nReturn negative twice the log-likelihood of model m\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.deviance!",
    "page": "Model constructors",
    "title": "MixedModels.deviance!",
    "category": "function",
    "text": "deviance!(m::GeneralizedLinearMixedModel, nAGQ=1)\n\nUpdate m.η, m.μ, etc., install the working response and working weights in m.LMM, update m.LMM.A and m.LMM.R, then evaluate the deviance.\n\n\n\n\n\n"
},

{
    "location": "constructors/#Model-fit-statistics-1",
    "page": "Model constructors",
    "title": "Model-fit statistics",
    "category": "section",
    "text": "The statistics describing the quality of the model fit includeloglikelihood(::StatisticalModel)\naic\nbic\ndof(::StatisticalModel)\nnobs(::StatisticalModel)julia> loglikelihood(fm1)\n-163.66352994057004\n\njulia> aic(fm1)\n333.3270598811401\n\njulia> bic(fm1)\n337.5306520261266\n\njulia> dof(fm1)   # 1 fixed effect, 2 variances\n3\n\njulia> nobs(fm1)  # 30 observations\n30\n\njulia> loglikelihood(gm1)\n-4067.9164279054075\nIn general the deviance of a statistical model fit is negative twice the log-likelihood adjusting for the saturated model.deviance(::StatisticalModel)Because it is not clear what the saturated model corresponding to a particular LinearMixedModel should be, negative twice the log-likelihood is called the objective.objectiveThis value is also accessible as the deviance but the user should bear in mind that this doesn\'t have all the properties of a deviance which is corrected for the saturated model. For example, it is not necessarily non-negative.julia> objective(fm1)\n327.3270598811401\n\njulia> deviance(fm1)\n327.3270598811401\nThe value optimized when fitting a GeneralizedLinearMixedModel is the Laplace approximation to the deviance or an adaptive Gauss-Hermite evaluation.MixedModels.deviance!julia> MixedModels.deviance!(gm1)\n8135.83285581081\n"
},

{
    "location": "constructors/#StatsBase.coef",
    "page": "Model constructors",
    "title": "StatsBase.coef",
    "category": "function",
    "text": "coef(obj::StatisticalModel)\n\nReturn the coefficients of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.fixef",
    "page": "Model constructors",
    "title": "MixedModels.fixef",
    "category": "function",
    "text": "fixef(m::MixedModel, permuted=true)\n\nReturn the fixed-effects parameter vector estimate of m.\n\nIf permuted is true the vector elements are permuted according to m.trms[end - 1].piv and truncated to the rank of that term.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.vcov",
    "page": "Model constructors",
    "title": "StatsBase.vcov",
    "category": "function",
    "text": "vcov(obj::StatisticalModel)\n\nReturn the variance-covariance matrix for the coefficients of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.stderror",
    "page": "Model constructors",
    "title": "StatsBase.stderror",
    "category": "function",
    "text": "stderror(obj::StatisticalModel)\n\nReturn the standard errors for the coefficients of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.coeftable",
    "page": "Model constructors",
    "title": "StatsBase.coeftable",
    "category": "function",
    "text": "coeftable(obj::StatisticalModel; level::Real=0.95)\n\nReturn a table of class CoefTable with coefficients and related statistics. level determines the level for confidence intervals (by default, 95%).\n\n\n\n\n\n"
},

{
    "location": "constructors/#Fixed-effects-parameter-estimates-1",
    "page": "Model constructors",
    "title": "Fixed-effects parameter estimates",
    "category": "section",
    "text": "The coef and fixef extractors both return the maximum likelihood estimates of the fixed-effects coefficients.coef\nfixefjulia> show(coef(fm1))\n[1527.4999999999993]\njulia> show(fixef(fm1))\n[1527.4999999999993]\njulia> show(fixef(gm1))\n[0.5534165116610926, 0.0574208860653126, 0.32082404652678687, -1.0598288085440888, -2.1038544589499235, -1.0543098270140103, -0.7070331511257679]An alternative extractor for the fixed-effects coefficient is the β property. Properties whose names are Greek letters usually have an alternative spelling, which is the name of the Greek letter.julia> show(fm1.β)\n[1527.4999999999993]\njulia> show(fm1.beta)\n[1527.4999999999993]\njulia> show(gm1.β)\n[0.5534165116610926, 0.0574208860653126, 0.32082404652678687, -1.0598288085440888, -2.1038544589499235, -1.0543098270140103, -0.7070331511257679]A full list of property names is returned by propertynamesjulia> propertynames(fm1)\n(:formula, :sqrtwts, :A, :L, :optsum, :θ, :theta, :β, :beta, :λ, :lambda, :stderror, :σ, :sigma, :σs, :sigmas, :b, :u, :lowerbd, :X, :y, :rePCA, :reterms, :feterms, :objective, :pvalues)\n\njulia> propertynames(gm1)\n(:A, :L, :theta, :beta, :coef, :λ, :lambda, :σ, :sigma, :X, :y, :lowerbd, :σρs, :σs, :LMM, :β, :β₀, :θ, :b, :u, :u₀, :resp, :η, :wt, :devc, :devc0, :sd, :mult)\nThe variance-covariance matrix of the fixed-effects coefficients is returned byvcovjulia> vcov(fm2)\n2×2 Array{Float64,2}:\n 43.9868   -1.37039\n -1.37039   2.25671\n\njulia> vcov(gm1)\n7×7 Array{Float64,2}:\n  0.136092    -0.00513622   -0.00895422   …  -0.0104972    -0.0104983\n -0.00513622   0.000257199   6.59023e-5      -1.35603e-5   -9.38557e-6\n -0.00895422   6.59023e-5    0.0335046       -7.37194e-5   -4.81936e-5\n -0.0155519   -1.31703e-5   -8.48265e-5       0.000243581   0.000157713\n -0.0157099   -2.6628e-5    -0.000148819      0.00060388    0.000477017\n -0.0104972   -1.35603e-5   -7.37194e-5   …   0.0209483     0.000227071\n -0.0104983   -9.38557e-6   -4.81936e-5       0.000227071   0.0208965\nThe standard errors are the square roots of the diagonal elements of the estimated variance-covariance matrix of the fixed-effects coefficient estimators.stderrorjulia> show(StatsBase.stderror(fm2))\n[6.632257775855945, 1.5022355309693658]\njulia> show(StatsBase.stderror(gm1))\n[0.3689071484215327, 0.016037436665456914, 0.18304259636914205, 0.1762914328649777, 0.1785494442913891, 0.14473534699876994, 0.14455627086638523]Finally, the coeftable generic produces a table of coefficient estimates, their standard errors, and their ratio. The p-values quoted here should be regarded as approximations.coeftablejulia> coeftable(fm2)\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.63226  37.9064    <1e-99\nU             10.4673    1.50224   6.96781   <1e-11\n───────────────────────────────────────────────────\n"
},

{
    "location": "constructors/#MixedModels.VarCorr",
    "page": "Model constructors",
    "title": "MixedModels.VarCorr",
    "category": "type",
    "text": "VarCorr\n\nInformation from the fitted random-effects variance-covariance matrices.\n\nMembers\n\nσρ: a NamedTuple of NamedTuples as returned from σρs\ns: the estimate of the scale parameter in the distribution of the conditional dist\'n of Y\n\nThe main purpose of defining this type is to isolate the logic in the show method.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.varest",
    "page": "Model constructors",
    "title": "MixedModels.varest",
    "category": "function",
    "text": "varest(m::LinearMixedModel)\n\nReturns the estimate of σ², the variance of the conditional distribution of Y given B.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.sdest",
    "page": "Model constructors",
    "title": "MixedModels.sdest",
    "category": "function",
    "text": "sdest(m::LinearMixedModel)\n\nReturn the estimate of σ, the standard deviation of the per-observation noise.\n\n\n\n\n\n"
},

{
    "location": "constructors/#Covariance-parameter-estimates-1",
    "page": "Model constructors",
    "title": "Covariance parameter estimates",
    "category": "section",
    "text": "The covariance parameters estimates, in the form shown in the model summary, are a VarCorr objectVarCorrjulia> VarCorr(fm2)\nVariance components:\n            Column    Variance   Std.Dev.    Corr.\nG        (Intercept)  565.510678 23.7804684\n         U             32.682124  5.7168282  0.08\nResidual              654.941447 25.5918238\n\n\njulia> VarCorr(gm1)\nVariance components:\n        Column    Variance   Std.Dev. \nid   (Intercept)  1.64359543 1.2820279\nitem (Intercept)  0.10735333 0.3276482\n\n\nIndividual components are returned by other extractorsvarest\nsdestjulia> varest(fm2)\n654.94144673283\n\njulia> sdest(fm2)\n25.591823825839963\n\njulia> fm2.σ\n25.591823825839963\n"
},

{
    "location": "constructors/#MixedModels.ranef",
    "page": "Model constructors",
    "title": "MixedModels.ranef",
    "category": "function",
    "text": "ranef(m::LinearMixedModel; uscale=false) #, named=true)\n\nReturn, as a Vector{Vector{T}} (Vector{NamedVector{T}} if named=true), the conditional modes of the random effects in model m.\n\nIf uscale is true the random effects are on the spherical (i.e. u) scale, otherwise on the original scale.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.condVar",
    "page": "Model constructors",
    "title": "MixedModels.condVar",
    "category": "function",
    "text": "condVar(m::LinearMixedModel)\n\nReturn the conditional variances matrices of the random effects.\n\nThe random effects are returned by ranef as a vector of length k, where k is the number of random effects terms.  The ith element is a matrix of size vᵢ × ℓᵢ  where vᵢ is the size of the vector-valued random effects for each of the ℓᵢ levels of the grouping factor.  Technically those values are the modes of the conditional distribution of the random effects given the observed data.\n\nThis function returns an array of k three dimensional arrays, where the ith array is of size vᵢ × vᵢ × ℓᵢ.  These are the diagonal blocks from the conditional variance-covariance matrix,\n\ns² Λ(Λ\'Z\'ZΛ + I)⁻¹Λ\'\n\n\n\n\n\n"
},

{
    "location": "constructors/#Conditional-modes-of-the-random-effects-1",
    "page": "Model constructors",
    "title": "Conditional modes of the random effects",
    "category": "section",
    "text": "The ranef extractorranefjulia> ranef(fm1)\n1-element Array{Array{Float64,2},1}:\n [-16.62822143006428 0.3695160317797815 … 53.57982460798647 -42.49434365460914]\n\njulia> fm1.b\n1-element Array{Array{Float64,2},1}:\n [-16.62822143006428 0.3695160317797815 … 53.57982460798647 -42.49434365460914]\nreturns the conditional modes of the random effects given the observed data. That is, these are the values that maximize the conditional density of the random effects given the observed data. For a LinearMixedModel these are also the conditional mean values.These are sometimes called the best linear unbiased predictors or BLUPs but that name is not particularly meaningful.At a superficial level these can be considered as the \"estimates\" of the random effects, with a bit of hand waving, but pursuing this analogy too far usually results in confusion.The corresponding conditional variances are returned bycondVarjulia> condVar(fm1)\n1-element Array{Array{Float64,3},1}:\n [362.3104715146578]\n\n[362.3104715146578]\n\n[362.3104715146578]\n\n[362.3104715146578]\n\n[362.3104715146578]\n\n[362.3104715146578]\n"
},

{
    "location": "optimization/#",
    "page": "Details of the parameter estimation",
    "title": "Details of the parameter estimation",
    "category": "page",
    "text": ""
},

{
    "location": "optimization/#Details-of-the-parameter-estimation-1",
    "page": "Details of the parameter estimation",
    "title": "Details of the parameter estimation",
    "category": "section",
    "text": ""
},

{
    "location": "optimization/#The-probability-model-1",
    "page": "Details of the parameter estimation",
    "title": "The probability model",
    "category": "section",
    "text": "Maximum likelihood estimates are based on the probability model for the observed responses. In the probability model the distribution of the responses is expressed as a function of one or more parameters.For a continuous distribution the probability density is a function of the responses, given the parameters. The likelihood function is the same expression as the probability density but regarding the observed values as fixed and the parameters as varying.In general a mixed-effects model incorporates two random variables: mathcalB, the q-dimensional vector of random effects, and mathcalY, the n-dimensional response vector. The value, bf y, of mathcalY is observed; the value, bf b, of mathcalB is not."
},

{
    "location": "optimization/#Linear-Mixed-Effects-Models-1",
    "page": "Details of the parameter estimation",
    "title": "Linear Mixed-Effects Models",
    "category": "section",
    "text": "In a linear mixed model the unconditional distribution of mathcalB and the conditional distribution, (mathcalY  mathcalB=bfb), are both multivariate Gaussian distributions,beginequation\nbeginaligned\n  (mathcalY  mathcalB=bfb) simmathcalN(bf Xbeta + Z bsigma^2bfI)\n  mathcalBsimmathcalN(bf0Sigma_theta) \nendaligned\nendequationThe conditional mean of mathcal Y, given mathcal B=bf b, is the linear predictor, bf Xbfbeta+bf Zbf b, which depends on the p-dimensional fixed-effects parameter, bf beta, and on bf b. The model matrices, bf X and bf Z, of dimension ntimes p and ntimes q, respectively, are determined from the formula for the model and the values of covariates. Although the matrix bf Z can be large (i.e. both n and q can be large), it is sparse (i.e. most of the elements in the matrix are zero).The relative covariance factor, Lambda_theta, is a qtimes q lower-triangular matrix, depending on the variance-component parameter, bftheta, and generating the symmetric qtimes q variance-covariance matrix, Sigma_theta, asbeginequation\nSigma_theta=sigma^2Lambda_thetaLambda_theta\nendequationThe spherical random effects, mathcalUsimmathcalN(bf0sigma^2bfI_q), determine mathcal B according tobeginequation\nmathcalB=Lambda_thetamathcalU\nendequationThe penalized residual sum of squares (PRSS),beginequation\nr^2(thetabetabfu)=bfy - bfXbeta -bfZLambda_thetabfu^2+bfu^2\nendequationis the sum of the residual sum of squares, measuring fidelity of the model to the data, and a penalty on the size of bf u, measuring the complexity of the model. Minimizing r^2 with respect to bf u,beginequation\nr^2_betatheta =min_bfuleft(bfy -bfXbeta -bfZLambda_thetabfu^2+bfu^2right)\nendequationis a direct (i.e. non-iterative) computation. The particular method used to solve this generates a blocked Choleksy factor, bfL_theta, which is an lower triangular qtimes q matrix satisfyingbeginequation\nbfL_thetabfL_theta=Lambda_thetabfZbfZLambda_theta+bfI_q \nendequationwhere bf I_q is the qtimes q identity matrix.Negative twice the log-likelihood of the parameters, given the data, bf y, isbeginequation\nd(bfthetabfbetasigmabf y)\n=nlog(2pisigma^2)+log(bf L_theta^2)+fracr^2_betathetasigma^2\nendequationwhere bf L_theta denotes the determinant of bf L_theta. Because bf L_theta is triangular, its determinant is the product of its diagonal elements.Because the conditional mean, bfmu_mathcal Ymathcal B=bf b=bf Xbfbeta+bf ZLambda_thetabf u, is a linear function of both bfbeta and bf u, minimization of the PRSS with respect to both bfbeta and bf u to producebeginequation\nr^2_theta =min_bfbetabf uleft(bf y -bf Xbfbeta -bf ZLambda_thetabf u^2+bf u^2right)\nendequationis also a direct calculation. The values of bf u and bfbeta that provide this minimum are called, respectively, the conditional mode, tildebf u_theta, of the spherical random effects and the conditional estimate, widehatbfbeta_theta, of the fixed effects. At the conditional estimate of the fixed effects the objective isbeginequation\nd(bfthetawidehatbeta_thetasigmabf y)\n=nlog(2pisigma^2)+log(bf L_theta^2)+fracr^2_thetasigma^2\nendequationMinimizing this expression with respect to sigma^2 produces the conditional estimatebeginequation\nwidehatsigma^2_theta=fracr^2_thetan\nendequationwhich provides the profiled log-likelihood on the deviance scale asbeginequation\ntilded(thetabf y)=d(thetawidehatbeta_thetawidehatsigma_thetabf y)\n=log(bf L_theta^2)+nleft1+logleft(frac2pi r^2_thetanright)right\nendequationa function of bftheta alone.The MLE of bftheta, written widehatbftheta, is the value that minimizes this profiled objective. We determine this value by numerical optimization. In the process of evaluating tilded(widehatthetabf y) we determine widehatbeta=widehatbeta_widehattheta, tildebf u_widehattheta and r^2_widehattheta, from which we can evaluate widehatsigma=sqrtr^2_widehatthetan.The elements of the conditional mode of mathcal B, evaluated at the parameter estimates,beginequation\ntildebf b_widehattheta=Lambda_widehatthetatildebf u_widehattheta\nendequationare sometimes called the best linear unbiased predictors or BLUPs of the random effects. Although BLUPs an appealing acronym, I don’t find the term particularly instructive (what is a “linear unbiased predictor” and in what sense are these the “best”?) and prefer the term “conditional modes”, because these are the values of bf b that maximize the density of the conditional distribution mathcalB  mathcalY = bf y. For a linear mixed model, where all the conditional and unconditional distributions are Gaussian, these values are also the conditional means."
},

{
    "location": "optimization/#MixedModels.ReMat",
    "page": "Details of the parameter estimation",
    "title": "MixedModels.ReMat",
    "category": "type",
    "text": "ReMat{T,S} <: AbstractMatrix{T}\n\nA section of a model matrix generated by a random-effects term.\n\nFields\n\ntrm: the grouping factor as a StatsModels.CategoricalTerm\nrefs: indices into the levels of the grouping factor as a Vector{Int32}\nz: transpose of the model matrix generated by the left-hand side of the term\nwtz: a weighted copy of z (z and wtz are the same object for unweighted cases)\nλ: a LowerTriangular matrix of size S×S\ninds: a Vector{Int} of linear indices of the potential nonzeros in λ\nadjA: the adjoint of the matrix as a SparseMatrixCSC{T}\n\n\n\n\n\n"
},

{
    "location": "optimization/#Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z-1",
    "page": "Details of the parameter estimation",
    "title": "Internal structure of Lambda_theta and bf Z",
    "category": "section",
    "text": "In the types of LinearMixedModel available through the MixedModels package, groups of random effects and the corresponding columns of the model matrix, bf Z, are associated with random-effects terms in the model formula.For the simple examplejulia> fm1 = fit(MixedModel, @formula(Y ~ 1 + (1|G)), dat[:Dyestuff])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n            Column    Variance  Std.Dev. \nG        (Intercept)  1388.3333 37.260345\nResidual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)    1527.5    17.6946   86.326   <1e-99\n──────────────────────────────────────────────────\nthe only random effects term in the formula is (1|G), a simple, scalar random-effects term.julia> t1 = first(fm1.reterms);\n30×6 MixedModels.ReMat{Float64,1}:\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n ⋮                        ⋮\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n\njulia> Int.(t1)  # convert to integers for more compact display\n30×6 Array{Int64,2}:\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n ⋮              ⋮\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\nReMatThis RandomEffectsTerm contributes a block of columns to the model matrix bf Z and a diagonal block to Lambda_theta. In this case the diagonal block of Lambda_theta (which is also the only block) is a multiple of the 6times6 identity matrix where the multiple isjulia> t1.λ\n1×1 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:\n 0.7525806757718846\nBecause there is only one random-effects term in the model, the matrix bf Z is the indicators matrix shown as the result of Matrix(t1), but stored in a special sparse format. Furthermore, there is only one block in Lambda_theta.For a vector-valued random-effects term, as injulia> fm2 = fit(MixedModel, @formula(Y ~ 1+U+(1+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n            Column    Variance   Std.Dev.    Corr.\nG        (Intercept)  565.510678 23.7804684\n         U             32.682124  5.7168282  0.08\nResidual              654.941447 25.5918238\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.63226  37.9064    <1e-99\nU             10.4673    1.50224   6.96781   <1e-11\n───────────────────────────────────────────────────\nthe model matrix bf Z is of the formjulia> t21 = first(fm2.reterms);\n180×36 MixedModels.ReMat{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  3.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  4.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  5.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  6.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  7.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  8.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  9.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n ⋮                        ⋮              ⋱       ⋮                        ⋮\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  2.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  3.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  4.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  1.0  5.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  6.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  7.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  8.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  9.0\n\njulia> Int.(t21) # convert to integers for more compact display\n180×36 Array{Int64,2}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\nand Lambda_theta is a 36times36 block diagonal matrix with 18 diagonal blocks, all of the formjulia> t21.λ\n2×2 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:\n 0.929221    ⋅ \n 0.0181684  0.222645\nThe theta vector isjulia> MixedModels.getθ(t21)\n3-element Array{Float64,1}:\n 0.9292213237442482\n 0.01816835880992632\n 0.22264488486883216\nRandom-effects terms in the model formula that have the same grouping factor are amagamated into a single ReMat object.julia> fm3 = fit!(zerocorr!(LinearMixedModel(@formula(Y ~ 1+U+(1+U|G)),\n    dat[:sleepstudy])))\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -876.00163 1752.00326 1762.00326 1777.96804\n\nVariance components:\n            Column    Variance  Std.Dev.   Corr.\nG        (Intercept)  584.258971 24.17145\n         U             33.632805  5.79938   .  \nResidual              653.115782 25.55613\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.70771  37.48      <1e-99\nU             10.4673    1.51931   6.88951   <1e-11\n───────────────────────────────────────────────────\n\njulia> t31 = first(fm3.reterms);\n180×36 MixedModels.ReMat{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  3.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  4.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  5.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  6.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  7.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  8.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  9.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n ⋮                        ⋮              ⋱       ⋮                        ⋮\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  2.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  3.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  4.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  1.0  5.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  6.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  7.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  8.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  9.0\n\njulia> Int.(t31)\n180×36 Array{Int64,2}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\nNote that we could also have achieved this by re-fitting (a copy of) fm2.julia> fm3alt = fit!(zerocorr!(deepcopy(fm2)))\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -876.00163 1752.00326 1762.00326 1777.96804\n\nVariance components:\n            Column    Variance  Std.Dev.   Corr.\nG        (Intercept)  584.258971 24.17145\n         U             33.632805  5.79938   .  \nResidual              653.115782 25.55613\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.70771  37.48      <1e-99\nU             10.4673    1.51931   6.88951   <1e-11\n───────────────────────────────────────────────────\nFor this model the matrix bf Z is the same as that of model fm2 but the diagonal blocks of Lambda_theta are themselves diagonal.julia> t31.λ\n2×2 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:\n 0.945818   ⋅ \n 0.0       0.226927\n\njulia> MixedModels.getθ(t31)\n2-element Array{Float64,1}:\n 0.9458180666713115\n 0.22692714856454094\nRandom-effects terms with distinct grouping factors generate distinct elements of the trms member of the LinearMixedModel object. Multiple ReMat objects are sorted by decreasing numbers of random effects.julia> fm4 = fit(MixedModel, @formula(Y ~ 1 + (1|H) + (1|G)),\n    dat[:Penicillin])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + (1 | H) + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -166.09417  332.18835  340.18835  352.06760\n\nVariance components:\n            Column    Variance   Std.Dev. \nG        (Intercept)  0.71497950 0.8455646\nH        (Intercept)  3.13519287 1.7706476\nResidual              0.30242640 0.5499331\n Number of obs: 144; levels of grouping factors: 24, 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)   22.9722   0.744596  30.8519   <1e-99\n──────────────────────────────────────────────────\n\njulia> Int.(first(fm4.reterms))\n144×24 Array{Int64,2}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮              ⋮              ⋮        \n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n\njulia> Int.(last(fm4.reterms))\n144×6 Array{Int64,2}:\n 1  0  0  0  0  0\n 0  1  0  0  0  0\n 0  0  1  0  0  0\n 0  0  0  1  0  0\n 0  0  0  0  1  0\n 0  0  0  0  0  1\n 1  0  0  0  0  0\n 0  1  0  0  0  0\n 0  0  1  0  0  0\n 0  0  0  1  0  0\n ⋮              ⋮\n 0  0  0  1  0  0\n 0  0  0  0  1  0\n 0  0  0  0  0  1\n 1  0  0  0  0  0\n 0  1  0  0  0  0\n 0  0  1  0  0  0\n 0  0  0  1  0  0\n 0  0  0  0  1  0\n 0  0  0  0  0  1\nNote that the first ReMat in fm4.terms corresponds to grouping factor G even though the term (1|G) occurs in the formula after (1|H)."
},

{
    "location": "optimization/#MixedModels.OptSummary",
    "page": "Details of the parameter estimation",
    "title": "MixedModels.OptSummary",
    "category": "type",
    "text": "OptSummary\n\nSummary of an NLopt optimization\n\nFields\n\ninitial: a copy of the initial parameter values in the optimization\nlowerbd: lower bounds on the parameter values\nftol_rel: as in NLopt\nftol_abs: as in NLopt\nxtol_rel: as in NLopt\nxtol_abs: as in NLopt\ninitial_step: as in NLopt\nmaxfeval: as in NLopt\nfinal: a copy of the final parameter values from the optimization\nfmin: the final value of the objective\nfeval: the number of function evaluations\noptimizer: the name of the optimizer used, as a Symbol\nreturnvalue: the return value, as a Symbol\nnAGQ: number of adaptive Gauss-Hermite quadrature points in deviance evaluation for GLMMs\nREML: use the REML criterion for LMM fits\n\nThe latter field doesn\'t really belong here but it has to be in a mutable struct in case it is changed.\n\n\n\n\n\n"
},

{
    "location": "optimization/#Progress-of-the-optimization-1",
    "page": "Details of the parameter estimation",
    "title": "Progress of the optimization",
    "category": "section",
    "text": "An optional named argument, verbose=true, in the call to fit for a LinearMixedModel causes printing of the objective and the theta parameter at each evaluation during the optimization.julia> fit(MixedModel, @formula(Y ~ 1 + (1|G)), dat[:Dyestuff],\n    verbose=true);\nf_1: 327.76702 [1.0]\nf_2: 331.03619 [1.75]\nf_3: 330.64583 [0.25]\nf_4: 327.69511 [0.9761896354666064]\nf_5: 327.56631 [0.9285689063998191]\nf_6: 327.3826 [0.8333274482662446]\nf_7: 327.35315 [0.8071883308443906]\nf_8: 327.34663 [0.7996883308443905]\nf_9: 327.341 [0.7921883308443906]\nf_10: 327.33253 [0.7771883308443905]\nf_11: 327.32733 [0.7471883308443905]\nf_12: 327.32862 [0.7396883308443906]\nf_13: 327.32706 [0.7527765100479509]\nf_14: 327.32707 [0.7535265100479508]\nf_15: 327.32706 [0.7525837539403791]\nf_16: 327.32706 [0.7525087539403791]\nf_17: 327.32706 [0.7525912539403792]\nf_18: 327.32706 [0.7525806757718846]\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n            Column    Variance  Std.Dev. \nG        (Intercept)  1388.3333 37.260345\nResidual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)    1527.5    17.6946   86.326   <1e-99\n──────────────────────────────────────────────────\n\njulia> fit(MixedModel, @formula(Y ~ 1 + U + (1+U|G)), dat[:sleepstudy],\n    verbose=true);\nf_1: 1784.6423 [1.0, 0.0, 1.0]\nf_2: 1790.12564 [1.75, 0.0, 1.0]\nf_3: 1798.99962 [1.0, 1.0, 1.0]\nf_4: 1803.8532 [1.0, 0.0, 1.75]\nf_5: 1800.61398 [0.25, 0.0, 1.0]\nf_6: 1798.60463 [1.0, -1.0, 1.0]\nf_7: 1752.26074 [1.0, 0.0, 0.25]\nf_8: 1797.58769 [1.1832612965369789, -0.008661887957844248, 0.0]\nf_9: 1754.95411 [1.075, 0.0, 0.32499999999999996]\nf_10: 1753.69568 [0.8166315695342607, 0.011167254456758141, 0.288237686896882]\nf_11: 1754.817 [1.0, -0.07071067811865475, 0.19696699141100893]\nf_12: 1753.10673 [0.9436827046396487, 0.06383542916425794, 0.2626963029644811]\nf_13: 1752.93938 [0.9801419885633835, -0.026656844944178512, 0.2747427560953827]\nf_14: 1752.25688 [0.9843428851818214, -0.013234749183506141, 0.2471909875448263]\nf_15: 1752.05745 [0.9731403970872217, 0.0025378492302566393, 0.23791031400599733]\nf_16: 1752.02239 [0.9545259030381561, 0.00386421061809991, 0.235892012275948]\nf_17: 1752.02273 [0.9359285300962036, 0.0013317973242921088, 0.23444534669799294]\nf_18: 1751.97169 [0.9549646039744177, 0.007906642484400388, 0.22904616789197482]\nf_19: 1751.9526 [0.9533132639141466, 0.01662737064000363, 0.22576831302931963]\nf_20: 1751.94852 [0.9469287318339392, 0.013076079970067874, 0.22287112675351367]\nf_21: 1751.98718 [0.9334175303181721, 0.006137673812079807, 0.21895094166950246]\nf_22: 1751.98321 [0.9515444328087636, 0.005788999137980141, 0.22061819881558956]\nf_23: 1751.95197 [0.9528093408786977, 0.019033192069848805, 0.224177609031404]\nf_24: 1751.94628 [0.9463215304109949, 0.015373858743613206, 0.22508817725769822]\nf_25: 1751.9467 [0.9471235457918263, 0.01488940582577702, 0.22489234773906214]\nf_26: 1751.94757 [0.9464970169191279, 0.015464270388076334, 0.2258141982361452]\nf_27: 1751.94531 [0.9460858412952298, 0.015793369914683577, 0.22444946254989226]\nf_28: 1751.94418 [0.9453036925631952, 0.016690245702241382, 0.2233605290644677]\nf_29: 1751.94353 [0.9440720728197414, 0.017210606432868543, 0.2227158772065898]\nf_30: 1751.94244 [0.9412710980117244, 0.01630994634387096, 0.22252263180967394]\nf_31: 1751.94217 [0.9390004001605622, 0.015899017037510642, 0.22213197694377954]\nf_32: 1751.94237 [0.9389790832553158, 0.016547964735469448, 0.2215617504865366]\nf_33: 1751.94228 [0.9388628188324375, 0.01524658793301089, 0.2226834617816739]\nf_34: 1751.9422 [0.9382687961029822, 0.015732966996625672, 0.22202359841090216]\nf_35: 1751.94131 [0.9388391784231889, 0.01663733056935153, 0.22261144011889564]\nf_36: 1751.94093 [0.9383965533751186, 0.017396535209898915, 0.22281726223461015]\nf_37: 1751.94057 [0.9370059167925914, 0.01804448868017087, 0.2225344764246926]\nf_38: 1751.94018 [0.9341094752715515, 0.018735420306306462, 0.2219495861766975]\nf_39: 1751.94008 [0.9326416017981755, 0.018924172810914682, 0.2217257565829335]\nf_40: 1751.94027 [0.9313571500157254, 0.019008175762026507, 0.22130945815905506]\nf_41: 1751.9415 [0.9328207209123454, 0.020645421994228894, 0.2213673038893804]\nf_42: 1751.93949 [0.931867480793054, 0.01795737189567532, 0.22256364422576927]\nf_43: 1751.93939 [0.9291674334026199, 0.017782424144124873, 0.2225338455013277]\nf_44: 1751.9394 [0.929658753806991, 0.017772087956706102, 0.22250844010261328]\nf_45: 1751.93943 [0.9291934464129211, 0.018780631688984466, 0.22257042456739484]\nf_46: 1751.93935 [0.9289855981172797, 0.018236602610112413, 0.22248440057299693]\nf_47: 1751.93949 [0.9286970814832717, 0.01829369548702864, 0.2231753597822244]\nf_48: 1751.93936 [0.9282426127717024, 0.018269522687237527, 0.22258371410329972]\nf_49: 1751.93934 [0.9291127470204624, 0.018179125436286685, 0.2226238896772403]\nf_50: 1751.93934 [0.9291905962620962, 0.01816575389497612, 0.22264320534468532]\nf_51: 1751.93935 [0.9292543022205629, 0.018209270205891553, 0.2226208142333736]\nf_52: 1751.93935 [0.9291892275952831, 0.01812979083650926, 0.22257323662227585]\nf_53: 1751.93934 [0.9292535871567192, 0.01816762583654475, 0.2226499048609019]\nf_54: 1751.93934 [0.9292145141482749, 0.018171738500539613, 0.22264674345810112]\nf_55: 1751.93934 [0.9292084267190316, 0.01817147724444257, 0.2226461976270125]\nf_56: 1751.93934 [0.9292093075008536, 0.018172965688292082, 0.22265206264068668]\nf_57: 1751.93934 [0.9292213237442482, 0.01816835880992632, 0.22264488486883216]\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n            Column    Variance   Std.Dev.    Corr.\nG        (Intercept)  565.510678 23.7804684\n         U             32.682124  5.7168282  0.08\nResidual              654.941447 25.5918238\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.63226  37.9064    <1e-99\nU             10.4673    1.50224   6.96781   <1e-11\n───────────────────────────────────────────────────\nA shorter summary of the optimization process is always available as anOptSummaryobject, which is the optsum member of the LinearMixedModel.julia> fm2.optsum\nInitial parameter vector: [1.0, 0.0, 1.0]\nInitial objective value:  1784.642296192471\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [0.0, -Inf, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 1.0, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     57\nFinal parameter vector:   [0.9292213237442482, 0.01816835880992632, 0.22264488486883216]\nFinal objective value:    1751.9393444646867\nReturn code:              FTOL_REACHED\n\n"
},

{
    "location": "optimization/#Modifying-the-optimization-process-1",
    "page": "Details of the parameter estimation",
    "title": "Modifying the optimization process",
    "category": "section",
    "text": "The OptSummary object contains both input and output fields for the optimizer. To modify the optimization process the input fields can be changed after constructing the model but before fitting it.Suppose, for example, that the user wishes to try a Nelder-Mead optimization method instead of the default BOBYQA (Bounded Optimization BY Quadratic Approximation) method.julia> fm2 = LinearMixedModel(@formula(Y ~ 1+U+(1+U|G)), dat[:sleepstudy]);\n\n\njulia> fm2.optsum.optimizer = :LN_NELDERMEAD;\n:LN_NELDERMEAD\n\njulia> fit!(fm2)\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n            Column    Variance   Std.Dev.   Corr.\nG        (Intercept)  565.528831 23.780850\n         U             32.681047  5.716734  0.08\nResidual              654.941678 25.591828\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405     6.63233  37.906    <1e-99\nU             10.4673    1.50222   6.9679   <1e-11\n──────────────────────────────────────────────────\n\njulia> fm2.optsum\nInitial parameter vector: [1.0, 0.0, 1.0]\nInitial objective value:  1784.642296192471\n\nOptimizer (from NLopt):   LN_NELDERMEAD\nLower bounds:             [0.0, -Inf, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 1.0, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     140\nFinal parameter vector:   [0.9292360739538559, 0.018168794976407835, 0.22264111430139058]\nFinal objective value:    1751.9393444750306\nReturn code:              FTOL_REACHED\n\nThe parameter estimates are quite similar to those using :LN_BOBYQA but at the expense of 140 functions evaluations for :LN_NELDERMEAD versus 57 for :LN_BOBYQA.See the documentation for the NLopt package for details about the various settings."
},

{
    "location": "optimization/#MixedModels.issingular",
    "page": "Details of the parameter estimation",
    "title": "MixedModels.issingular",
    "category": "function",
    "text": "issingular(m::LinearMixedModel, θ=m.θ)\n\nTest whether the model m is singular if the parameter vector is θ.\n\n\n\n\n\n"
},

{
    "location": "optimization/#Convergence-to-singular-covariance-matrices-1",
    "page": "Details of the parameter estimation",
    "title": "Convergence to singular covariance matrices",
    "category": "section",
    "text": "To ensure identifiability of Sigma_theta=sigma^2Lambda_theta Lambda_theta, the elements of theta corresponding to diagonal elements of Lambda_theta are constrained to be non-negative. For example, in a trivial case of a single, simple, scalar, random-effects term as in fm1, the one-dimensional theta vector is the ratio of the standard deviation of the random effects to the standard deviation of the response. It happens that -theta produces the same log-likelihood but, by convention, we define the standard deviation to be the positive square root of the variance. Requiring the diagonal elements of Lambda_theta to be non-negative is a generalization of using this positive square root.If the optimization converges on the boundary of the feasible region, that is if one or more of the diagonal elements of Lambda_theta is zero at convergence, the covariance matrix Sigma_theta will be singular. This means that there will be linear combinations of random effects that are constant. Usually convergence to a singular covariance matrix is a sign of an over-specified model.Singularity can be checked with the issingular predicate function.issingularjulia> issingular(fm2)\nfalse\n"
},

{
    "location": "optimization/#Distributions.Bernoulli",
    "page": "Details of the parameter estimation",
    "title": "Distributions.Bernoulli",
    "category": "type",
    "text": "Bernoulli(p)\n\nA Bernoulli distribution is parameterized by a success rate p, which takes value 1 with probability p and 0 with probability 1-p.\n\nP(X = k) = begincases\n1 - p  quad textfor  k = 0 \np  quad textfor  k = 1\nendcases\n\nBernoulli()    # Bernoulli distribution with p = 0.5\nBernoulli(p)   # Bernoulli distribution with success rate p\n\nparams(d)      # Get the parameters, i.e. (p,)\nsuccprob(d)    # Get the success rate, i.e. p\nfailprob(d)    # Get the failure rate, i.e. 1 - p\n\nExternal links:\n\nBernoulli distribution on Wikipedia\n\n\n\n\n\n"
},

{
    "location": "optimization/#Distributions.Poisson",
    "page": "Details of the parameter estimation",
    "title": "Distributions.Poisson",
    "category": "type",
    "text": "Poisson(λ)\n\nA Poisson distribution descibes the number of independent events occurring within a unit time interval, given the average rate of occurrence λ.\n\nP(X = k) = fraclambda^kk e^-lambda quad text for  k = 012ldots\n\nPoisson()        # Poisson distribution with rate parameter 1\nPoisson(lambda)       # Poisson distribution with rate parameter lambda\n\nparams(d)        # Get the parameters, i.e. (λ,)\nmean(d)          # Get the mean arrival rate, i.e. λ\n\nExternal links:\n\nPoisson distribution on Wikipedia\n\n\n\n\n\n"
},

{
    "location": "optimization/#Generalized-Linear-Mixed-Effects-Models-1",
    "page": "Details of the parameter estimation",
    "title": "Generalized Linear Mixed-Effects Models",
    "category": "section",
    "text": "In a generalized linear model the responses are modelled as coming from a particular distribution, such as Bernoulli for binary responses or Poisson for responses that represent counts. The scalar distributions of individual responses differ only in their means, which are determined by a linear predictor expression eta=bf Xbeta, where, as before, bf X is a model matrix derived from the values of covariates and beta is a vector of coefficients.The unconstrained components of eta are mapped to the, possiby constrained, components of the mean response, mu, via a scalar function, g^-1, applied to each component of eta. For historical reasons, the inverse of this function, taking components of mu to the corresponding component of eta is called the link function and more frequently used map from eta to mu is the inverse link.A generalized linear mixed-effects model (GLMM) is defined, for the purposes of this package, bybeginequation\nbeginaligned\n  (mathcalY  mathcalB=bfb) simmathcalD(bfg^-1(Xbeta + Z b)phi)\n  mathcalBsimmathcalN(bf0Sigma_theta) \nendaligned\nendequationwhere mathcalD indicates the distribution family parameterized by the mean and, when needed, a common scale parameter, phi. (There is no scale parameter for Bernoulli or for Poisson. Specifying the mean completely determines the distribution.)Bernoulli\nPoissonA GeneralizedLinearMixedModel object is generated from a formula, data frame and distribution family.julia> const vaform = @formula(r2 ~ 1 + a + g + b + s + (1|id) + (1|item));\nFormulaTerm\nResponse:\n  r2(unknown)\nPredictors:\n  1\n  a(unknown)\n  g(unknown)\n  b(unknown)\n  s(unknown)\n  (id)->1 | id\n  (item)->1 | item\n\njulia> mdl = GeneralizedLinearMixedModel(vaform, dat[:VerbAgg], Bernoulli());\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 10209.8325\n\nVariance components:\n        Column    Variance   Std.Dev. \nid   (Intercept)  0.76562584 0.8750005\nitem (Intercept)  0.76562584 0.8750005\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n──────────────────────────────────────────────────────\n               Estimate  Std.Error    z value  P(>|z|)\n──────────────────────────────────────────────────────\n(Intercept)   0.206053   0.426816    0.482768   0.6293\na             0.0399404  0.0111115   3.59451    0.0003\ng: M          0.231317   0.12743     1.81525    0.0695\nb: scold     -0.794186   0.440571   -1.80263    0.0714\nb: shout     -1.53919    0.440866   -3.49129    0.0005\ns: self      -0.776656   0.359837   -2.15835    0.0309\n──────────────────────────────────────────────────────\n\njulia> typeof(mdl)\nMixedModels.GeneralizedLinearMixedModel{Float64}\nA separate call to fit! can be used to fit the model. This involves optimizing an objective function, the Laplace approximation to the deviance, with respect to the parameters, which are beta, the fixed-effects coefficients, and theta, the covariance parameters. The starting estimate for beta is determined by fitting a GLM to the fixed-effects part of the formulajulia> mdl.β\n6-element Array{Float64,1}:\n  0.20605302210322915\n  0.039940376051149806\n  0.23131667674984452\n -0.7941857249205364\n -1.5391882085456925\n -0.7766556048305918\nand the starting estimate for theta, which is a vector of the two standard deviations of the random effects, is chosen to bejulia> mdl.θ\n2-element Array{Float64,1}:\n 1.0\n 1.0\nThe Laplace approximation to the deviance requires determining the conditional modes of the random effects. These are the values that maximize the conditional density of the random effects, given the model parameters and the data. This is done using Penalized Iteratively Reweighted Least Squares (PIRLS). In most cases PIRLS is fast and stable. It is simply a penalized version of the IRLS algorithm used in fitting GLMs.The distinction between the \"fast\" and \"slow\" algorithms in the MixedModels package (nAGQ=0 or nAGQ=1 in lme4) is whether the fixed-effects parameters, beta, are optimized in PIRLS or in the nonlinear optimizer. In a call to the pirls! function the first argument is a GeneralizedLinearMixedModel, which is modified during the function call. (By convention, the names of such mutating functions end in ! as a warning to the user that they can modify an argument, usually the first argument.) The second and third arguments are optional logical values indicating if beta is to be varied and if verbose output is to be printed.julia> pirls!(mdl, true, true)\nvaryβ = true, obj₀ = 10210.853438905406, β = [0.20605302210322915, 0.039940376051149806, 0.23131667674984452, -0.7941857249205364, -1.5391882085456925, -0.7766556048305918]\n   1: 8301.483049027263\n   2: 8205.604285133919\n   3: 8201.89659746689\n   4: 8201.848598910705\n   5: 8201.848559060705\n   6: 8201.848559060621\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8201.8486\n\nVariance components:\n        Column    Variance  Std.Dev. \nid   (Intercept)  0.8920336 0.9444753\nitem (Intercept)  0.8920336 0.9444753\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n─────────────────────────────────────────────────────\n               Estimate  Std.Error   z value  P(>|z|)\n─────────────────────────────────────────────────────\n(Intercept)   0.218535    0.464651   0.47032   0.6381\na             0.0514385   0.012319   4.17556   <1e-4\ng: M          0.290225    0.140555   2.06485   0.0389\nb: scold     -0.979124    0.476395  -2.05528   0.0399\nb: shout     -1.95402     0.477182  -4.09491   <1e-4\ns: self      -0.979493    0.389283  -2.51615   0.0119\n─────────────────────────────────────────────────────\njulia> deviance(mdl)\n8201.848559060621\njulia> mdl.β\n6-element Array{Float64,1}:\n  0.21853493716546202\n  0.05143854258080739\n  0.2902245416630019\n -0.9791237061900767\n -1.954016762814142\n -0.9794925718038499\njulia> mdl.θ # current values of the standard deviations of the random effects\n2-element Array{Float64,1}:\n 1.0\n 1.0\nIf the optimization with respect to beta is performed within PIRLS then the nonlinear optimization of the Laplace approximation to the deviance requires optimization with respect to theta only. This is the \"fast\" algorithm. Given a value of theta, PIRLS is used to determine the conditional estimate of beta and the conditional mode of the random effects, b.julia> mdl.b # conditional modes of b\n2-element Array{Array{Float64,2},1}:\n [-0.6007716038488773 -1.9322680866219855 … -0.14455373975335264 -0.575223843355732]\n [-0.18636418747921735 0.1805518407165629 … 0.2820923275093614 -0.2219744597240067]\njulia> fit!(mdl, fast=true, verbose=true);\nvaryβ = true, obj₀ = 10251.00311604298, β = [0.21853493716546202, 0.05143854258080739, 0.2902245416630019, -0.9791237061900767, -1.954016762814142, -0.9794925718038499]\n   1: 8292.390783437775\n   2: 8204.692089323944\n   3: 8201.87681054392\n   4: 8201.848569551963\n   5: 8201.848559060627\n   6: 8201.848559060621\nf_1: 8201.848559060621 [1.0, 1.0]\nvaryβ = true, obj₀ = 10565.660003719016, β = [0.21853493716501687, 0.05143854258081827, 0.2902245416630123, -0.9791237061900281, -1.954016762814054, -0.9794925718036391]\n   1: 8356.488185490085\n   2: 8200.93270879382\n   3: 8190.472561852921\n   4: 8190.119918353261\n   5: 8190.117817016411\n   6: 8190.117816802472\nf_2: 8190.117816802472 [1.75, 1.0]\nvaryβ = true, obj₀ = 10317.24630468929, β = [0.19342058735124745, 0.0573877636838676, 0.3179960395260668, -1.0485464263730828, -2.096031981211253, -1.05112880168366]\n   1: 8322.599130522525\n   2: 8227.32605948889\n   3: 8224.479329416503\n   4: 8224.450989053388\n   5: 8224.450978980776\n   6: 8224.45097898077\nf_3: 8224.45097898077 [1.0, 1.75]\nvaryβ = true, obj₀ = 9776.196238005314, β = [0.21876438895214098, 0.05148884100230899, 0.2904607047886008, -0.9797643406722724, -1.9572962794106608, -0.9812079030638575]\n   1: 9035.72524034431\n   2: 9026.060558389583\n   3: 9026.003998095695\n   4: 9026.003906404212\n   5: 9026.003906403306\nf_4: 9026.003906403306 [0.25, 1.0]\nvaryβ = true, obj₀ = 10149.608772216947, β = [0.19893695106092513, 0.04180670725991054, 0.24189864813053613, -0.8153959340434026, -1.6334718386668798, -0.820177206423453]\n   1: 8286.34604472773\n   2: 8208.252412394582\n   3: 8205.812828017322\n   4: 8205.793778239713\n   5: 8205.793775487933\nf_5: 8205.793775487933 [1.0, 0.25]\nvaryβ = true, obj₀ = 10406.59045641377, β = [0.21728094120275768, 0.05073109226841098, 0.2867962059041748, -0.9699881548970225, -1.912733596946607, -0.9591823894789927]\n   1: 8290.678720764576\n   2: 8163.613865769224\n   3: 8157.167433289951\n   4: 8157.041222813376\n   5: 8157.041032394059\n   6: 8157.041032392805\nf_6: 8157.041032392805 [1.385829382367975, 0.7364567143287959]\nvaryβ = true, obj₀ = 10334.285609061479, β = [0.20721523412580223, 0.05490998576683059, 0.30652969414238945, -1.0227806700669033, -2.0401155431545677, -1.0227350541868125]\n   1: 8461.553656168613\n   2: 8371.201859251127\n   3: 8367.771392346047\n   4: 8367.724263488171\n   5: 8367.724223100673\n   6: 8367.724223100575\nf_7: 8367.724223100575 [1.3371524465545725, 0.0]\nvaryβ = true, obj₀ = 10441.992888908888, β = [0.2131077542758387, 0.05262369623724909, 0.2959485161926231, -0.9987562912090399, -1.9339374639156983, -0.9735353380195946]\n   1: 8308.813446103562\n   2: 8177.3181054035895\n   3: 8170.433438731547\n   4: 8170.289084465013\n   5: 8170.288828193685\n   6: 8170.288828191519\nf_8: 8170.288828191519 [1.4136494266702275, 1.1104233507216974]\nvaryβ = true, obj₀ = 10394.123754834101, β = [0.20658453714123365, 0.055213373183277493, 0.3079133180419644, -1.0262965361212193, -2.050456621649834, -1.0280362485645498]\n   1: 8283.540345224605\n   2: 8163.918696581624\n   3: 8158.905795827854\n   4: 8158.829375849114\n   5: 8158.82931759209\n   6: 8158.829317591995\nf_9: 8158.829317591995 [1.2722464441311776, 0.7628110428959163]\nvaryβ = true, obj₀ = 10449.122512319083, β = [0.21122282489374417, 0.054005484057216085, 0.3023095984920893, -1.0122995687447376, -2.0190655839377976, -1.0121221456080796]\n   1: 8299.98230974481\n   2: 8168.539378419905\n   3: 8162.06111254569\n   4: 8161.933600781368\n   5: 8161.933408558587\n   6: 8161.933408557299\nf_10: 8161.933408557299 [1.409362366753737, 0.8680844411698714]\nvaryβ = true, obj₀ = 10414.107933396275, β = [0.20653729580044045, 0.05513368643866937, 0.3075576744964987, -1.0253459434061702, -2.0468884488248316, -1.026189553447287]\n   1: 8286.1383166633\n   2: 8161.902045658136\n   3: 8156.392925946188\n   4: 8156.301068837267\n   5: 8156.300980018685\n   6: 8156.300980018443\nf_11: 8156.300980018443 [1.3269393938014973, 0.7210153433276734]\nvaryβ = true, obj₀ = 10407.145905747226, β = [0.20929363010807042, 0.054440786160254453, 0.304345039527247, -1.01741003899207, -2.0289135444050372, -1.0170762549016008]\n   1: 8284.877807447836\n   2: 8161.70870009135\n   3: 8156.20852356967\n   4: 8156.116764659554\n   5: 8156.116675394206\n   6: 8156.116675393961\nf_12: 8156.116675393961 [1.3236493916636063, 0.7142754704852028]\nvaryβ = true, obj₀ = 10404.356312407195, β = [0.2093966506705094, 0.05441123183581724, 0.30420785069726736, -1.0170635346400032, -2.0281059261520027, -1.0166669610427586]\n   1: 8284.15587979821\n   2: 8161.5390145667825\n   3: 8156.092168783242\n   4: 8156.00215545072\n   5: 8156.002070048329\n   6: 8156.002070048107\nf_13: 8156.002070048107 [1.318465324298525, 0.7088555585344144]\nvaryβ = true, obj₀ = 10404.532964559643, β = [0.20956728950787862, 0.054366773553631904, 0.30400097123018793, -1.016543388757436, -2.026962379921631, -1.0160886153662663]\n   1: 8284.027704597476\n   2: 8161.308933576595\n   3: 8155.84427374012\n   4: 8155.753676771536\n   5: 8155.7535900297025\n   6: 8155.753590029471\nf_14: 8155.753590029471 [1.3207169388571862, 0.7017015224792058]\nvaryβ = true, obj₀ = 10406.080590126905, β = [0.2094760903349143, 0.054381708709038336, 0.3040713812584685, -1.0167138677796828, -2.027198583515585, -1.0162057468818386]\n   1: 8283.987555395466\n   2: 8160.874305438913\n   3: 8155.367283709277\n   4: 8155.275310500366\n   5: 8155.275220625104\n   6: 8155.275220624852\nf_15: 8155.275220624852 [1.3263558167618574, 0.6878017722664787]\nvaryβ = true, obj₀ = 10409.588006838967, β = [0.2092533197474668, 0.054420524079510396, 0.30425395783377623, -1.0171578113766713, -2.027878717803089, -1.0165449133815716]\n   1: 8284.07087504112\n   2: 8160.104155117684\n   3: 8154.505123130813\n   4: 8154.4100933577265\n   5: 8154.409996159079\n   6: 8154.40999615877\nf_16: 8154.40999615877 [1.33858571655632, 0.660407803021088]\nvaryβ = true, obj₀ = 10422.176224406534, β = [0.2087679854481972, 0.05450401062924677, 0.30464664470267566, -1.018107253104718, -2.029334902760005, -1.017271875479348]\n   1: 8286.148930569583\n   2: 8159.408927883551\n   3: 8153.501004776235\n   4: 8153.3950829176165\n   5: 8153.394956151807\n   6: 8153.394956151226\nf_17: 8153.394956151226 [1.3758192930052207, 0.6133582463300428]\nvaryβ = true, obj₀ = 10427.671854015562, β = [0.20732305195121367, 0.05476609730564816, 0.3058757219214257, -1.0210628480445827, -2.0344083911052158, -1.0198199412185298]\n   1: 8286.691066751897\n   2: 8158.869556551065\n   3: 8152.851268971617\n   4: 8152.74108389242\n   5: 8152.740942646442\n   6: 8152.740942645671\nf_18: 8152.740942645671 [1.3951508993587807, 0.5630963988351514]\nvaryβ = true, obj₀ = 10412.134716831908, β = [0.2064753179539796, 0.05487438320972788, 0.3063896760690085, -1.022223083723564, -2.035530371922619, -1.0203755723552408]\n   1: 8282.017684912329\n   2: 8157.522016815687\n   3: 8151.862297991461\n   4: 8151.7648334004025\n   5: 8151.764726543747\n   6: 8151.764726543309\nf_19: 8151.764726543309 [1.3676323395249583, 0.5091235621149712]\nvaryβ = true, obj₀ = 10364.111639275476, β = [0.20730625718500434, 0.05461186270254565, 0.3051748195861246, -1.0191804210353104, -2.0276807789650904, -1.016407572613156]\n   1: 8271.970907130086\n   2: 8157.555557753209\n   3: 8152.875645706707\n   4: 8152.808979106536\n   5: 8152.808935981282\n   6: 8152.808935981224\nf_20: 8152.808935981224 [1.267764057774217, 0.4751231463229955]\nvaryβ = true, obj₀ = 10420.049185022914, β = [0.2106775805664285, 0.05376804338837729, 0.3012295551224639, -1.0092843628511423, -2.006396181497351, -1.0056787436387589]\n   1: 8285.752253359977\n   2: 8159.073978096132\n   3: 8152.9808118979845\n   4: 8152.866577307029\n   5: 8152.866415002038\n   6: 8152.8664150009035\nf_21: 8152.8664150009035 [1.4147967067106257, 0.47109908701108005]\nvaryβ = true, obj₀ = 10395.489389326465, β = [0.2054165953687407, 0.05492082437712702, 0.30662779729796014, -1.0225711061458684, -2.0331407851405006, -1.0191682418215868]\n   1: 8278.256525895966\n   2: 8157.15467263907\n   3: 8151.854900402064\n   4: 8151.769667985722\n   5: 8151.769591419811\n   6: 8151.769591419608\nf_22: 8151.769591419608 [1.3258860654117663, 0.5275226916758308]\nvaryβ = true, obj₀ = 10406.847022579668, β = [0.2088636721793111, 0.054303178542409444, 0.3037280282634005, -1.0156611090056704, -2.0211403904579233, -1.0131090903118896]\n   1: 8281.195202178582\n   2: 8157.492470480242\n   3: 8151.835428349292\n   4: 8151.737863139749\n   5: 8151.737755041531\n   6: 8151.7377550410765\nf_23: 8151.7377550410765 [1.3668062704720423, 0.4986062131306244]\nvaryβ = true, obj₀ = 10396.623183198439, β = [0.20729825419592576, 0.05459363004469543, 0.3050918349449191, -1.0189553661910153, -2.0268543124609257, -1.0159907029306003]\n   1: 8278.460002110194\n   2: 8157.053323937635\n   3: 8151.673309975079\n   4: 8151.585246119004\n   5: 8151.5851619426\n   6: 8151.585161942338\nf_24: 8151.585161942338 [1.3397371137779222, 0.49349219821841317]\nvaryβ = true, obj₀ = 10393.609316249982, β = [0.20825496597630092, 0.054376602982532884, 0.3040778501928797, -1.0164625987921343, -2.02161181700163, -1.0133460788419977]\n   1: 8277.91817976762\n   2: 8157.043669989371\n   3: 8151.689440593437\n   4: 8151.602147764981\n   5: 8151.60206496142\n   6: 8151.6020649611655\nf_25: 8151.6020649611655 [1.337575279538163, 0.48631052204082026]\nvaryβ = true, obj₀ = 10397.764486559114, β = [0.20830542908620922, 0.05435096553108807, 0.30395931061739667, -1.0161551809704839, -2.020721975207284, -1.0128978657449859]\n   1: 8278.935236559668\n   2: 8157.145230819059\n   3: 8151.691194952195\n   4: 8151.600591190689\n   5: 8151.600500691272\n   6: 8151.600500690965\nf_26: 8151.600500690965 [1.3469240905682538, 0.49134805162472306]\nvaryβ = true, obj₀ = 10395.774178197602, β = [0.20798988251464295, 0.05443059796926496, 0.3043309127062346, -1.0170803419713945, -2.022785425821709, -1.0139382184727785]\n   1: 8278.406326249215\n   2: 8157.0644254978615\n   3: 8151.671951343676\n   4: 8151.583465843252\n   5: 8151.583380717897\n   6: 8151.583380717631\nf_27: 8151.583380717631 [1.339581761035688, 0.4973372768687634]\nvaryβ = true, obj₀ = 10395.443943189344, β = [0.20827453095443368, 0.054379842471557256, 0.3040922464549642, -1.0165060438641547, -2.021839296729289, -1.0134606162646753]\n   1: 8278.362655736886\n   2: 8157.06454498357\n   3: 8151.672160260854\n   4: 8151.583675017631\n   5: 8151.583589892588\n   6: 8151.583589892323\nf_28: 8151.583589892323 [1.3392718767157306, 0.49802026420996715]\nvaryβ = true, obj₀ = 10395.535382999697, β = [0.20828805461942254, 0.05437818021173771, 0.30408433495433423, -1.0164879468260286, -2.0218271157127536, -1.0134544452465126]\n   1: 8278.380710340934\n   2: 8157.066651045131\n   3: 8151.671999799075\n   4: 8151.583432800489\n   5: 8151.583347442416\n   6: 8151.583347442147\nf_29: 8151.583347442147 [1.3397210818335894, 0.496955385338418]\nvaryβ = true, obj₀ = 10395.855932031192, β = [0.2082681763394386, 0.05438050242434872, 0.30409540944513563, -1.0165130522338301, -2.0218397675904405, -1.0134608700295133]\n   1: 8278.456305937336\n   2: 8157.0741064683425\n   3: 8151.672400464072\n   4: 8151.583600201322\n   5: 8151.583514305467\n   6: 8151.583514305195\nf_30: 8151.583514305195 [1.3404085881869978, 0.4972551102356541]\nvaryβ = true, obj₀ = 10395.450813218897, β = [0.20824473328006815, 0.05438627252764484, 0.3041223466790985, -1.0165800287332136, -2.021986355042818, -1.0135347946044502]\n   1: 8278.353361601032\n   2: 8157.063550036684\n   3: 8151.671954135133\n   4: 8151.58348562132\n   5: 8151.583400470984\n   6: 8151.583400470718\nf_31: 8151.583400470718 [1.3395728772326896, 0.4962201742148133]\nvaryβ = true, obj₀ = 10395.72665354126, β = [0.20827079143730157, 0.0543784836445478, 0.30408610909569717, -1.0164886208436905, -2.0217637169706513, -1.0134225446120881]\n   1: 8278.426177247793\n   2: 8157.071791450593\n   3: 8151.672300242835\n   4: 8151.583568754749\n   5: 8151.583482990039\n   6: 8151.583482989768\nf_32: 8151.583482989768 [1.340312897111376, 0.4964946742825724]\nvaryβ = true, obj₀ = 10395.480743444838, β = [0.20824538649028715, 0.054384640541713415, 0.3041148615287857, -1.0165600218527853, -2.0219183943816645, -1.0135005493945244]\n   1: 8278.364600272944\n   2: 8157.064659600618\n   3: 8151.67192850791\n   4: 8151.583425349217\n   5: 8151.583340139287\n   6: 8151.583340139021\nf_33: 8151.583340139021 [1.3395583304713574, 0.4968332751137155]\nvaryβ = true, obj₀ = 10395.44226753084, β = [0.20827353831168496, 0.054379076960377144, 0.3040887641339209, -1.016496426972225, -2.0218016612254686, -1.013441655028943]\n   1: 8278.35970289635\n   2: 8157.064704875618\n   3: 8151.671931416344\n   4: 8151.5834264301775\n   5: 8151.58334121545\n   6: 8151.583341215183\nf_34: 8151.583341215183 [1.3395286260144534, 0.49690214197976906]\nvaryβ = true, obj₀ = 10395.479774749043, β = [0.20827484776267674, 0.054378921879052376, 0.3040880249695913, -1.0164947473253754, -2.0218007362745873, -1.0134411854626086]\n   1: 8278.368369189146\n   2: 8157.06558474134\n   3: 8151.671959910018\n   4: 8151.583426365367\n   5: 8151.583341082107\n   6: 8151.5833410818395\nf_35: 8151.5833410818395 [1.3396255162935666, 0.496866607759786]\nvaryβ = true, obj₀ = 10395.478844625617, β = [0.20827126293043166, 0.054379645750214735, 0.304091418793493, -1.0165030367587584, -2.021816263524662, -1.013449018764116]\n   1: 8278.367429147469\n   2: 8157.065445657711\n   3: 8151.671955750739\n   4: 8151.5834264294745\n   5: 8151.5833411541125\n   6: 8151.583341153844\nf_36: 8151.583341153844 [1.3396267569261564, 0.49680256973497444]\nvaryβ = true, obj₀ = 10395.453884480581, β = [0.20827098617839448, 0.054379581686992376, 0.30409113168531715, -1.016502197472964, -2.0218122548203574, -1.0134469996599624]\n   1: 8278.361837064744\n   2: 8157.064895678987\n   3: 8151.671936855653\n   4: 8151.583425364514\n   5: 8151.583340132135\n   6: 8151.583340131867\nf_37: 8151.583340131867 [1.3395638999978707, 0.49683278387610685]\nvaryβ = true, obj₀ = 10395.451247329844, β = [0.20827333787785218, 0.05437912035963998, 0.3040889672857522, -1.0164969265412893, -2.0218026564182896, -1.0134421570125243]\n   1: 8278.3615386939\n   2: 8157.064912191765\n   3: 8151.6719374546865\n   4: 8151.583425366178\n   5: 8151.583340132135\n   6: 8151.583340131868\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8151.5833\n\nVariance components:\n        Column    Variance   Std.Dev.  \nid   (Intercept)  1.63965872 1.28049159\nitem (Intercept)  0.22555221 0.47492337\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n──────────────────────────────────────────────────────\n               Estimate  Std.Error    z value  P(>|z|)\n──────────────────────────────────────────────────────\n(Intercept)   0.208273   0.387547    0.537414   0.5910\na             0.0543791  0.0160145   3.39561    0.0007\ng: M          0.304089   0.182791    1.66359    0.0962\nb: scold     -1.0165     0.246175   -4.12917    <1e-4\nb: shout     -2.0218     0.247803   -8.15891    <1e-15\ns: self      -1.01344    0.201588   -5.02728    <1e-6\n──────────────────────────────────────────────────────\nThe optimization process is summarized byjulia> mdl.LMM.optsum\nInitial parameter vector: [1.0, 1.0]\nInitial objective value:  8201.848559060621\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [0.0, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     37\nFinal parameter vector:   [1.3395638999978707, 0.49683278387610685]\nFinal objective value:    8151.583340131867\nReturn code:              FTOL_REACHED\n\nAs one would hope, given the name of the option, this fit is comparatively fast.julia> @time(fit!(GeneralizedLinearMixedModel(vaform,\n    dat[:VerbAgg], Bernoulli()), fast=true))\n  0.459437 seconds (42.62 k allocations: 8.603 MiB)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8151.5833\n\nVariance components:\n        Column    Variance   Std.Dev.  \nid   (Intercept)  1.63965872 1.28049159\nitem (Intercept)  0.22555221 0.47492337\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n──────────────────────────────────────────────────────\n               Estimate  Std.Error    z value  P(>|z|)\n──────────────────────────────────────────────────────\n(Intercept)   0.208273   0.387547    0.537414   0.5910\na             0.0543791  0.0160145   3.39561    0.0007\ng: M          0.304089   0.182791    1.66359    0.0962\nb: scold     -1.0165     0.246175   -4.12917    <1e-4\nb: shout     -2.0218     0.247803   -8.15891    <1e-15\ns: self      -1.01344    0.201588   -5.02728    <1e-6\n──────────────────────────────────────────────────────\nThe alternative algorithm is to use PIRLS to find the conditional mode of the random effects, given beta and theta and then use the general nonlinear optimizer to fit with respect to both beta and theta. Because it is slower to incorporate the beta parameters in the general nonlinear optimization, the fast fit is performed first and used to determine starting estimates for the more general optimization.julia> @time mdl1 = fit(MixedModel, vaform, dat[:VerbAgg], Bernoulli())\n  2.679921 seconds (285.79 k allocations: 20.602 MiB)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8151.3997\n\nVariance components:\n        Column    Variance   Std.Dev. \nid   (Intercept)  1.64365421 1.2820508\nitem (Intercept)  0.22466196 0.4739852\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n──────────────────────────────────────────────────────\n               Estimate  Std.Error    z value  P(>|z|)\n──────────────────────────────────────────────────────\n(Intercept)   0.199062   0.38774     0.513391   0.6077\na             0.0574299  0.0160361   3.58129    0.0003\ng: M          0.320709   0.183027    1.75225    0.0797\nb: scold     -1.05883    0.245752   -4.30855    <1e-4\nb: shout     -2.10541    0.247401   -8.51012    <1e-16\ns: self      -1.05546    0.201251   -5.24452    <1e-6\n──────────────────────────────────────────────────────\nThis fit provided slightly better results (Laplace approximation to the deviance of 8151.400 versus 8151.583) but took 6 times as long. That is not terribly important when the times involved are a few seconds but can be important when the fit requires many hours or days of computing time.The comparison of the slow and fast fit is available in the optimization summary after the slow fit.julia> mdl1.LMM.optsum\nInitial parameter vector: [0.20827333787834262, 0.0543791203596153, 0.3040889672856593, -1.0164969265411627, -2.0218026564187426, -1.0134421570127687, 1.3395638999925776, 0.49683278389608354]\nInitial objective value:  8151.583340131868\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [-Inf, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10]\ninitial_step:             [0.12918231705359043, 0.005338175496741923, 0.06093022804483975, 0.08205826500126519, 0.08260097852670857, 0.06719613448040833, 0.05, 0.05]\nmaxfeval:                 -1\n\nFunction evaluations:     182\nFinal parameter vector:   [0.19906219627647223, 0.05742989972624096, 0.32070865476554944, -1.0588333264231595, -2.105411768903398, -1.0554623961370777, 1.3397120282857848, 0.4953030525863003]\nFinal objective value:    8151.399719763307\nReturn code:              FTOL_REACHED\n\n"
},

{
    "location": "GaussHermite/#",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "Normalized Gauss-Hermite Quadrature",
    "category": "page",
    "text": ""
},

{
    "location": "GaussHermite/#Normalized-Gauss-Hermite-Quadrature-1",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "Normalized Gauss-Hermite Quadrature",
    "category": "section",
    "text": "Gaussian Quadrature rules provide sets of x values, called abscissae, and weights, w, to approximate an integral with respect to a weight function, g(x). For a kth order rule the approximation isint f(x)g(x)dx approx sum_i=1^k w_i f(x_i)For the Gauss-Hermite rule the weight function isg(x) = e^-x^2and the domain of integration is (-infty infty). A slight variation of this is the normalized Gauss-Hermite rule for which the weight function is the standard normal densityg(z) = phi(z) = frace^-z^22sqrt2piThus, the expected value of f(z), where mathcalZsimmathscrN(01), is approximated asmathbbEf=int_-infty^infty f(z) phi(z)dzapproxsum_i=1^k w_if(z_i) Naturally, there is a caveat. For the approximation to be accurate the function f(z) must behave like a low-order polynomial over the range of interest. More formally, a kth order rule is exact when f is a k-1 order polynomial."
},

{
    "location": "GaussHermite/#MixedModels.GHnorm",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "MixedModels.GHnorm",
    "category": "function",
    "text": "GHnorm(k::Int)\n\nReturn the (unique) GaussHermiteNormalized{k} object.\n\nThe function values are stored (memoized) when first evaluated.  Subsequent evaluations for the same k have very low overhead.\n\n\n\n\n\n"
},

{
    "location": "GaussHermite/#Evaluating-the-weights-and-abscissae-1",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "Evaluating the weights and abscissae",
    "category": "section",
    "text": "In the Golub-Welsch algorithm the abscissae for a particular Gaussian quadrature rule are determined as the eigenvalues of a symmetric tri-diagonal matrix and the weights are derived from the squares of the first row of the matrix of eigenvectors. For a kth order normalized Gauss-Hermite rule the tridiagonal matrix has zeros on the diagonal and the square roots of 1:k-1 on the super- and sub-diagonal, e.g.julia> using LinearAlgebra, Gadfly\n\njulia> sym3 = SymTridiagonal(zeros(3), sqrt.(1:2))\n3×3 LinearAlgebra.SymTridiagonal{Float64,Array{Float64,1}}:\n 0.0  1.0       ⋅ \n 1.0  0.0      1.41421\n  ⋅   1.41421  0.0\n\njulia> ev = eigen(sym3);\nLinearAlgebra.Eigen{Float64,Float64,Array{Float64,2},Array{Float64,1}}\nvalues:\n3-element Array{Float64,1}:\n -1.7320508075688739\n  1.1102230246251565e-15\n  1.7320508075688774\nvectors:\n3×3 Array{Float64,2}:\n -0.408248  -0.816497     0.408248\n  0.707107  -5.43896e-16  0.707107\n -0.57735    0.57735      0.57735\n\njulia> show(ev.values)\n[-1.7320508075688739, 1.1102230246251565e-15, 1.7320508075688774]\njulia> show(abs2.(ev.vectors[1,:]))\n[0.16666666666666743, 0.6666666666666657, 0.16666666666666677]As a function of k this can be written asfunction gausshermitenorm(k)\n    ev = eigen(SymTridiagonal(zeros(k), sqrt.(1:k-1)))\n    ev.values, abs2.(ev.vectors[1,:])\nendprovidingjulia> gausshermitenorm(3)\n([-1.7320508075688739, 1.1102230246251565e-15, 1.7320508075688774], [0.16666666666666743, 0.6666666666666657, 0.16666666666666677])\nThe weights and positions are often shown as a lollipop plot. For the 9th order rule these are (Image: Lollipop plot of 9th order normalized Gauss-Hermite rule)Notice that the magnitudes of the weights drop quite dramatically away from zero, even on a logarithmic scale (Image: Lollipop plot of 9th order normalized Gauss-Hermite rule (logarithmic scale)The definition of MixedModels.GHnorm is similar to the gausshermitenorm function with some extra provisions for ensuring symmetry of the abscissae and the weights and for caching values once they have been calculated.GHnormjulia> using MixedModels\n\njulia> GHnorm(3)\nMixedModels.GaussHermiteNormalized{3}([-1.7320508075688772, 0.0, 1.7320508075688772], [0.16666666666666666, 0.6666666666666666, 0.16666666666666666])\nBy the properties of the normal distribution, when mathcalXsimmathscrN(mu sigma^2)mathbbEg(x) approx sum_i=1^k g(mu + sigma z_i)w_iFor example, mathbbEmathcalX^2 where mathcalXsimmathcalN(2 3^2) isjulia> μ = 2; σ = 3; ghn3 = GHnorm(3);\nMixedModels.GaussHermiteNormalized{3}([-1.7320508075688772, 0.0, 1.7320508075688772], [0.16666666666666666, 0.6666666666666666, 0.16666666666666666])\n\njulia> sum(@. ghn3.w * abs2(μ + σ * ghn3.z))  # should be μ² + σ² = 13\n13.0\n(In general a dot, \'.\', after the function name in a function call, as in abs2.(...), or before an operator creates a fused vectorized evaluation in Julia. The macro @. has the effect of vectorizing all operations in the subsequent expression.)"
},

{
    "location": "GaussHermite/#Application-to-a-model-for-contraception-use-1",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "Application to a model for contraception use",
    "category": "section",
    "text": "A binary response is a \"Yes\"/\"No\" type of answer. For example, in a 1989 fertility survey of women in Bangladesh (reported in Huq, N. M. and Cleland, J., 1990) one response of interest was whether the woman used artificial contraception. Several covariates were recorded including the woman\'s age (centered at the mean), the number of live children the woman has had (in 4 categories: 0, 1, 2, and 3 or more), whether she lived in an urban setting, and the district in which she lived. The version of the data used here is that used in review of multilevel modeling software conducted by the Center for Multilevel Modelling, currently at University of Bristol (http://www.bristol.ac.uk/cmm/learning/mmsoftware/data-rev.html). These data are available as the Contraception data frame in the test data for the MixedModels package.julia> using DataFrames, PooledArrays, RData\n\njulia> const dat = Dict(Symbol(k)=>v for (k,v) in\n    load(joinpath(dirname(pathof(MixedModels)), \"..\", \"test\", \"dat.rda\")));\nDict{Symbol,DataFrames.DataFrame} with 62 entries:\n  :bs10          => 1104×6 DataFrame…\n  :Genetics      => 60×5 DataFrame…\n  :Contraception => 1934×6 DataFrame…\n  :Mmmec         => 354×6 DataFrame…\n  :kb07          => 1790×10 DataFrame. Omitted printing of 3 columns…\n  :Rail          => 18×2 DataFrame…\n  :KKL           => 53765×24 DataFrame. Omitted printing of 16 columns…\n  :Bond          => 21×3 DataFrame…\n  :VerbAgg       => 7584×9 DataFrame. Omitted printing of 1 columns…\n  :ml1m          => 1000209×3 DataFrame…\n  :ergoStool     => 36×3 DataFrame…\n  :s3bbx         => 2449×6 DataFrame…\n  :cake          => 270×5 DataFrame…\n  :Cultivation   => 24×4 DataFrame…\n  :Pastes        => 60×4 DataFrame…\n  :Exam          => 4059×5 DataFrame…\n  :Socatt        => 1056×9 DataFrame. Omitted printing of 2 columns…\n  :WWheat        => 60×3 DataFrame…\n  :Pixel         => 102×5 DataFrame…\n  ⋮              => ⋮\n\njulia> contra = dat[:Contraception];\n1934×6 DataFrame\n│ Row  │ w    │ d    │ use  │ l    │ a       │ urb  │\n│      │ Cat… │ Cat… │ Cat… │ Cat… │ Float64 │ Cat… │\n├──────┼──────┼──────┼──────┼──────┼─────────┼──────┤\n│ 1    │ 1    │ 1    │ N    │ 3+   │ 18.44   │ Y    │\n│ 2    │ 2    │ 1    │ N    │ 0    │ -5.5599 │ Y    │\n│ 3    │ 3    │ 1    │ N    │ 2    │ 1.44    │ Y    │\n│ 4    │ 4    │ 1    │ N    │ 3+   │ 8.44    │ Y    │\n│ 5    │ 5    │ 1    │ N    │ 0    │ -13.559 │ Y    │\n│ 6    │ 6    │ 1    │ N    │ 0    │ -11.56  │ Y    │\n│ 7    │ 7    │ 1    │ N    │ 3+   │ 18.44   │ Y    │\n⋮\n│ 1927 │ 1927 │ 61   │ N    │ 3+   │ 19.44   │ N    │\n│ 1928 │ 1928 │ 61   │ Y    │ 2    │ -9.5599 │ N    │\n│ 1929 │ 1929 │ 61   │ N    │ 2    │ -2.5599 │ N    │\n│ 1930 │ 1930 │ 61   │ N    │ 3+   │ 14.44   │ N    │\n│ 1931 │ 1931 │ 61   │ N    │ 2    │ -4.5599 │ N    │\n│ 1932 │ 1932 │ 61   │ N    │ 3+   │ 14.44   │ N    │\n│ 1933 │ 1933 │ 61   │ N    │ 0    │ -13.56  │ N    │\n│ 1934 │ 1934 │ 61   │ N    │ 3+   │ 10.44   │ N    │\n\njulia> contra.urbdist = PooledArray(string.(contra.urb, contra.d));\n1934-element PooledArrays.PooledArray{String,UInt8,1,Array{UInt8,1}}:\n \"Y1\"\n \"Y1\"\n \"Y1\"\n \"Y1\"\n \"Y1\"\n \"Y1\"\n \"Y1\"\n \"Y1\"\n \"Y1\"\n \"Y1\"\n ⋮\n \"N61\"\n \"N61\"\n \"N61\"\n \"N61\"\n \"N61\"\n \"N61\"\n \"N61\"\n \"N61\"\n \"N61\"\n\njulia> describe(contra)\n7×8 DataFrame. Omitted printing of 1 columns\n│ Row │ variable │ mean       │ min    │ median  │ max   │ nunique │ nmissing │\n│     │ Symbol   │ Union…     │ Any    │ Union…  │ Any   │ Union…  │ Nothing  │\n├─────┼──────────┼────────────┼────────┼─────────┼───────┼─────────┼──────────┤\n│ 1   │ w        │            │ 1      │         │ 1934  │ 1934    │          │\n│ 2   │ d        │            │ 1      │         │ 61    │ 60      │          │\n│ 3   │ use      │            │ N      │         │ Y     │ 2       │          │\n│ 4   │ l        │            │ 0      │         │ 3+    │ 4       │          │\n│ 5   │ a        │ 0.00219788 │ -13.56 │ -1.5599 │ 19.44 │         │          │\n│ 6   │ urb      │            │ N      │         │ Y     │ 2       │          │\n│ 7   │ urbdist  │            │ N1     │         │ Y9    │ 102     │          │\nA smoothed scatterplot of contraception use versus age (Image: Scatterplot smooth of contraception use versus age)shows that the proportion of women using artificial contraception is approximately quadratic in age.A model with fixed-effects for age, age squared, number of live children and urban location and with random effects for district, is fit asjulia> const form1 = @formula use ~ 1 + a + abs2(a) + l + urb + (1|d);\nFormulaTerm\nResponse:\n  use(unknown)\nPredictors:\n  1\n  a(unknown)\n  (a)->abs2(a)\n  l(unknown)\n  urb(unknown)\n  (d)->1 | d\n\njulia> m1 = fit!(GeneralizedLinearMixedModel(form1, contra,\n    Bernoulli()), fast=true)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  use ~ 1 + a + :(abs2(a)) + l + urb + (1 | d)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 2372.7844\n\nVariance components:\n     Column    Variance   Std.Dev.  \nd (Intercept)  0.21774679 0.46663346\n\n Number of obs: 1934; levels of grouping factors: 60\n\nFixed-effects parameters:\n─────────────────────────────────────────────────────────\n                Estimate    Std.Error    z value  P(>|z|)\n─────────────────────────────────────────────────────────\n(Intercept)  -1.01528     0.17102      -5.93659    <1e-8\na             0.00351346  0.00905427    0.388044   0.6980\nabs2(a)      -0.0044867   0.000710568  -6.31423    <1e-9\nl: 1          0.801881    0.15912       5.03948    <1e-6\nl: 2          0.901017    0.181635      4.96058    <1e-6\nl: 3+         0.899412    0.182255      4.93492    <1e-6\nurb: Y        0.684401    0.117653      5.81711    <1e-8\n─────────────────────────────────────────────────────────\nFor a model such as m1, which has a single, scalar random-effects term, the unscaled conditional density of the spherical random effects variable, mathcalU, given the observed data, mathcalY=mathbfy_0, can be expressed as a product of scalar density functions, f_i(u_i) i=1dotsq. In the PIRLS algorithm, which determines the conditional mode vector, tildemathbfu, the optimization is performed on the deviance scale,D(mathbfu)=-2sum_i=1^q log(f_i(u_i))The objective, D, consists of two parts: the sum of the (squared) deviance residuals, measuring fidelity to the data, and the squared length of mathbfu, which is the penalty. In the PIRLS algorithm, only the sum of these components is needed. To use Gauss-Hermite quadrature the contributions of each of the u_ii=1dotsq should be separately evaluated.julia> const devc0 = map!(abs2, m1.devc0, m1.u[1]);  # start with uᵢ²\n60-element Array{Float64,1}:\n 2.5114845771790026\n 0.00529013694640354\n 0.2016563339973937\n 0.11668047713913482\n 0.045719989630254636\n 0.27058435274201\n 0.11609748306414912\n 0.08246085408202662\n 0.1512911211265302\n 0.5616082988628139\n ⋮\n 0.24386891144694575\n 0.08786176949664101\n 0.6611473860322736\n 1.64843814391994\n 1.1004629794859535\n 0.35156940377585666\n 0.5883872791086797\n 0.8152641899281493\n 1.136872080691981\n\njulia> const devresid = m1.resp.devresid;   # n-dimensional vector of deviance residuals\n1934-element Array{Float64,1}:\n 0.3531013712907607\n 0.5077123699687018\n 1.2085924946407482\n 0.9681875441541081\n 0.26463858514142513\n 0.3284338910625065\n 0.3531013712907607\n 1.1491655294159453\n 0.994704930987852\n 1.2071367416804202\n ⋮\n 0.40035223366430367\n 0.2005505507364385\n 2.7209367282940846\n 0.8340996078942532\n 0.40035223366430367\n 0.7868549216747006\n 0.40035223366430367\n 0.17466693804975833\n 0.5876119662635706\n\njulia> const refs = first(m1.LMM.reterms).refs;  # n-dimensional vector of indices in 1:q\n1934-element Array{Int32,1}:\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  1\n  ⋮\n 60\n 60\n 60\n 60\n 60\n 60\n 60\n 60\n 60\n\njulia> for (dr, i) in zip(devresid, refs)\n    devc0[i] += dr\nend\n\njulia> show(devc0)\n[121.29271723748425, 22.0226165831912, 2.918947004593085, 30.787663284677702, 47.541948927264784, 69.55510271698343, 23.404661972654246, 46.27897251261291, 24.452790654470174, 7.759486786996406, 9.773641928923807, 42.75889919869982, 27.552582954552403, 156.42050914569697, 26.19252846238105, 27.41606587298361, 24.53807997847252, 57.566227945749255, 31.17918972210783, 22.341704266100074, 27.477954351640907, 19.98849929467636, 16.010831820133824, 9.761474205785612, 83.86327332081078, 15.56869100019553, 42.7598348904854, 51.46860857082799, 32.733185389528536, 70.41570015607486, 39.68595069616566, 27.544035457863412, 14.697521325431218, 53.047387016108736, 64.84987517176233, 19.743884710056975, 19.415607617025813, 11.242251950257174, 37.41690565364499, 54.265100914177104, 39.5825676682947, 17.398376854030023, 60.227534906701756, 28.819203188191878, 42.44406989090426, 112.99168542633706, 17.297670135024173, 51.57724961555985, 2.1872365328155148, 22.961445506449667, 47.414478727875135, 87.23151591746938, 25.92354472114336, 9.47033825103759, 61.17621086655625, 27.1027737898575, 48.01630958361095, 8.460230421231415, 30.365249759249785, 47.37405163542367]One thing to notice is that, even on the deviance scale, the contributions of different districts can be of different magnitudes. This is primarily due to different sample sizes in the different districts.julia> using FreqTables\n\njulia> freqtable(contra, :d)\'\n1×60 Named LinearAlgebra.Adjoint{Int64,Array{Int64,1}}\n\' ╲ d │   …  \n──────┼──────\n1     │   …  \nBecause the first district has one of the largest sample sizes and the third district has the smallest sample size, these two will be used for illustration. For a range of u values, evaluate the individual components of the deviance and store them in a matrix.const devc = m1.devc;\nconst xvals = -5.0:2.0^(-4):5.0;\nconst uv = vec(m1.u[1]);\nconst u₀ = vec(m1.u₀[1]);\nresults = zeros(length(devc0), length(xvals))\nfor (j, u) in enumerate(xvals)\n    fill!(devc, abs2(u))\n    fill!(uv, u)\n    MixedModels.updateη!(m1)\n    for (dr, i) in zip(devresid, refs)\n        devc[i] += dr\n    end\n    copyto!(view(results, :, j), devc)\nendA plot of the deviance contribution versus u_1 (Image: Deviance contribution of u₁)shows that the deviance contribution is very close to a quadratic. This is also true for u_3 (Image: Deviance contribution of u₃)The PIRLS algorithm provides the locations of the minima of these scalar functions, stored asjulia> m1.u₀[1]\n1×60 Array{Float64,2}:\n -1.58477  -0.0727333  0.449062  0.341585  …  -0.767064  -0.90292  -1.06624\nthe minima themselves, evaluated as devc0 above, and a horizontal scale, which is the inverse of diagonal of the Cholesky factor. As shown below, this is an estimate of the conditional standard deviations of the components of mathcalU.julia> const s = inv.(m1.LMM.L[Block(1,1)].diag);\n60-element Array{Float64,1}:\n 0.4068892704882805\n 0.7135114006793071\n 0.9521643926743314\n 0.6271354674398224\n 0.5947434104257698\n 0.50415162521186\n 0.739007504179373\n 0.6003008023768596\n 0.6905248617901879\n 0.8347786416508123\n ⋮\n 0.49346933412966276\n 0.7126399240251733\n 0.8683321336910307\n 0.5508138526804186\n 0.6901024711150282\n 0.6152405328431272\n 0.8396793450870781\n 0.654965324022308\n 0.603259538980649\n\njulia> s\'\n1×60 LinearAlgebra.Adjoint{Float64,Array{Float64,1}}:\n 0.406889  0.713511  0.952164  0.627135  …  0.839679  0.654965  0.60326\nThe curves can be put on a common scale, corresponding to the standard normal, asjulia> for (j, z) in enumerate(xvals)\n    @. uv = u₀ + z * s\n    MixedModels.updateη!(m1)\n    @. devc = abs2(uv) - devc0\n    for (dr, i) in zip(devresid, refs)\n        devc[i] += dr\n    end\n    copyto!(view(results, :, j), devc)\nend\n(Image: Scaled and shifted deviance contributions)(Image: Scaled and shifted deviance contributions)On the original density scale these becomejulia> for (j, z) in enumerate(xvals)\n    @. uv = u₀ + z * s\n    MixedModels.updateη!(m1)\n    @. devc = abs2(uv) - devc0\n    for (dr, i) in zip(devresid, refs)\n        devc[i] += dr\n    end\n    copyto!(view(results, :, j), @. exp(-devc/2))\nend\n(Image: Scaled and shifted conditional density)(Image: Scaled and shifted conditional density)and the function to be integrated with the normalized Gauss-Hermite rule isjulia> for (j, z) in enumerate(xvals)\n    @. uv = u₀ + z * s\n    MixedModels.updateη!(m1)\n    @. devc = abs2(uv) - devc0\n    for (dr, i) in zip(devresid, refs)\n        devc[i] += dr\n    end\n    copyto!(view(results, :, j), @. exp((abs2(z) - devc)/2))\nend\n(Image: Function to be integrated with normalized Gauss-Hermite rule)(Image: Function to be integrated with normalized Gauss-Hermite rule)"
},

{
    "location": "bootstrap/#",
    "page": "Parametric bootstrap for linear mixed-effects models",
    "title": "Parametric bootstrap for linear mixed-effects models",
    "category": "page",
    "text": ""
},

{
    "location": "bootstrap/#MixedModels.parametricbootstrap",
    "page": "Parametric bootstrap for linear mixed-effects models",
    "title": "MixedModels.parametricbootstrap",
    "category": "function",
    "text": "parametricbootstrap(rng::AbstractRNG, nsamp::Integer, m::LinearMixedModel,\n    props=(:objective, :σ, :β, :θ); β = m.β, σ = m.σ, θ = m.θ)\nparametricbootstrap(nsamp::Integer, m::LinearMixedModel,\n    props=(:objective, :σ, :β, :θ); β = m.β, σ = m.σ, θ = m.θ)\n\nPerform nsamp parametric bootstrap replication fits of m, returning a Tables.ColumnTable of parameter estimates of the refit model.\n\nThe default random number generator is Random.GLOBAL_RNG.\n\nNamed Arguments\n\nβ, σ, and θ are the values of m\'s parameters for simulating the responses.\n\n\n\n\n\n"
},

{
    "location": "bootstrap/#Parametric-bootstrap-for-linear-mixed-effects-models-1",
    "page": "Parametric bootstrap for linear mixed-effects models",
    "title": "Parametric bootstrap for linear mixed-effects models",
    "category": "section",
    "text": "Julia is well-suited to implementing bootstrapping and other simulation-based methods for statistical models. The parametricbootstrap function in the MixedModels package provides an efficient parametric bootstrap for linear mixed-effects models.parametricbootstrap"
},

{
    "location": "bootstrap/#The-parametric-bootstrap-1",
    "page": "Parametric bootstrap for linear mixed-effects models",
    "title": "The parametric bootstrap",
    "category": "section",
    "text": "Bootstrapping is a family of procedures for generating sample values of a statistic, allowing for visualization of the distribution of the statistic or for inference from this sample of values.A parametric bootstrap is used with a parametric model, m, that has been fit to data. The procedure is to simulate n response vectors from m using the estimated parameter values and refit m to these responses in turn, accumulating the statistics of interest at each iteration.The parameters of a LinearMixedModel object are the fixed-effects parameters, β, the standard deviation, σ, of the per-observation noise, and the covariance parameter, θ, that defines the variance-covariance matrices of the random effects.For example, a simple linear mixed-effects model for the Dyestuff data in the lme4 package for R is fit byjulia> using DataFrames, Gadfly, MixedModels, Random, RData\n\njulia> datf = joinpath(dirname(pathof(MixedModels)), \"..\", \"test\", \"dat.rda\");\n\"/home/travis/build/JuliaStats/MixedModels.jl/src/../test/dat.rda\"\n\njulia> const dat = Dict(Symbol(k)=>v for (k,v) in load(datf));\nDict{Symbol,DataFrames.DataFrame} with 62 entries:\n  :bs10          => 1104×6 DataFrame…\n  :Genetics      => 60×5 DataFrame…\n  :Contraception => 1934×6 DataFrame…\n  :Mmmec         => 354×6 DataFrame…\n  :kb07          => 1790×10 DataFrame. Omitted printing of 3 columns…\n  :Rail          => 18×2 DataFrame…\n  :KKL           => 53765×24 DataFrame. Omitted printing of 16 columns…\n  :Bond          => 21×3 DataFrame…\n  :VerbAgg       => 7584×9 DataFrame. Omitted printing of 1 columns…\n  :ml1m          => 1000209×3 DataFrame…\n  :ergoStool     => 36×3 DataFrame…\n  :s3bbx         => 2449×6 DataFrame…\n  :cake          => 270×5 DataFrame…\n  :Cultivation   => 24×4 DataFrame…\n  :Pastes        => 60×4 DataFrame…\n  :Exam          => 4059×5 DataFrame…\n  :Socatt        => 1056×9 DataFrame. Omitted printing of 2 columns…\n  :WWheat        => 60×3 DataFrame…\n  :Pixel         => 102×5 DataFrame…\n  ⋮              => ⋮\njulia> ds = names!(dat[:Dyestuff], [:Batch, :Yield])  # the Dyestuff data\n30×2 DataFrame\n│ Row │ Batch │ Yield   │\n│     │ Cat…  │ Float64 │\n├─────┼───────┼─────────┤\n│ 1   │ A     │ 1545.0  │\n│ 2   │ A     │ 1440.0  │\n│ 3   │ A     │ 1440.0  │\n│ 4   │ A     │ 1520.0  │\n│ 5   │ A     │ 1580.0  │\n│ 6   │ B     │ 1540.0  │\n│ 7   │ B     │ 1555.0  │\n⋮\n│ 23  │ E     │ 1515.0  │\n│ 24  │ E     │ 1635.0  │\n│ 25  │ E     │ 1625.0  │\n│ 26  │ F     │ 1520.0  │\n│ 27  │ F     │ 1455.0  │\n│ 28  │ F     │ 1450.0  │\n│ 29  │ F     │ 1480.0  │\n│ 30  │ F     │ 1445.0  │\n\njulia> m1 = fit(MixedModel, @formula(Yield ~ 1 + (1 | Batch)), ds)\nLinear mixed model fit by maximum likelihood\n Yield ~ 1 + (1 | Batch)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n            Column    Variance  Std.Dev. \nBatch    (Intercept)  1388.3333 37.260345\nResidual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)    1527.5    17.6946   86.326   <1e-99\n──────────────────────────────────────────────────\nTo bootstrap the model parameters, first initialize a random number generatorjulia> const rng = MersenneTwister(1234321);\nRandom.MersenneTwister(UInt32[0x0012d591], Random.DSFMT.DSFMT_state(Int32[-1066020669, 1073631810, 397127531, 1072701603, -312796895, 1073626997, 1020815149, 1073320576, 650048908, 1073512247  …  -352178910, 1073735534, 1816227101, 1072823316, -1468787611, -2121692099, 358864500, -310934288, 382, 0]), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], UInt128[0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000  …  0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000], 1002, 0)\nthen create a bootstrap samplejulia> samp = parametricbootstrap(rng, 10_000, m1);\nMixedModels.MixedModelBootstrap{Float64}(Linear mixed model fit by maximum likelihood\n Yield ~ 1 + (1 | Batch)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n            Column    Variance  Std.Dev. \nBatch    (Intercept)  1388.3333 37.260345\nResidual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n             Estimate  Std.Error  z value  P(>|z|)\n──────────────────────────────────────────────────\n(Intercept)    1527.5    17.6946   86.326   <1e-99\n──────────────────────────────────────────────────, (objective = [339.021863850754, 322.6888354106908, 324.0016457910033, 331.8870128504819, 317.77134783688007, 315.18080922959905, 333.64103198417166, 325.72878851707304, 318.60860053332925, 337.0925761232729  …  318.9463465155626, 314.41544022454036, 333.7011776583149, 345.06018724251896, 325.49087959400464, 337.0944970180786, 313.8709417213373, 326.57348308338555, 325.5755203177548, 310.72062270779793], σ = [67.4314916481805, 47.9830956010639, 50.13459856403843, 53.2237808663952, 45.29750506813072, 36.75557692579198, 53.81608757276362, 47.89890534520966, 40.95913674030757, 66.13736554684341  …  45.218090536364244, 41.93177111116575, 57.28935548071347, 67.40915181268834, 43.4966731007059, 54.77092339069152, 40.03200399440087, 43.75485449334468, 50.53305786901695, 36.6760549125889], β₁ = [1509.1284446460834, 1538.0812176033469, 1508.0209147055527, 1538.472695544024, 1520.623038448402, 1536.9434765279964, 1519.8834492242709, 1528.427793571972, 1510.4845889057676, 1516.0935235523627  …  1504.667464294557, 1510.9261788423394, 1535.4323805790123, 1510.4359472039882, 1544.9158611720923, 1525.2833456278975, 1563.3155682947622, 1519.098192704569, 1529.7393020896018, 1537.9241993159928], θ = StaticArrays.SArray{Tuple{1},Float64,1,1}[[0.21224540715905238], [0.5328401956731739], [0.43407618998421665], [0.7713825334733218], [0.42342771023984627], [1.3381150681332619], [0.8679925874573443], [0.785752405076357], [0.9967273169639079], [0.12512803753626725]  …  [0.5191410190752364], [0.5188621833216299], [0.5614687007579606], [0.6874613006322968], [1.3638983454247982], [1.1053986473813475], [0.6937385250661969], [1.4580130410592453], [0.5162288796279336], [0.8759306289747937]]), Dict(\"(Intercept)\" => 3))\n\njulia> propertynames(samp)\n7-element Array{Symbol,1}:\n :model\n :objective\n :σ\n :θ\n :σs\n :σρs\n Symbol(\"(Intercept)\")\nAs shown above, the sample has several named properties, which allow for convenient extraction of information.  For example, a density plot of the estimates of σ, the residual standard deviation, can be created asplot(x=samp.σ, Geom.density, Guide.xlabel(\"Parametric bootstrap estimates of σ\"))(Image: )For the estimates of the intercept parameter, the getproperty extractor must be usedplot(x = getproperty(samp, Symbol(\"(Intercept)\")), Geom.density,\n    Guide.xlabel(\"Parametric bootstrap estimates of β₁\"))(Image: )The σs property contains the estimates of the standard deviation of the random effects in a hierarchical format.julia> typeof(samp.σs)\nNamedTuple{(:Batch,),Tuple{Array{NamedTuple{(Symbol(\"(Intercept)\"),),Tuple{Float64}},1}}}\nThis is to allow for random effects associated with more than one grouping factor. If we only have one grouping factor for random effects, which is the case here, we can use the first extractor, as injulia> first(samp.σs)\n10000-element Array{NamedTuple{(Symbol(\"(Intercept)\"),),Tuple{Float64}},1}:\n ((Intercept) = 14.31202440021031,)\n ((Intercept) = 25.567322049075496,)\n ((Intercept) = 21.76223553106598,)\n ((Intercept) = 41.05589492574884,)\n ((Intercept) = 19.18021885057642,)\n ((Intercept) = 49.18319132233348,)\n ((Intercept) = 46.711965099114124,)\n ((Intercept) = 37.636680075523266,)\n ((Intercept) = 40.82509046832459,)\n ((Intercept) = 8.275638758695251,)\n ⋮\n ((Intercept) = 21.75681030928231,)\n ((Intercept) = 32.16617998901714,)\n ((Intercept) = 46.34118317967067,)\n ((Intercept) = 59.3250404735361,)\n ((Intercept) = 60.54370463189782,)\n ((Intercept) = 27.77174340651976,)\n ((Intercept) = 63.795148460946265,)\n ((Intercept) = 26.086623847896156,)\n ((Intercept) = 32.12567984789807,)\nor, to carry this one step further,plot(x=first.(first(samp.σs)), Geom.density,\n    Guide.xlabel(\"Parametric bootstrap estimates of σ₁\"))(Image: )Notice that this density plot has a spike, or mode, at zero. Although this mode appears to be diffuse, this is an artifact of the way that density plots are created. In fact, it is a pulse, as can be seen from a histogram.plot(x=first.(first(samp.σs)), Geom.histogram,\n    Guide.xlabel(\"Parametric bootstrap estimates of σ₁\"))(Image: )A value of zero for the standard deviation of the random effects is an example of a singular covariance. It is easy to detect the singularity in the case of a scalar random-effects term. However, it is not as straightforward to detect singularity in vector-valued random-effects terms.For example, if we bootstrap a model fit to the sleepstudy datajulia> m2 = fit(MixedModel, @formula(Y ~ 1+U+(1+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n            Column    Variance   Std.Dev.    Corr.\nG        (Intercept)  565.510678 23.7804684\n         U             32.682124  5.7168282  0.08\nResidual              654.941447 25.5918238\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.63226  37.9064    <1e-99\nU             10.4673    1.50224   6.96781   <1e-11\n───────────────────────────────────────────────────\njulia> samp2 = parametricbootstrap(rng, 10_000, m2);\nMixedModels.MixedModelBootstrap{Float64}(Linear mixed model fit by maximum likelihood\n Y ~ 1 + U + (1 + U | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n            Column    Variance   Std.Dev.    Corr.\nG        (Intercept)  565.510678 23.7804684\n         U             32.682124  5.7168282  0.08\nResidual              654.941447 25.5918238\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────\n             Estimate  Std.Error   z value  P(>|z|)\n───────────────────────────────────────────────────\n(Intercept)  251.405     6.63226  37.9064    <1e-99\nU             10.4673    1.50224   6.96781   <1e-11\n───────────────────────────────────────────────────, (objective = [1752.610805686423, 1750.6433968770227, 1763.0761823998494, 1729.0466608477655, 1746.4043964310654, 1770.7271762999399, 1768.6664526277698, 1765.952120035669, 1722.2942482975877, 1762.5188670632056  …  1748.4922340620237, 1750.1758991698052, 1734.6025052324753, 1768.1642635284718, 1735.9095045075633, 1773.7555233940284, 1750.4237601165719, 1734.415086664164, 1746.5763028539777, 1734.1961260570354], σ = [26.366356794050816, 26.137153345194626, 27.080063608233825, 24.8863832002222, 25.320423159098027, 27.96373020193531, 27.906227094181567, 26.221935258927044, 24.124342549581986, 27.058908050593583  …  25.273061394597455, 25.503669921655007, 23.79917489885199, 27.783576684388006, 24.70085327464388, 27.83991801939834, 26.011371075025185, 25.06924550534468, 25.713456142369402, 25.357631856348398], β₁ = [248.5601411770255, 260.42244339674755, 245.90847207717263, 267.4372482416199, 247.93888071908813, 243.7428118595536, 243.51982144722027, 247.94937815128534, 244.1391749380658, 249.44325914293472  …  252.87575687250282, 249.23609488651402, 252.7917786296442, 253.69835353054714, 256.3260728845328, 251.0875448267856, 248.6853400859739, 254.47910735321514, 250.1758696324152, 254.6693985479193], β₂ = [9.341228514017605, 9.992562120062306, 11.208330884887944, 11.550558248059845, 12.037534245522314, 11.433612895513559, 10.36313198552376, 10.783198558722203, 9.480559904683972, 10.36744050650216  …  10.892984705123329, 11.061003483285797, 8.114709881911544, 10.315661074984812, 8.32111200269778, 11.236848634014533, 10.91961620688782, 11.453298205617864, 12.034898339764974, 9.322554399563312], θ = StaticArrays.SArray{Tuple{3},Float64,1,3}[[0.6685649095333001, -0.002130374770627311, 0.2119058961269979], [0.8741185458468017, 0.1416044114043354, 0.11230895566778003], [0.9489677641919867, 0.06617690454837777, 0.13763223672305944], [1.0740817750105875, -0.0389737092186008, 0.12890918617499325], [1.1748505757334857, -0.11591519616351914, 0.18523777204660635], [0.592506296469461, 0.03303645479722885, 0.19872690177111685], [0.5857301753084287, 0.0044736486861657035, 0.19917606785117573], [1.049790975362002, 0.12285401754474407, 0.21134035457236724], [0.674166659951378, 0.09482181481988072, 0.19158568417458904], [0.8183134040802323, 0.12384702163422046, 0.13219641607246557]  …  [1.1599899020087132, 0.0259686394883999, 0.18360028260213743], [1.0825169002949757, 0.05241293225705042, 0.17953338407651115], [0.9897388815918189, 0.08873277393879761, 0.2671986923057685], [0.8489499258176735, 0.021264559437824133, 0.1467188762476598], [1.0136195663177447, -0.0259014664080036, 0.19140412945017726], [1.0081757866000411, -0.033140481480658575, 0.161723255709992], [1.081751393646913, 0.05606842245773908, 0.1303389424583289], [0.7314450540760008, -0.06298776479053886, 0.20757474712403934], [0.6160108695034013, -0.03533980711248402, 0.25134925058389346], [0.7691718531729048, 0.04838012493232449, 0.14513770782505866]]), Dict(\"U\" => 4,\"(Intercept)\" => 3))\nthe singularity can be exhibited as a standard deviation of zero or as a correlation of pm1. The σρs property of the sample is a vector of named tuplesjulia> σρ = first(samp2.σρs);\n10000-element Array{NamedTuple{(:σ, :ρ),Tuple{NamedTuple{(Symbol(\"(Intercept)\"), :U),Tuple{Float64,Float64}},Tuple{Float64}}},1}:\n (σ = ((Intercept) = 17.627620944737295, U = 5.5874688077695165), ρ = (-0.010052892148462242,))\n (σ = ((Intercept) = 22.846970474676397, U = 4.723896285967684), ρ = (0.7834922680723013,))\n (σ = ((Intercept) = 25.698107416482436, U = 4.135544686218198), ρ = (0.4333346440526252,))\n (σ = ((Intercept) = 26.730010641288324, U = 3.3514972156973455), ρ = (-0.2893974244720719,))\n (σ = ((Intercept) = 29.747713726281802, U = 5.532924692359015), ρ = (-0.5304648048948569,))\n (σ = ((Intercept) = 16.568686217419902, U = 5.6334104898212995), ρ = (0.16398991524713905,))\n (σ = ((Intercept) = 16.34551928807179, U = 5.559654435754668), ρ = (0.022455110766031083,))\n (σ = ((Intercept) = 27.527550991348292, U = 6.41006216326687), ρ = (0.5025645636977025,))\n (σ = ((Intercept) = 16.263827440174598, U = 5.15698385675058), ρ = (0.44357593613434276,))\n (σ = ((Intercept) = 22.14266715757524, U = 4.90162071588571), ρ = (0.6836851247750406,))\n ⋮\n (σ = ((Intercept) = 27.608153709736182, U = 4.769892106449085), ρ = (0.28024158494121426,))\n (σ = ((Intercept) = 23.554968747197858, U = 6.7005834690606125), ρ = (0.31516162972682127,))\n (σ = ((Intercept) = 23.58686536516084, U = 4.1189665594033285), ρ = (0.14343537615078317,))\n (σ = ((Intercept) = 25.037268183922773, U = 4.770938109461922), ρ = (-0.13410115718612223,))\n (σ = ((Intercept) = 28.06753124808758, U = 4.59592299370252), ρ = (-0.20074929210283513,))\n (σ = ((Intercept) = 28.137836911075496, U = 3.6906742297750323), ρ = (0.3951626319043677,))\n (σ = ((Intercept) = 18.33677563430138, U = 5.4380466083859975), ρ = (-0.29037186568645185,))\n (σ = ((Intercept) = 15.839768476198552, U = 6.526627697921208), ρ = (-0.13923095085630277,))\n (σ = ((Intercept) = 19.504376687023782, U = 3.879435143350915), ρ = (0.31623299574957126,))\n\njulia> typeof(σρ)\nArray{NamedTuple{(:σ, :ρ),Tuple{NamedTuple{(Symbol(\"(Intercept)\"), :U),Tuple{Float64,Float64}},Tuple{Float64}}},1}\nwhere the first element of the tuple is itself a tuple of standard deviations and the second (also the last) element of the tuple is the correlation.A histogram of the estimated correlations from the bootstrap sample has a spike at +1.ρs = first.(last.(σρ))\nplot(x = ρs, Geom.histogram,\n    Guide.xlabel(\"Parametric bootstrap samples of correlation of random effects\"))(Image: )or, as a count,julia> sum(ρs .≈ 1)\n299\nClose examination of the histogram shows a few values of -1.julia> sum(ρs .≈ -1)\n5\nFurthermore there are even a few cases where the estimate of the standard deviation of the random effect for the intercept is zero.julia> sum(first.(first.(first.(σρ))) .≈ 0)\n6\nThere is a general condition to check for singularity of an estimated covariance matrix or matrices in a bootstrap sample. The parameter optimized in the estimation is θ, the relative covariance parameter. Some of the elements of this parameter vector must be non-negative and, when one of these components is approximately zero, one of the covariance matrices will be singular.The boundary values are available asjulia> samp2.model.optsum.lowerbd\n3-element Array{Float64,1}:\n   0.0\n -Inf\n   0.0\nso the check on singularity becomesjulia> sum(θ -> any(θ .≈ samp2.model.optsum.lowerbd), samp2.θ)\n310\nThe issingular method for a LinearMixedModel object that tests if a parameter vector θ corresponds to a boundary or singular fit. The default value of θ is that from the model but another value can be given as the second argument.Using this function the number of singular fits in the bootstrap sample can be counted asjulia> sum(issingular.(Ref(samp2.m), samp2.θ))\n310\n"
},

]}
