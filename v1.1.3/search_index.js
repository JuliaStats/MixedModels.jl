var documenterSearchIndex = {"docs": [

{
    "location": "#",
    "page": "MixedModels.jl Documentation",
    "title": "MixedModels.jl Documentation",
    "category": "page",
    "text": ""
},

{
    "location": "#MixedModels.jl-Documentation-1",
    "page": "MixedModels.jl Documentation",
    "title": "MixedModels.jl Documentation",
    "category": "section",
    "text": "CurrentModule = MixedModelsMixedModels.jl is a Julia package providing capabilities for fitting and examining linear and generalized linear mixed-effect models. It is similar in scope to the lme4 package for R.Pages = [\n        \"constructors.md\",\n        \"optimization.md\",\n        \"GaussHermite.md\",\n        \"bootstrap.md\",\n        \"SimpleLMM.md\",\n        \"MultipleTerms.md\",\n        \"SingularCovariance.md\",\n        \"SubjectItem.md\"]\nDepth = 2"
},

{
    "location": "constructors/#",
    "page": "Model constructors",
    "title": "Model constructors",
    "category": "page",
    "text": ""
},

{
    "location": "constructors/#MixedModels.LinearMixedModel",
    "page": "Model constructors",
    "title": "MixedModels.LinearMixedModel",
    "category": "type",
    "text": "LinearMixedModel\n\nLinear mixed-effects model representation\n\nFields\n\nformula: the formula for the model\ntrms: a Vector of AbstractTerm types representing the model.  The last element is the response.\nsqrtwts: vector of square roots of the case weights.  Can be empty.\nA: an nt × nt symmetric BlockMatrix of matrices representing hcat(Z,X,y)\'hcat(Z,X,y)\nL: a nt × nt BlockMatrix - the lower Cholesky factor of Λ\'AΛ+I\noptsum: an OptSummary object\n\nProperties\n\nθ or theta: the covariance parameter vector used to form λ\nβ or beta: the fixed-effects coefficient vector\nλ or lambda: a vector of lower triangular matrices repeated on the diagonal blocks of Λ\nσ or sigma: current value of the standard deviation of the per-observation noise\nb: random effects on the original scale, as a vector of matrices\nu: random effects on the orthogonal scale, as a vector of matrices\nlowerbd: lower bounds on the elements of θ\nX: the fixed-effects model matrix\ny: the response vector\n\n\n\n\n\n"
},

{
    "location": "constructors/#Model-constructors-1",
    "page": "Model constructors",
    "title": "Model constructors",
    "category": "section",
    "text": "The LinearMixedModel type represents a linear mixed-effects model. Typically it is constructed from a Formula and an appropriate data type, usually a DataFrame.LinearMixedModel"
},

{
    "location": "constructors/#Examples-of-linear-mixed-effects-model-fits-1",
    "page": "Model constructors",
    "title": "Examples of linear mixed-effects model fits",
    "category": "section",
    "text": "For illustration, several data sets from the lme4 package for R are made available in .rda format in this package. These include the Dyestuff and Dyestuff2 data sets.julia> using DataFrames, MixedModels, RData, StatsBase\n\njulia> const dat = Dict(Symbol(k)=>v for (k,v) in \n    load(joinpath(dirname(pathof(MixedModels)), \"..\", \"test\", \"dat.rda\")));\n\njulia> describe(dat[:Dyestuff])\n2×8 DataFrames.DataFrame. Omitted printing of 1 columns\n│ Row │ variable │ mean   │ min    │ median │ max    │ nunique │ nmissing │\n│     │ Symbol   │ Union… │ Any    │ Union… │ Any    │ Union…  │ Nothing  │\n├─────┼──────────┼────────┼────────┼────────┼────────┼─────────┼──────────┤\n│ 1   │ G        │        │ A      │        │ F      │ 6       │          │\n│ 2   │ Y        │ 1527.5 │ 1440.0 │ 1530.0 │ 1635.0 │         │          │\nThe columns in these data sets have been renamed for convenience. The response is always named Y. Potential grouping factors for random-effects terms are named G, H, etc. Numeric covariates are named starting with U. Categorical covariates not suitable as grouping factors are named starting with A."
},

{
    "location": "constructors/#Models-with-simple,-scalar-random-effects-1",
    "page": "Model constructors",
    "title": "Models with simple, scalar random effects",
    "category": "section",
    "text": "The formula language in Julia is similar to that in R except that the formula must be enclosed in a call to the @formula macro. A basic model with simple, scalar random effects for the levels of G (the batch of an intermediate product, in this case) is declared and fit asjulia> fm1 = fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G)), dat[:Dyestuff]))\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n              Column    Variance  Std.Dev. \n G        (Intercept)  1388.3333 37.260345\n Residual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)    1527.5   17.6946  86.326  <1e-99\n\n(If you are new to Julia you may find that this first fit takes an unexpectedly long time, due to Just-In-Time (JIT) compilation of the code. The second and subsequent calls to such functions are much faster.)julia> @time fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G)), dat[:Dyestuff2]));\n  0.004960 seconds (2.09 k allocations: 87.922 KiB)\nBy default, the model fit is by maximum likelihood.  To use the REML criterion instead, add the optional named argument REML = true to the call to fit!julia> fm1R = fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G)), dat[:Dyestuff]), REML=true)\nLinear mixed model fit by REML\n Formula: Y ~ 1 + (1 | G)\n REML criterion at convergence: 319.65427684225216\n\nVariance components:\n              Column    Variance  Std.Dev. \n G        (Intercept)  1764.0510 42.000607\n Residual              2451.2498 49.510098\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)    1527.5   19.3834 78.8045  <1e-99\n\n"
},

{
    "location": "constructors/#Simple,-scalar-random-effects-1",
    "page": "Model constructors",
    "title": "Simple, scalar random effects",
    "category": "section",
    "text": "A simple, scalar random effects term in a mixed-effects model formula is of the form (1|G). All random effects terms end with |G where G is the grouping factor for the random effect. The name or, more generally, the expression G should evaluate to a categorical array that has a distinct set of levels. The random effects are associated with the levels of the grouping factor.A scalar random effect is, as the name implies, one scalar value for each level of the grouping factor. A simple, scalar random effects term is of the form, (1|G). It corresponds to a shift in the intercept for each level of the grouping factor."
},

{
    "location": "constructors/#Models-with-vector-valued-random-effects-1",
    "page": "Model constructors",
    "title": "Models with vector-valued random effects",
    "category": "section",
    "text": "The sleepstudy data are observations of reaction time, Y, on several subjects, G, after 0 to 9 days of sleep deprivation, U. A model with random intercepts and random slopes for each subject, allowing for within-subject correlation of the slope and intercept, is fit asjulia> fm2 = fit(LinearMixedModel, @formula(Y ~ 1 + U + (1+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + U + ((1 + U) | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n              Column    Variance  Std.Dev.   Corr.\n G        (Intercept)  565.51067 23.780468\n          U             32.68212  5.716828  0.08\n Residual              654.94145 25.591824\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   251.405   6.63226 37.9064  <1e-99\nU             10.4673   1.50224 6.96781  <1e-11\n\nA model with uncorrelated random effects for the intercept and slope by subject is fit asjulia> fm3 = fit(LinearMixedModel, @formula(Y ~ 1 + U + (1|G) + (0+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + U + (1 | G) + ((0 + U) | G)\n   logLik   -2 logLik     AIC        BIC    \n -876.00163 1752.00326 1762.00326 1777.96804\n\nVariance components:\n              Column    Variance  Std.Dev.   Corr.\n G        (Intercept)  584.258973 24.17145\n          U             33.632805  5.79938  0.00\n Residual              653.115782 25.55613\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   251.405   6.70771   37.48  <1e-99\nU             10.4673   1.51931 6.88951  <1e-11\n\nAlthough technically there are two random-effects terms in the formula for fm3 both have the same grouping factor and, internally, are amalgamated into a single vector-valued term."
},

{
    "location": "constructors/#Models-with-multiple,-scalar-random-effects-terms-1",
    "page": "Model constructors",
    "title": "Models with multiple, scalar random-effects terms",
    "category": "section",
    "text": "A model for the Penicillin data incorporates random effects for the plate, G, and for the sample, H. As every sample is used on every plate these two factors are crossed.julia> fm4 = fit(LinearMixedModel, @formula(Y ~ 1 + (1|G) + (1|H)), dat[:Penicillin])\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G) + (1 | H)\n   logLik   -2 logLik     AIC        BIC    \n -166.09417  332.18835  340.18835  352.06760\n\nVariance components:\n              Column    Variance   Std.Dev. \n G        (Intercept)  0.71497949 0.8455646\n H        (Intercept)  3.13519326 1.7706477\n Residual              0.30242640 0.5499331\n Number of obs: 144; levels of grouping factors: 24, 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   22.9722  0.744596 30.8519  <1e-99\n\nIn contrast the sample, G, grouping factor is nested within the batch, H, grouping factor in the Pastes data. That is, each level of G occurs in conjunction with only one level of H.julia> fm5 = fit(LinearMixedModel, @formula(Y ~ 1 + (1|G) + (1|H)), dat[:Pastes])\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G) + (1 | H)\n   logLik   -2 logLik     AIC        BIC    \n -123.99723  247.99447  255.99447  264.37184\n\nVariance components:\n              Column    Variance  Std.Dev.  \n G        (Intercept)  8.4336167 2.90406899\n H        (Intercept)  1.1991787 1.09507018\n Residual              0.6780021 0.82340886\n Number of obs: 60; levels of grouping factors: 30, 10\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   60.0533  0.642136 93.5212  <1e-99\n\nIn observational studies it is common to encounter partially crossed grouping factors. For example, the InstEval data are course evaluations by students, G, of instructors, H. Additional covariates include the academic department, I, in which the course was given and A, whether or not it was a service course.julia> fm6 = fit(LinearMixedModel, @formula(Y ~ 1 + A * I + (1|G) + (1|H)), dat[:InstEval])\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + A + I + A & I + (1 | G) + (1 | H)\n     logLik        -2 logLik          AIC             BIC       \n -1.18792777×10⁵  2.37585553×10⁵  2.37647553×10⁵  2.37932876×10⁵\n\nVariance components:\n              Column    Variance   Std.Dev.  \n G        (Intercept)  0.10541790 0.32468122\n H        (Intercept)  0.25841634 0.50834667\n Residual              1.38472780 1.17674458\n Number of obs: 73421; levels of grouping factors: 2972, 1128\n\n  Fixed-effects parameters:\n                Estimate Std.Error  z value P(>|z|)\n(Intercept)      3.22961  0.064053  50.4209  <1e-99\nA: 1            0.252025 0.0686507  3.67112  0.0002\nI: 5            0.129536  0.101294  1.27882  0.2010\nI: 10          -0.176751 0.0881352 -2.00545  0.0449\nI: 12          0.0517102 0.0817523 0.632523  0.5270\nI: 6           0.0347319  0.085621 0.405647  0.6850\nI: 7             0.14594 0.0997984  1.46235  0.1436\nI: 4            0.151689 0.0816897  1.85689  0.0633\nI: 8            0.104206  0.118751 0.877517  0.3802\nI: 9           0.0440401 0.0962985  0.45733  0.6474\nI: 14          0.0517546 0.0986029 0.524879  0.5997\nI: 1           0.0466719  0.101942 0.457828  0.6471\nI: 3           0.0563461 0.0977925  0.57618  0.5645\nI: 11          0.0596536  0.100233 0.595151  0.5517\nI: 2          0.00556285  0.110867 0.050176  0.9600\nA: 1 & I: 5    -0.180757  0.123179 -1.46744  0.1423\nA: 1 & I: 10   0.0186492  0.110017 0.169512  0.8654\nA: 1 & I: 12   -0.282269 0.0792937  -3.5598  0.0004\nA: 1 & I: 6    -0.494464 0.0790278 -6.25684   <1e-9\nA: 1 & I: 7    -0.392054  0.110313 -3.55403  0.0004\nA: 1 & I: 4    -0.278547 0.0823727 -3.38154  0.0007\nA: 1 & I: 8    -0.189526  0.111449 -1.70056  0.0890\nA: 1 & I: 9    -0.499868 0.0885423 -5.64553   <1e-7\nA: 1 & I: 14   -0.497162 0.0917162 -5.42065   <1e-7\nA: 1 & I: 1     -0.24042 0.0982071  -2.4481  0.0144\nA: 1 & I: 3    -0.223013 0.0890548 -2.50422  0.0123\nA: 1 & I: 11   -0.516997 0.0809077 -6.38997   <1e-9\nA: 1 & I: 2    -0.384773  0.091843 -4.18946   <1e-4\n\n"
},

{
    "location": "constructors/#MixedModels.GeneralizedLinearMixedModel",
    "page": "Model constructors",
    "title": "MixedModels.GeneralizedLinearMixedModel",
    "category": "type",
    "text": "GeneralizedLinearMixedModel\n\nGeneralized linear mixed-effects model representation\n\nFields\n\nLMM: a LinearMixedModel - the local approximation to the GLMM.\nβ: the fixed-effects vector\nβ₀: similar to β. Used in the PIRLS algorithm if step-halving is needed.\nθ: covariance parameter vector\nb: similar to u, equivalent to broadcast!(*, b, LMM.Λ, u)\nu: a vector of matrices of random effects\nu₀: similar to u.  Used in the PIRLS algorithm if step-halving is needed.\nresp: a GlmResp object\nη: the linear predictor\nwt: vector of prior case weights, a value of T[] indicates equal weights.\n\nThe following fields are used in adaptive Gauss-Hermite quadrature, which applies only to models with a single random-effects term, in which case their lengths are the number of levels in the grouping factor for that term.  Otherwise they are zero-length vectors.\n\ndevc: vector of deviance components\ndevc0: vector of deviance components at offset of zero\nsd: approximate standard deviation of the conditional density\nmult: multiplier\n\nProperties\n\nIn addition to the fieldnames, the following names are also accessible through the . extractor\n\ntheta: synonym for θ\nbeta: synonym for β\nσ or sigma: common scale parameter (value is NaN for distributions without a scale parameter)\nlowerbd: vector of lower bounds on the combined elements of β and θ\nformula, trms, A, L, and optsum: fields of the LMM field\nX: fixed-effects model matrix\ny: response vector\n\n\n\n\n\n"
},

{
    "location": "constructors/#Fitting-generalized-linear-mixed-models-1",
    "page": "Model constructors",
    "title": "Fitting generalized linear mixed models",
    "category": "section",
    "text": "To create a GLMM representationGeneralizedLinearMixedModelthe distribution family for the response, and possibly the link function, must be specified.julia> verbaggform = @formula(r2 ~ 1 + a + g + b + s + m + (1|id) + (1|item));\n\njulia> gm1 = fit(GeneralizedLinearMixedModel, verbaggform, dat[:VerbAgg], Bernoulli())\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  Formula: r2 ~ 1 + a + g + b + s + m + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8135.8329\n\nVariance components:\n          Column     Variance   Std.Dev. \n id   (Intercept)  1.793480630 1.3392090\n item (Intercept)  0.117154495 0.3422784\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n              Estimate Std.Error  z value P(>|z|)\n(Intercept)   0.553223  0.385364  1.43558  0.1511\na            0.0574216 0.0167528  3.42759  0.0006\ng: M          0.320801  0.191207  1.67777  0.0934\nb: scold      -1.05979  0.184162 -5.75464   <1e-8\nb: shout       -2.1038  0.186521 -11.2792  <1e-28\ns: self       -1.05402  0.151197 -6.97116  <1e-11\nm: do        -0.707036   0.15101 -4.68205   <1e-5\n\nThe canonical link, which is GLM.LogitLink for the Bernoulli distribution, is used if no explicit link is specified.Note that, in keeping with convention in the GLM package, the distribution family for a binary (i.e. 0/1) response is the Bernoulli distribution. The Binomial distribution is only used when the response is the fraction of trials returning a positive, in which case the number of trials must be specified as the case weights."
},

{
    "location": "constructors/#Optional-arguments-to-fit!-1",
    "page": "Model constructors",
    "title": "Optional arguments to fit!",
    "category": "section",
    "text": "An alternative approach is to create the GeneralizedLinearMixedModel object then call fit! on it. In this form optional arguments fast and/or nAGQ can be passed to the optimization process.As the name implies, fast=true, provides a faster but somewhat less accurate fit. These fits may suffice for model comparisons.julia> gm1a = fit!(GeneralizedLinearMixedModel(verbaggform, dat[:VerbAgg], Bernoulli()), fast=true)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  Formula: r2 ~ 1 + a + g + b + s + m + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8136.1709\n\nVariance components:\n          Column    Variance   Std.Dev.  \n id   (Intercept)  1.79270002 1.33891748\n item (Intercept)  0.11875573 0.34460953\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n              Estimate Std.Error  z value P(>|z|)\n(Intercept)   0.548543  0.385673   1.4223  0.1549\na            0.0543802 0.0167462  3.24732  0.0012\ng: M          0.304244  0.191141  1.59172  0.1114\nb: scold      -1.01749  0.185216 -5.49352   <1e-7\nb: shout      -2.02067  0.187522 -10.7756  <1e-26\ns: self       -1.01255   0.15204 -6.65975  <1e-10\nm: do        -0.679102  0.151857 -4.47198   <1e-5\n\n\njulia> deviance(gm1a) - deviance(gm1)\n0.33801208853947173\n\njulia> @time fit(GeneralizedLinearMixedModel, verbaggform, dat[:VerbAgg], Bernoulli());\n 48.354810 seconds (50.44 M allocations: 425.030 MiB, 0.19% gc time)\n\njulia> @time fit!(GeneralizedLinearMixedModel(verbaggform, dat[:VerbAgg], Bernoulli()), fast=true);\n  1.455715 seconds (2.43 M allocations: 26.565 MiB, 0.50% gc time)\nThe optional argument nAGQ=k causes evaluation of the deviance function to use a k point adaptive Gauss-Hermite quadrature rule. This method only applies to models with a single, simple, scalar random-effects term, such asjulia> contraform = @formula(use ~ 1 + a + l + urb + (1|d))\nFormula: use ~ 1 + a + l + urb + (1 | d)\n\njulia> @time gm2 = fit!(GeneralizedLinearMixedModel(contraform, dat[:Contraception], Bernoulli()), nAGQ=9)\n  1.707273 seconds (10.05 M allocations: 155.271 MiB, 3.82% gc time)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 9)\n  Formula: use ~ 1 + a + l + urb + (1 | d)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 2413.3485\n\nVariance components:\n       Column    Variance   Std.Dev.  \n d (Intercept)  0.21548177 0.46420014\n\n Number of obs: 1934; levels of grouping factors: 60\n\nFixed-effects parameters:\n               Estimate  Std.Error  z value P(>|z|)\n(Intercept)    -1.68982   0.145706 -11.5975  <1e-30\na            -0.0265921 0.00782925 -3.39651  0.0007\nl: 1            1.10911   0.156852  7.07105  <1e-11\nl: 2            1.37627   0.173343  7.93957  <1e-14\nl: 3+            1.3453   0.177809  7.56598  <1e-13\nurb: Y          0.73235   0.118484  6.18102   <1e-9\n\n\njulia> @time deviance(fit!(GeneralizedLinearMixedModel(contraform, dat[:Contraception], Bernoulli()), nAGQ=9, fast=true))\n  0.129813 seconds (431.83 k allocations: 5.154 MiB)\n2413.663718869012\n\njulia> @time deviance(fit!(GeneralizedLinearMixedModel(contraform, dat[:Contraception], Bernoulli())))\n  0.689261 seconds (4.60 M allocations: 42.840 MiB, 1.82% gc time)\n2413.6156912345245\n\njulia> @time deviance(fit!(GeneralizedLinearMixedModel(contraform, dat[:Contraception], Bernoulli()), fast=true))\n  0.084874 seconds (225.54 k allocations: 3.042 MiB)\n2413.6618664984017\n"
},

{
    "location": "constructors/#Extractor-functions-1",
    "page": "Model constructors",
    "title": "Extractor functions",
    "category": "section",
    "text": "LinearMixedModel and GeneralizedLinearMixedModel are subtypes of StatsBase.RegressionModel which, in turn, is a subtype of StatsBase.StatisticalModel. Many of the generic extractors defined in the StatsBase package have methods for these models."
},

{
    "location": "constructors/#StatsBase.loglikelihood-Tuple{StatisticalModel}",
    "page": "Model constructors",
    "title": "StatsBase.loglikelihood",
    "category": "method",
    "text": "loglikelihood(obj::StatisticalModel)\n\nReturn the log-likelihood of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.aic",
    "page": "Model constructors",
    "title": "StatsBase.aic",
    "category": "function",
    "text": "aic(obj::StatisticalModel)\n\nAkaike\'s Information Criterion, defined as -2 log L + 2k, with L the likelihood of the model, and k its number of consumed degrees of freedom (as returned by dof).\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.bic",
    "page": "Model constructors",
    "title": "StatsBase.bic",
    "category": "function",
    "text": "bic(obj::StatisticalModel)\n\nBayesian Information Criterion, defined as -2 log L + k log n, with L the likelihood of the model,  k its number of consumed degrees of freedom (as returned by dof), and n the number of observations (as returned by nobs).\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.dof-Tuple{StatisticalModel}",
    "page": "Model constructors",
    "title": "StatsBase.dof",
    "category": "method",
    "text": "dof(obj::StatisticalModel)\n\nReturn the number of degrees of freedom consumed in the model, including when applicable the intercept and the distribution\'s dispersion parameter.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.nobs-Tuple{StatisticalModel}",
    "page": "Model constructors",
    "title": "StatsBase.nobs",
    "category": "method",
    "text": "nobs(obj::StatisticalModel)\n\nReturn the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.deviance-Tuple{StatisticalModel}",
    "page": "Model constructors",
    "title": "StatsBase.deviance",
    "category": "method",
    "text": "deviance(obj::StatisticalModel)\n\nReturn the deviance of the model relative to a reference, which is usually when applicable the saturated model. It is equal, up to a constant, to -2 log L, with L the likelihood of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.objective",
    "page": "Model constructors",
    "title": "MixedModels.objective",
    "category": "function",
    "text": "objective(m::LinearMixedModel)\n\nReturn negative twice the log-likelihood of model m or the REML criterion, according to the value of m.optsum.REML\n\n\n\n\n\n"
},

{
    "location": "constructors/#Model-fit-statistics-1",
    "page": "Model constructors",
    "title": "Model-fit statistics",
    "category": "section",
    "text": "The statistics describing the quality of the model fit includeloglikelihood(::StatisticalModel)\naic\nbic\ndof(::StatisticalModel)\nnobs(::StatisticalModel)julia> loglikelihood(fm1)\n-163.66352994056865\n\njulia> aic(fm1)\n333.3270598811373\n\njulia> bic(fm1)\n337.5306520261238\n\njulia> dof(fm1)   # 1 fixed effect, 2 variances\n3\n\njulia> nobs(fm1)  # 30 observations\n30\n\njulia> loglikelihood(gm1)\n-4067.916429808715\nIn general the deviance of a statistical model fit is negative twice the log-likelihood adjusting for the saturated model.deviance(::StatisticalModel)Because it is not clear what the saturated model corresponding to a particular LinearMixedModel should be, negative twice the log-likelihood is called the objective.objectiveThis value is also accessible as the deviance but the user should bear in mind that this doesn\'t have all the properties of a deviance which is corrected for the saturated model. For example, it is not necessarily non-negative.julia> objective(fm1)\n327.3270598811373\n\njulia> deviance(fm1)\n327.3270598811373\nThe value optimized when fitting a GeneralizedLinearMixedModel is the Laplace approximation to the deviance or an adaptive Gauss-Hermite evaluation.deviance!julia> MixedModels.deviance!(gm1)\n8135.832859617447\n"
},

{
    "location": "constructors/#StatsBase.coef",
    "page": "Model constructors",
    "title": "StatsBase.coef",
    "category": "function",
    "text": "coef(obj::StatisticalModel)\n\nReturn the coefficients of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.fixef",
    "page": "Model constructors",
    "title": "MixedModels.fixef",
    "category": "function",
    "text": "fixef(m::MixedModel, permuted=true)\n\nReturn the fixed-effects parameter vector estimate of m.\n\nIf permuted is true the vector elements are permuted according to m.trms[end - 1].piv and truncated to the rank of that term.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.vcov",
    "page": "Model constructors",
    "title": "StatsBase.vcov",
    "category": "function",
    "text": "vcov(obj::StatisticalModel)\n\nReturn the variance-covariance matrix for the coefficients of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.stderror",
    "page": "Model constructors",
    "title": "StatsBase.stderror",
    "category": "function",
    "text": "stderror(obj::StatisticalModel)\n\nReturn the standard errors for the coefficients of the model.\n\n\n\n\n\n"
},

{
    "location": "constructors/#StatsBase.coeftable",
    "page": "Model constructors",
    "title": "StatsBase.coeftable",
    "category": "function",
    "text": "coeftable(obj::StatisticalModel)\n\nReturn a table of class CoefTable with coefficients and related statistics.\n\n\n\n\n\n"
},

{
    "location": "constructors/#Fixed-effects-parameter-estimates-1",
    "page": "Model constructors",
    "title": "Fixed-effects parameter estimates",
    "category": "section",
    "text": "The coef and fixef extractors both return the maximum likelihood estimates of the fixed-effects coefficients.coef\nfixefjulia> show(coef(fm1))\n[1527.5]\njulia> show(fixef(fm1))\n[1527.5]\njulia> show(fixef(gm1))\n[0.0574216, -1.05402, -0.707036, -1.05979, 0.320801, -2.1038, 0.553223]An alternative extractor for the fixed-effects coefficient is the β property. Properties whose names are Greek letters usually have an alternative spelling, which is the name of the Greek letter.julia> show(fm1.β)\n[1527.5]\njulia> show(fm1.beta)\n[1527.5]\njulia> show(gm1.β)\n[0.0574216, -1.05402, -0.707036, -1.05979, 0.320801, -2.1038, 0.553223]The variance-covariance matrix of the fixed-effects coefficients is returned byvcovjulia> vcov(fm2)\n2×2 Array{Float64,2}:\n 43.9868   -1.37039\n -1.37039   2.25671\n\njulia> vcov(gm1)\n7×7 Array{Float64,2}:\n  0.148506    -0.00560462   -0.00977081   …  -0.0114554    -0.0114566  \n -0.00560462   0.000280655   7.19123e-5      -1.47964e-5   -1.02415e-5 \n -0.00977081   7.19123e-5    0.03656         -8.04385e-5   -5.25882e-5 \n -0.0169716   -1.43714e-5   -9.25614e-5       0.000265781   0.000172095\n -0.017144    -2.90564e-5   -0.000162389      0.000658924   0.000520519\n -0.0114554   -1.47964e-5   -8.04385e-5   …   0.0228606     0.00024777 \n -0.0114566   -1.02415e-5   -5.25882e-5       0.00024777    0.0228041  \nThe standard errors are the square roots of the diagonal elements of the estimated variance-covariance matrix of the fixed-effects coefficient estimators.stderrorjulia> show(StatsBase.stderror(fm2))\n[6.63226, 1.50224]\njulia> show(StatsBase.stderror(gm1))\n[0.385364, 0.0167528, 0.191207, 0.184162, 0.186521, 0.151197, 0.15101]Finally, the coeftable generic produces a table of coefficient estimates, their standard errors, and their ratio. The p-values quoted here should be regarded as approximations.coeftablejulia> coeftable(fm2)\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   251.405   6.63226 37.9064  <1e-99\nU             10.4673   1.50224 6.96781  <1e-11\n\n"
},

{
    "location": "constructors/#MixedModels.VarCorr",
    "page": "Model constructors",
    "title": "MixedModels.VarCorr",
    "category": "type",
    "text": "VarCorr\n\nAn encapsulation of information on the fitted random-effects variance-covariance matrices.\n\nMembers\n\nσ: a Vector{Vector{T}} of unscaled standard deviations\nρ: a Vector{Matrix{T}} of correlation matrices\nfnms: a Vector{Symbol} of grouping factor names\ncnms: a Vector{Vector{String}} of column names\ns: the estimate of σ, the standard deviation of the per-observation noise.  When there is no scaling factor this value is NaN\n\nThe main purpose of defining this type is to isolate the logic in the show method.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.varest",
    "page": "Model constructors",
    "title": "MixedModels.varest",
    "category": "function",
    "text": "varest(m::LinearMixedModel)\n\nReturns the estimate of σ², the variance of the conditional distribution of Y given B.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.sdest",
    "page": "Model constructors",
    "title": "MixedModels.sdest",
    "category": "function",
    "text": "sdest(m::LinearMixedModel)\n\nReturn the estimate of σ, the standard deviation of the per-observation noise.\n\n\n\n\n\n"
},

{
    "location": "constructors/#Covariance-parameter-estimates-1",
    "page": "Model constructors",
    "title": "Covariance parameter estimates",
    "category": "section",
    "text": "The covariance parameters estimates, in the form shown in the model summary, are a VarCorr objectVarCorrjulia> VarCorr(fm2)\nVariance components:\n              Column    Variance  Std.Dev.   Corr.\n G        (Intercept)  565.51067 23.780468\n          U             32.68212  5.716828  0.08\n Residual              654.94145 25.591824\n\n\njulia> VarCorr(gm1)\nVariance components:\n          Column     Variance   Std.Dev. \n id   (Intercept)  1.793480630 1.3392090\n item (Intercept)  0.117154495 0.3422784\n\nIndividual components are returned by other extractorsvarest\nsdestjulia> varest(fm2)\n654.941450830681\n\njulia> sdest(fm2)\n25.591823905901684\n\njulia> fm2.σ\n25.591823905901684\n"
},

{
    "location": "constructors/#MixedModels.ranef",
    "page": "Model constructors",
    "title": "MixedModels.ranef",
    "category": "function",
    "text": "ranef(m::MixedModel; uscale=false, named=true)\n\nReturn, as a Vector{Vector{T}} (Vector{NamedVector{T}} if named=true), the conditional modes of the random effects in model m.\n\nIf uscale is true the random effects are on the spherical (i.e. u) scale, otherwise on the original scale.\n\n\n\n\n\n"
},

{
    "location": "constructors/#MixedModels.condVar",
    "page": "Model constructors",
    "title": "MixedModels.condVar",
    "category": "function",
    "text": "condVar(m::MixedModel)\n\nReturn the conditional variances matrices of the random effects.\n\nThe random effects are returned by ranef as a vector of length k, where k is the number of random effects terms.  The ith element is a matrix of size vᵢ × ℓᵢ  where vᵢ is the size of the vector-valued random effects for each of the ℓᵢ levels of the grouping factor.  Technically those values are the modes of the conditional distribution of the random effects given the observed data.\n\nThis function returns an array of k three dimensional arrays, where the ith array is of size vᵢ × vᵢ × ℓᵢ.  These are the diagonal blocks from the conditional variance-covariance matrix,\n\ns² Λ(Λ\'Z\'ZΛ + I)⁻¹Λ\'\n\n\n\n\n\n"
},

{
    "location": "constructors/#Conditional-modes-of-the-random-effects-1",
    "page": "Model constructors",
    "title": "Conditional modes of the random effects",
    "category": "section",
    "text": "The ranef extractorranefjulia> ranef(fm1)\n1-element Array{Array{Float64,2},1}:\n [-16.6282 0.369516 … 53.5798 -42.4943]\n\njulia> fm1.b\n1-element Array{Array{Float64,2},1}:\n [-16.6282 0.369516 … 53.5798 -42.4943]\n\njulia> ranef(fm1, named=true)[1]\n1×6 Named Array{Float64,2}\n      A ╲ B │        A         B         C         D         E         F\n────────────┼───────────────────────────────────────────────────────────\n(Intercept) │ -16.6282  0.369516   26.9747  -21.8014   53.5798  -42.4943\nreturns the conditional modes of the random effects given the observed data. That is, these are the values that maximize the conditional density of the random effects given the observed data. For a LinearMixedModel these are also the conditional mean values.These are sometimes called the best linear unbiased predictors or BLUPs but that name is not particularly meaningful.At a superficial level these can be considered as the \"estimates\" of the random effects, with a bit of hand waving, but pursuing this analogy too far usually results in confusion.The corresponding conditional variances are returned bycondVarjulia> condVar(fm1)\n1-element Array{Array{Float64,3},1}:\n [362.31]\n\n[362.31]\n\n[362.31]\n\n[362.31]\n\n[362.31]\n\n[362.31]\n"
},

{
    "location": "optimization/#",
    "page": "Details of the parameter estimation",
    "title": "Details of the parameter estimation",
    "category": "page",
    "text": ""
},

{
    "location": "optimization/#Details-of-the-parameter-estimation-1",
    "page": "Details of the parameter estimation",
    "title": "Details of the parameter estimation",
    "category": "section",
    "text": ""
},

{
    "location": "optimization/#The-probability-model-1",
    "page": "Details of the parameter estimation",
    "title": "The probability model",
    "category": "section",
    "text": "Maximum likelihood estimates are based on the probability model for the observed responses. In the probability model the distribution of the responses is expressed as a function of one or more parameters.For a continuous distribution the probability density is a function of the responses, given the parameters. The likelihood function is the same expression as the probability density but regarding the observed values as fixed and the parameters as varying.In general a mixed-effects model incorporates two random variables: mathcalB, the q-dimensional vector of random effects, and mathcalY, the n-dimensional response vector. The value, bf y, of mathcalY is observed; the value, bf b, of mathcalB is not."
},

{
    "location": "optimization/#Linear-Mixed-Effects-Models-1",
    "page": "Details of the parameter estimation",
    "title": "Linear Mixed-Effects Models",
    "category": "section",
    "text": "In a linear mixed model the unconditional distribution of mathcalB and the conditional distribution, (mathcalY  mathcalB=bfb), are both multivariate Gaussian distributions,\\begin{equation} \\begin{aligned}   (\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\beta + Z b},\\sigma^2\\bf{I})\\\\\n  \\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_\\theta) . \\end{aligned} \\end{equation}The conditional mean of mathcal Y, given mathcal B=bf b, is the linear predictor, bf Xbfbeta+bf Zbf b, which depends on the p-dimensional fixed-effects parameter, bf beta, and on bf b. The model matrices, bf X and bf Z, of dimension ntimes p and ntimes q, respectively, are determined from the formula for the model and the values of covariates.  Although the matrix bf Z can be large (i.e. both n and q can be large), it is sparse (i.e. most of the elements in the matrix are zero).The relative covariance factor, Lambda_theta, is a qtimes q lower-triangular matrix, depending on the variance-component parameter, bftheta, and generating the symmetric qtimes q variance-covariance matrix, Sigma_theta, as\\begin{equation} \\Sigma\\theta=\\sigma^2\\Lambda\\theta\\Lambda_\\theta^\\prime \\end{equation}The spherical random effects, mathcalUsimmathcalN(bf 0sigma^2bf I_q), determine mathcal B according to\\begin{equation} \\mathcal{B}=\\Lambda_\\theta\\mathcal{U}. \\end{equation}The penalized residual sum of squares (PRSS),\\begin{equation} r^2(\\theta,\\beta,{\\bf u})=\\|{\\bf y} -{\\bf X}\\beta -{\\bf Z}\\Lambda_\\theta{\\bf u}\\|^2+\\|{\\bf u}\\|^2, \\end{equation}is the sum of the residual sum of squares, measuring fidelity of the model to the data, and a penalty on the size of bf u, measuring the complexity of the model. Minimizing r^2 with respect to bf u,\\begin{equation} r^2{\\beta,\\theta} =\\min{\\bf u}\\left(\\|{\\bf y} -{\\bf X}{\\beta} -{\\bf Z}\\Lambda_\\theta{\\bf u}\\|^2+\\|{\\bf u}\\|^2\\right) \\end{equation}is a direct (i.e. non-iterative) computation. The particular method used to solve this generates a blocked Choleksy factor, bf L_theta, which is an lower triangular qtimes q matrix satisfying\\begin{equation} {\\bf L}\\theta{\\bf L}\\theta\'=\\Lambda\\theta\'{\\bf Z}\'{\\bf Z}\\Lambda\\theta+{\\bf I}_q . \\end{equation}where bf I_q is the qtimes q identity matrix.Negative twice the log-likelihood of the parameters, given the data, bf y, is\\begin{equation} d({\\bf\\theta},{\\bf\\beta},\\sigma|{\\bf y}) =n\\log(2\\pi\\sigma^2)+\\log(|{\\bf L}\\theta|^2)+\\frac{r^2{\\beta,\\theta}}{\\sigma^2}. \\end{equation}where bf L_theta denotes the determinant of bf L_theta. Because bf L_theta is triangular, its determinant is the product of its diagonal elements.Because the conditional mean, bfmu_mathcal Ymathcal B=bf b=bf Xbfbeta+bf ZLambda_thetabf u, is a linear function of both bfbeta and bf u, minimization of the PRSS with respect to both bfbeta and bf u to produce\\begin{equation} r^2\\theta =\\min{{\\bf\\beta},{\\bf u}}\\left(\\|{\\bf y} -{\\bf X}{\\bf\\beta} -{\\bf Z}\\Lambda_\\theta{\\bf u}\\|^2+\\|{\\bf u}\\|^2\\right) \\end{equation}is also a direct calculation. The values of bf u and bfbeta that provide this minimum are called, respectively, the conditional mode, tildebf u_theta, of the spherical random effects and the conditional estimate, widehatbfbeta_theta, of the fixed effects. At the conditional estimate of the fixed effects the objective is\\begin{equation} d({\\bf\\theta},\\widehat{\\beta}\\theta,\\sigma|{\\bf y}) =n\\log(2\\pi\\sigma^2)+\\log(|{\\bf L}\\theta|^2)+\\frac{r^2_\\theta}{\\sigma^2}. \\end{equation}Minimizing this expression with respect to sigma^2 produces the conditional estimate\\begin{equation} \\widehat{\\sigma^2}\\theta=\\frac{r^2\\theta}{n} \\end{equation}which provides the profiled log-likelihood on the deviance scale as\\begin{equation} \\tilde{d}(\\theta|{\\bf y})=d(\\theta,\\widehat{\\beta}\\theta,\\widehat{\\sigma}\\theta|{\\bf y}) =\\log(|{\\bf L}\\theta|^2)+n\\left[1+\\log\\left(\\frac{2\\pi r^2\\theta}{n}\\right)\\right], \\end{equation}a function of bftheta alone.The MLE of bftheta, written widehatbftheta, is the value that minimizes this profiled objective. We determine this value by numerical optimization. In the process of evaluating tilded(widehatthetabf y) we determine widehatbeta=widehatbeta_widehattheta, tildebf u_widehattheta and r^2_widehattheta, from which we can evaluate widehatsigma=sqrtr^2_widehatthetan.The elements of the conditional mode of mathcal B, evaluated at the parameter estimates,\\begin{equation} \\tilde{\\bf b}{\\widehat{\\theta}}=\\Lambda{\\widehat{\\theta}}\\tilde{\\bf u}_{\\widehat{\\theta}} \\end{equation}are sometimes called the best linear unbiased predictors or BLUPs of the random effects. Although BLUPs an appealing acronym, I don’t find the term particularly instructive (what is a “linear unbiased predictor” and in what sense are these the “best”?) and prefer the term “conditional modes”, because these are the values of bf b that maximize the density of the conditional distribution mathcalB  mathcalY = bf y. For a linear mixed model, where all the conditional and unconditional distributions are Gaussian, these values are also the conditional means."
},

{
    "location": "optimization/#MixedModels.ScalarFactorReTerm",
    "page": "Details of the parameter estimation",
    "title": "MixedModels.ScalarFactorReTerm",
    "category": "type",
    "text": "ScalarFactorReTerm\n\nScalar random-effects term from a grouping factor\n\nMembers\n\nrefs: indices into levels for the grouping factor\nlevels: possible values of the grouping factor\nz: the raw random-effects model matrix as a vector.\nwtz: a weighted copy of z\nfnm: the name of the grouping factor as a Symbol\ncnm: the column name as a string\nΛ: the relative covariance multiplier\n\n\n\n\n\n"
},

{
    "location": "optimization/#MixedModels.VectorFactorReTerm",
    "page": "Details of the parameter estimation",
    "title": "MixedModels.VectorFactorReTerm",
    "category": "type",
    "text": "VectorFactorReTerm\n\nRandom-effects term from a grouping factor, model matrix and block pattern\n\nMembers\n\nrefs: indices into levels for the grouping factor\nlevels: possible values of the grouping factor\nz: the transposed raw random-effects model matrix\nwtz: a weighted copy of z\nwtzv: a view of wtz as a Vector{SVector{S,T}}\nfnm: the name of the grouping factor as a Symbol\ncnms: a Vector of column names (row names after transposition) of z\nblks: a Vector{Int} of block sizes within Λ\nΛ: the relative covariance factor\ninds: linear indices of θ elements in the relative covariance factor\n\n\n\n\n\n"
},

{
    "location": "optimization/#Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z-1",
    "page": "Details of the parameter estimation",
    "title": "Internal structure of Lambda_theta and bf Z",
    "category": "section",
    "text": "In the types of LinearMixedModel available through the MixedModels package, groups of random effects and the corresponding columns of the model matrix, bf Z, are associated with random-effects terms in the model formula.For the simple examplejulia> fm1 = fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G)), dat[:Dyestuff]))\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n              Column    Variance  Std.Dev. \n G        (Intercept)  1388.3333 37.260345\n Residual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)    1527.5   17.6946  86.326  <1e-99\n\nthe only random effects term in the formula is (1|G), a simple, scalar random-effects term.julia> t1 = fm1.trms[1]\nMixedModels.ScalarFactorReTerm{Float64,UInt8}(UInt8[0x01, 0x01, 0x01, 0x01, 0x01, 0x02, 0x02, 0x02, 0x02, 0x02  …  0x05, 0x05, 0x05, 0x05, 0x05, 0x06, 0x06, 0x06, 0x06, 0x06], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], :G, [\"(Intercept)\"], 0.7525806932030558)\nScalarFactorReTermThis ScalarFactorReTerm contributes a block of columns to the model matrix bf Z and a diagonal block to Lambda_theta.julia> getθ(t1)\n1-element Array{Float64,1}:\n 0.7525806932030558\n\njulia> getΛ(t1)\n0.7525806932030558\n\njulia> Matrix(t1)\n30×6 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n ⋮                        ⋮  \n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\nBecause there is only one random-effects term in the model, the matrix bf Z is the indicators matrix shown as the result of Matrix(t1), but stored in a special sparse format. Furthermore, there is only one block in Lambda_theta. For a ScalarFactorReTerm this block is a multiple of the identity, in this case 075258cdotbf I_6.For a vector-valued random-effects term, as injulia> fm2 = fit(LinearMixedModel, @formula(Y ~ 1 + U + (1+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + U + ((1 + U) | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n              Column    Variance  Std.Dev.   Corr.\n G        (Intercept)  565.51067 23.780468\n          U             32.68212  5.716828  0.08\n Residual              654.94145 25.591824\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   251.405   6.63226 37.9064  <1e-99\nU             10.4673   1.50224 6.96781  <1e-11\n\n\njulia> t21 = fm2.trms[1]\nMixedModels.VectorFactorReTerm{Float64,UInt8,2}(UInt8[0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01  …  0x12, 0x12, 0x12, 0x12, 0x12, 0x12, 0x12, 0x12, 0x12, 0x12], [\"308\", \"309\", \"310\", \"330\", \"331\", \"332\", \"333\", \"334\", \"335\", \"337\", \"349\", \"350\", \"351\", \"352\", \"369\", \"370\", \"371\", \"372\"], [1.0 1.0 … 1.0 1.0; 0.0 1.0 … 8.0 9.0], [1.0 1.0 … 1.0 1.0; 0.0 1.0 … 8.0 9.0], StaticArrays.SArray{Tuple{2},Float64,1,2}[[1.0, 0.0], [1.0, 1.0], [1.0, 2.0], [1.0, 3.0], [1.0, 4.0], [1.0, 5.0], [1.0, 6.0], [1.0, 7.0], [1.0, 8.0], [1.0, 9.0]  …  [1.0, 0.0], [1.0, 1.0], [1.0, 2.0], [1.0, 3.0], [1.0, 4.0], [1.0, 5.0], [1.0, 6.0], [1.0, 7.0], [1.0, 8.0], [1.0, 9.0]], :G, [\"(Intercept)\", \"U\"], [2], [0.929221 0.0; 0.0181684 0.222645], [1, 2, 4])\nthe random-effects term (1+U|G) generates aVectorFactorReTermThe model matrix bf Z for this model isjulia> convert(Array{Int}, Matrix(t21))  # convert to integers for more compact printing\n180×36 Array{Int64,2}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\nand Lambda_theta is a 36times36 block diagonal matrix with 18 diagonal blocks, all of the formjulia> getΛ(t21)\n2×2 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:\n 0.929221    ⋅      \n 0.0181684  0.222645\nThe theta vector isjulia> getθ(t21)\n3-element Array{Float64,1}:\n 0.929221316877856   \n 0.018168376276495105\n 0.22264487411010955 \nRandom-effects terms in the model formula that have the same grouping factor are amagamated into a single VectorFactorReTerm object.julia> fm3 = fit(LinearMixedModel, @formula(Y ~ 1 + U + (1|G) + (0+U|G)), dat[:sleepstudy])\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + U + (1 | G) + ((0 + U) | G)\n   logLik   -2 logLik     AIC        BIC    \n -876.00163 1752.00326 1762.00326 1777.96804\n\nVariance components:\n              Column    Variance  Std.Dev.   Corr.\n G        (Intercept)  584.258973 24.17145\n          U             33.632805  5.79938  0.00\n Residual              653.115782 25.55613\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   251.405   6.70771   37.48  <1e-99\nU             10.4673   1.51931 6.88951  <1e-11\n\n\njulia> t31 = fm3.trms[1]\nMixedModels.VectorFactorReTerm{Float64,UInt8,2}(UInt8[0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01  …  0x12, 0x12, 0x12, 0x12, 0x12, 0x12, 0x12, 0x12, 0x12, 0x12], [\"308\", \"309\", \"310\", \"330\", \"331\", \"332\", \"333\", \"334\", \"335\", \"337\", \"349\", \"350\", \"351\", \"352\", \"369\", \"370\", \"371\", \"372\"], [1.0 1.0 … 1.0 1.0; 0.0 1.0 … 8.0 9.0], [1.0 1.0 … 1.0 1.0; 0.0 1.0 … 8.0 9.0], StaticArrays.SArray{Tuple{2},Float64,1,2}[[1.0, 0.0], [1.0, 1.0], [1.0, 2.0], [1.0, 3.0], [1.0, 4.0], [1.0, 5.0], [1.0, 6.0], [1.0, 7.0], [1.0, 8.0], [1.0, 9.0]  …  [1.0, 0.0], [1.0, 1.0], [1.0, 2.0], [1.0, 3.0], [1.0, 4.0], [1.0, 5.0], [1.0, 6.0], [1.0, 7.0], [1.0, 8.0], [1.0, 9.0]], :G, [\"(Intercept)\", \"U\"], [1, 1], [0.945818 0.0; 0.0 0.226927], [1, 4])\nFor this model the matrix bf Z is the same as that of model fm2 but the diagonal blocks of Lambda_theta are themselves diagonal.julia> getΛ(t31)\n2×2 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:\n 0.945818   ⋅      \n 0.0       0.226927\n\njulia> getθ(t31)\n2-element Array{Float64,1}:\n 0.9458180688242811\n 0.2269271487186899\nRandom-effects terms with distinct grouping factors generate distinct elements of the trms member of the LinearMixedModel object. Multiple AbstractFactorReTerm (i.e. either a ScalarFactorReTerm or a VectorFactorReTerm) objects are sorted by decreasing numbers of random effects.julia> fm4 = fit!(LinearMixedModel(@formula(Y ~ 1 + (1|H) + (1|G)), dat[:Penicillin]))\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | H) + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -166.09417  332.18835  340.18835  352.06760\n\nVariance components:\n              Column    Variance   Std.Dev. \n G        (Intercept)  0.71497949 0.8455646\n H        (Intercept)  3.13519326 1.7706477\n Residual              0.30242640 0.5499331\n Number of obs: 144; levels of grouping factors: 24, 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   22.9722  0.744596 30.8519  <1e-99\n\n\njulia> t41 = fm4.trms[1]\nMixedModels.ScalarFactorReTerm{Float64,UInt8}(UInt8[0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x02, 0x02, 0x02, 0x02  …  0x17, 0x17, 0x17, 0x17, 0x18, 0x18, 0x18, 0x18, 0x18, 0x18], [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"  …  \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\"], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], :G, [\"(Intercept)\"], 1.5375772478878553)\n\njulia> t42 = fm4.trms[2]\nMixedModels.ScalarFactorReTerm{Float64,UInt8}(UInt8[0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x01, 0x02, 0x03, 0x04  …  0x03, 0x04, 0x05, 0x06, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], :H, [\"(Intercept)\"], 3.2197511648538444)\nNote that the first ScalarFactorReTerm in fm4.trms corresponds to grouping factor G even though the term (1|G) occurs in the formula after (1|H)."
},

{
    "location": "optimization/#MixedModels.OptSummary",
    "page": "Details of the parameter estimation",
    "title": "MixedModels.OptSummary",
    "category": "type",
    "text": "OptSummary\n\nSummary of an NLopt optimization\n\nFields\n\ninitial: a copy of the initial parameter values in the optimization\nlowerbd: lower bounds on the parameter values\nftol_rel: as in NLopt\nftol_abs: as in NLopt\nxtol_rel: as in NLopt\nxtol_abs: as in NLopt\ninitial_step: as in NLopt\nmaxfeval: as in NLopt\nfinal: a copy of the final parameter values from the optimization\nfmin: the final value of the objective\nfeval: the number of function evaluations\noptimizer: the name of the optimizer used, as a Symbol\nreturnvalue: the return value, as a Symbol\nnAGQ: number of adaptive Gauss-Hermite quadrature points in deviance evaluation for GLMMs\nREML: use the REML criterion for LMM fits\n\nThe latter field doesn\'t really belong here but it has to be in a mutable struct in case it is changed.\n\n\n\n\n\n"
},

{
    "location": "optimization/#Progress-of-the-optimization-1",
    "page": "Details of the parameter estimation",
    "title": "Progress of the optimization",
    "category": "section",
    "text": "An optional named argument, verbose=true, in the call to fit! of a LinearMixedModel causes printing of the objective and the theta parameter at each evaluation during the optimization.julia> fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G)), dat[:Dyestuff]), verbose=true);\nf_1: 327.76702 [1.0]\nf_2: 331.03619 [1.75]\nf_3: 330.64583 [0.25]\nf_4: 327.69511 [0.97619]\nf_5: 327.56631 [0.928569]\nf_6: 327.3826 [0.833327]\nf_7: 327.35315 [0.807188]\nf_8: 327.34663 [0.799688]\nf_9: 327.341 [0.792188]\nf_10: 327.33253 [0.777188]\nf_11: 327.32733 [0.747188]\nf_12: 327.32862 [0.739688]\nf_13: 327.32706 [0.752777]\nf_14: 327.32707 [0.753527]\nf_15: 327.32706 [0.752584]\nf_16: 327.32706 [0.752509]\nf_17: 327.32706 [0.752591]\nf_18: 327.32706 [0.752581]\n\njulia> fit!(LinearMixedModel(@formula(Y ~ 1 + U + (1+U|G)), dat[:sleepstudy]), verbose=true);\nf_1: 1784.6423 [1.0, 0.0, 1.0]\nf_2: 1790.12564 [1.75, 0.0, 1.0]\nf_3: 1798.99962 [1.0, 1.0, 1.0]\nf_4: 1803.8532 [1.0, 0.0, 1.75]\nf_5: 1800.61398 [0.25, 0.0, 1.0]\nf_6: 1798.60463 [1.0, -1.0, 1.0]\nf_7: 1752.26074 [1.0, 0.0, 0.25]\nf_8: 1797.58769 [1.18326, -0.00866189, 0.0]\nf_9: 1754.95411 [1.075, 0.0, 0.325]\nf_10: 1753.69568 [0.816632, 0.0111673, 0.288238]\nf_11: 1754.817 [1.0, -0.0707107, 0.196967]\nf_12: 1753.10673 [0.943683, 0.0638354, 0.262696]\nf_13: 1752.93938 [0.980142, -0.0266568, 0.274743]\nf_14: 1752.25688 [0.984343, -0.0132347, 0.247191]\nf_15: 1752.05745 [0.97314, 0.00253785, 0.23791]\nf_16: 1752.02239 [0.954526, 0.00386421, 0.235892]\nf_17: 1752.02273 [0.935929, 0.0013318, 0.234445]\nf_18: 1751.97169 [0.954965, 0.00790664, 0.229046]\nf_19: 1751.9526 [0.953313, 0.0166274, 0.225768]\nf_20: 1751.94852 [0.946929, 0.0130761, 0.222871]\nf_21: 1751.98718 [0.933418, 0.00613767, 0.218951]\nf_22: 1751.98321 [0.951544, 0.005789, 0.220618]\nf_23: 1751.95197 [0.952809, 0.0190332, 0.224178]\nf_24: 1751.94628 [0.946322, 0.0153739, 0.225088]\nf_25: 1751.9467 [0.947124, 0.0148894, 0.224892]\nf_26: 1751.94757 [0.946497, 0.0154643, 0.225814]\nf_27: 1751.94531 [0.946086, 0.0157934, 0.224449]\nf_28: 1751.94418 [0.945304, 0.0166902, 0.223361]\nf_29: 1751.94353 [0.944072, 0.0172106, 0.222716]\nf_30: 1751.94244 [0.941271, 0.0163099, 0.222523]\nf_31: 1751.94217 [0.939, 0.015899, 0.222132]\nf_32: 1751.94237 [0.938979, 0.016548, 0.221562]\nf_33: 1751.94228 [0.938863, 0.0152466, 0.222683]\nf_34: 1751.9422 [0.938269, 0.015733, 0.222024]\nf_35: 1751.94131 [0.938839, 0.0166373, 0.222611]\nf_36: 1751.94093 [0.938397, 0.0173965, 0.222817]\nf_37: 1751.94057 [0.937006, 0.0180445, 0.222534]\nf_38: 1751.94018 [0.934109, 0.0187354, 0.22195]\nf_39: 1751.94008 [0.932642, 0.0189242, 0.221726]\nf_40: 1751.94027 [0.931357, 0.0190082, 0.221309]\nf_41: 1751.9415 [0.932821, 0.0206454, 0.221367]\nf_42: 1751.93949 [0.931867, 0.0179574, 0.222564]\nf_43: 1751.93939 [0.929167, 0.0177824, 0.222534]\nf_44: 1751.9394 [0.929659, 0.0177721, 0.222508]\nf_45: 1751.93943 [0.929193, 0.0187806, 0.22257]\nf_46: 1751.93935 [0.928986, 0.0182366, 0.222484]\nf_47: 1751.93949 [0.928697, 0.0182937, 0.223175]\nf_48: 1751.93936 [0.928243, 0.0182695, 0.222584]\nf_49: 1751.93934 [0.929113, 0.0181791, 0.222624]\nf_50: 1751.93934 [0.929191, 0.0181658, 0.222643]\nf_51: 1751.93935 [0.929254, 0.0182093, 0.222621]\nf_52: 1751.93935 [0.929189, 0.0181298, 0.222573]\nf_53: 1751.93934 [0.929254, 0.0181676, 0.22265]\nf_54: 1751.93934 [0.929215, 0.0181717, 0.222647]\nf_55: 1751.93934 [0.929208, 0.0181715, 0.222646]\nf_56: 1751.93934 [0.929209, 0.018173, 0.222652]\nf_57: 1751.93934 [0.929221, 0.0181684, 0.222645]\nA shorter summary of the optimization process is always available as anOptSummaryobject, which is the optsum member of the LinearMixedModel.julia> fm2.optsum\nInitial parameter vector: [1.0, 0.0, 1.0]\nInitial objective value:  1784.6422961924507\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [0.0, -Inf, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 1.0, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     57\nFinal parameter vector:   [0.929221, 0.0181684, 0.222645]\nFinal objective value:    1751.9393444646757\nReturn code:              FTOL_REACHED\n\n"
},

{
    "location": "optimization/#Modifying-the-optimization-process-1",
    "page": "Details of the parameter estimation",
    "title": "Modifying the optimization process",
    "category": "section",
    "text": "The OptSummary object contains both input and output fields for the optimizer. To modify the optimization process the input fields can be changed after constructing the model but before fitting it.Suppose, for example, that the user wishes to try a Nelder-Mead optimization method instead of the default BOBYQA (Bounded Optimization BY Quadratic Approximation) method.julia> fm2 = LinearMixedModel(@formula(Y ~ 1 + U + (1+U|G)), dat[:sleepstudy]);\n\njulia> fm2.optsum.optimizer = :LN_NELDERMEAD;\n\njulia> fit!(fm2)\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + U + ((1 + U) | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n              Column    Variance   Std.Dev.   Corr.\n G        (Intercept)  565.528831 23.780850\n          U             32.681047  5.716734  0.08\n Residual              654.941678 25.591828\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   251.405   6.63233  37.906  <1e-99\nU             10.4673   1.50222  6.9679  <1e-11\n\n\njulia> fm2.optsum\nInitial parameter vector: [1.0, 0.0, 1.0]\nInitial objective value:  1784.6422961924507\n\nOptimizer (from NLopt):   LN_NELDERMEAD\nLower bounds:             [0.0, -Inf, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 1.0, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     140\nFinal parameter vector:   [0.929236, 0.0181688, 0.222641]\nFinal objective value:    1751.9393444749974\nReturn code:              FTOL_REACHED\n\nThe parameter estimates are quite similar to those using :LN_BOBYQA but at the expense of 140 functions evaluations for :LN_NELDERMEAD versus 57 for :LN_BOBYQA.See the documentation for the NLopt package for details about the various settings."
},

{
    "location": "optimization/#Convergence-to-singular-covariance-matrices-1",
    "page": "Details of the parameter estimation",
    "title": "Convergence to singular covariance matrices",
    "category": "section",
    "text": "To ensure identifiability of Sigma_theta=sigma^2Lambda_theta Lambda_theta, the elements of theta corresponding to diagonal elements of Lambda_theta are constrained to be non-negative. For example, in a trivial case of a single, simple, scalar, random-effects term as in fm1, the one-dimensional theta vector is the ratio of the standard deviation of the random effects to the standard deviation of the response. It happens that -theta produces the same log-likelihood but, by convention, we define the standard deviation to be the positive square root of the variance. Requiring the diagonal elements of Lambda_theta to be non-negative is a generalization of using this positive square root.If the optimization converges on the boundary of the feasible region, that is if one or more of the diagonal elements of Lambda_theta is zero at convergence, the covariance matrix Sigma_theta will be singular. This means that there will be linear combinations of random effects that are constant. Usually convergence to a singular covariance matrix is a sign of an over-specified model."
},

{
    "location": "optimization/#Distributions.Bernoulli",
    "page": "Details of the parameter estimation",
    "title": "Distributions.Bernoulli",
    "category": "type",
    "text": "Bernoulli(p)\n\nA Bernoulli distribution is parameterized by a success rate p, which takes value 1 with probability p and 0 with probability 1-p.\n\nP(X = k) = begincases\n1 - p  quad textfor  k = 0 \np  quad textfor  k = 1\nendcases\n\nBernoulli()    # Bernoulli distribution with p = 0.5\nBernoulli(p)   # Bernoulli distribution with success rate p\n\nparams(d)      # Get the parameters, i.e. (p,)\nsuccprob(d)    # Get the success rate, i.e. p\nfailprob(d)    # Get the failure rate, i.e. 1 - p\n\nExternal links:\n\nBernoulli distribution on Wikipedia\n\n\n\n\n\n"
},

{
    "location": "optimization/#Distributions.Poisson",
    "page": "Details of the parameter estimation",
    "title": "Distributions.Poisson",
    "category": "type",
    "text": "Poisson(λ)\n\nA Poisson distribution descibes the number of independent events occurring within a unit time interval, given the average rate of occurrence λ.\n\nP(X = k) = fraclambda^kk e^-lambda quad text for  k = 012ldots\n\nPoisson()        # Poisson distribution with rate parameter 1\nPoisson(lambda)       # Poisson distribution with rate parameter lambda\n\nparams(d)        # Get the parameters, i.e. (λ,)\nmean(d)          # Get the mean arrival rate, i.e. λ\n\nExternal links:\n\nPoisson distribution on Wikipedia\n\n\n\n\n\n"
},

{
    "location": "optimization/#Generalized-Linear-Mixed-Effects-Models-1",
    "page": "Details of the parameter estimation",
    "title": "Generalized Linear Mixed-Effects Models",
    "category": "section",
    "text": "In a generalized linear model the responses are modelled as coming from a particular distribution, such as Bernoulli for binary responses or Poisson for responses that represent counts. The scalar distributions of individual responses differ only in their means, which are determined by a linear predictor expression eta=bf Xbeta, where, as before, bf X is a model matrix derived from the values of covariates and beta is a vector of coefficients.The unconstrained components of eta are mapped to the, possiby constrained, components of the mean response, mu, via a scalar function, g^-1, applied to each component of eta. For historical reasons, the inverse of this function, taking components of mu to the corresponding component of eta is called the link function and more frequently used map from eta to mu is the inverse link.A generalized linear mixed-effects model (GLMM) is defined, for the purposes of this package, by\\begin{equation} \\begin{aligned}   (\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\beta + Z b)},\\phi)\\\\\n  \\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_\\theta) . \\end{aligned} \\end{equation}where mathcalD indicates the distribution family parameterized by the mean and, when needed, a common scale parameter, phi. (There is no scale parameter for Bernoulli or for Poisson. Specifying the mean completely determines the distribution.)Bernoulli\nPoissonA GeneralizedLinearMixedModel object is generated from a formula, data frame and distribution family.julia> mdl = GeneralizedLinearMixedModel(@formula(r2 ~ 1 + a + g + b + s + (1|id) + (1|item)),\n           dat[:VerbAgg], Bernoulli());\n\njulia> typeof(mdl)\nMixedModels.GeneralizedLinearMixedModel{Float64}\nA separate call to fit! is required to fit the model. This involves optimizing an objective function, the Laplace approximation to the deviance, with respect to the parameters, which are beta, the fixed-effects coefficients, and theta, the covariance parameters. The starting estimate for beta is determined by fitting a GLM to the fixed-effects part of the formulajulia> mdl.β\n6-element Array{Float64,1}:\n  0.039940376051149765\n -0.7766556048305931  \n -0.7941857249205394  \n  0.23131667674984369 \n -1.5391882085456954  \n  0.2060530221032335  \nand the starting estimate for theta, which is a vector of the two standard deviations of the random effects, is chosen to bejulia> mdl.θ\n2-element Array{Float64,1}:\n 1.0\n 1.0\nThe Laplace approximation to the deviance requires determining the conditional modes of the random effects. These are the values that maximize the conditional density of the random effects, given the model parameters and the data. This is done using Penalized Iteratively Reweighted Least Squares (PIRLS). In most cases PIRLS is fast and stable. It is simply a penalized version of the IRLS algorithm used in fitting GLMs.The distinction between the \"fast\" and \"slow\" algorithms in the MixedModels package (nAGQ=0 or nAGQ=1 in lme4) is whether the fixed-effects parameters, beta, are optimized in PIRLS or in the nonlinear optimizer. In a call to the pirls! function the first argument is a GeneralizedLinearMixedModel, which is modified during the function call. (By convention, the names of such mutating functions end in ! as a warning to the user that they can modify an argument, usually the first argument.) The second and third arguments are optional logical values indicating if beta is to be varied and if verbose output is to be printed.julia> pirls!(mdl, true, true)\nvaryβ = true\nobj₀ = 10210.853438905406\nβ = [0.0399404, -0.776656, -0.794186, 0.231317, -1.53919, 0.206053]\niter = 1\nobj = 8301.483049027265\niter = 2\nobj = 8205.604285133919\niter = 3\nobj = 8201.89659746689\niter = 4\nobj = 8201.848598910705\niter = 5\nobj = 8201.848559060703\niter = 6\nobj = 8201.848559060623\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  Formula: r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8201.8486\n\nVariance components:\n          Column   Variance Std.Dev. \n id   (Intercept)         1        1\n item (Intercept)         1        1\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n              Estimate Std.Error  z value P(>|z|)\n(Intercept)   0.218535  0.491968 0.444206  0.6569\na            0.0514385 0.0130432  3.94371   <1e-4\ng: M          0.290225  0.148818   1.9502  0.0512\nb: scold     -0.979124  0.504402 -1.94116  0.0522\nb: shout      -1.95402  0.505235 -3.86754  0.0001\ns: self      -0.979493  0.412168 -2.37644  0.0175\n\njulia> deviance(mdl)\n8201.848559060623\njulia> mdl.β\n6-element Array{Float64,1}:\n  0.051438542580815434\n -0.9794925718038259  \n -0.9791237061900352  \n  0.29022454166301187 \n -1.9540167628141156  \n  0.21853493716522268 \njulia> mdl.θ # current values of the standard deviations of the random effects\n2-element Array{Float64,1}:\n 1.0\n 1.0\nIf the optimization with respect to beta is performed within PIRLS then the nonlinear optimization of the Laplace approximation to the deviance requires optimization with respect to theta only. This is the \"fast\" algorithm. Given a value of theta, PIRLS is used to determine the conditional estimate of beta and the conditional mode of the random effects, b.julia> mdl.b # conditional modes of b\n2-element Array{Array{Float64,2},1}:\n [-0.600772 -1.93227 … -0.144554 -0.575224]\n [-0.186364 0.180552 … 0.282092 -0.221974] \njulia> fit!(mdl, fast=true, verbose=true);\nvaryβ = true\nobj₀ = 10251.003116042984\nβ = [0.0514385, -0.979493, -0.979124, 0.290225, -1.95402, 0.218535]\niter = 1\nobj = 8292.390783437775\niter = 2\nobj = 8204.692089323946\niter = 3\nobj = 8201.87681054392\niter = 4\nobj = 8201.848569551961\niter = 5\nobj = 8201.848559060627\niter = 6\nobj = 8201.848559060621\nvaryβ = true\nobj₀ = 10251.003116042953\nβ = [0.0514385, -0.979493, -0.979124, 0.290225, -1.95402, 0.218535]\niter = 1\nobj = 8292.39078343777\niter = 2\nobj = 8204.692089323944\niter = 3\nobj = 8201.876810543918\niter = 4\nobj = 8201.848569551961\niter = 5\nobj = 8201.848559060627\niter = 6\nobj = 8201.848559060621\nThe optimization process is summarized byjulia> mdl.LMM.optsum\nInitial parameter vector: [1.0, 1.0]\nInitial objective value:  8201.848559060621\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [0.0, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     1\nFinal parameter vector:   [1.0, 1.0]\nFinal objective value:    0.0\nReturn code:              FORCED_STOP\n\nAs one would hope, given the name of the option, this fit is comparatively fast.julia> @time(fit!(GeneralizedLinearMixedModel(@formula(r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)), \n        dat[:VerbAgg], Bernoulli()), fast=true))\n  1.370537 seconds (2.12 M allocations: 24.042 MiB, 0.40% gc time)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  Formula: r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8151.5833\n\nVariance components:\n          Column    Variance   Std.Dev. \n id   (Intercept)  1.79443144 1.3395639\n item (Intercept)  0.24684282 0.4968328\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n              Estimate Std.Error  z value P(>|z|)\n(Intercept)   0.208273  0.405425 0.513715  0.6075\na            0.0543791 0.0167533  3.24587  0.0012\ng: M          0.304089  0.191223  1.59023  0.1118\nb: scold       -1.0165  0.257531 -3.94708   <1e-4\nb: shout       -2.0218  0.259235 -7.79912  <1e-14\ns: self       -1.01344  0.210888 -4.80559   <1e-5\n\nThe alternative algorithm is to use PIRLS to find the conditional mode of the random effects, given beta and theta and then use the general nonlinear optimizer to fit with respect to both beta and theta. Because it is slower to incorporate the beta parameters in the general nonlinear optimization, the fast fit is performed first and used to determine starting estimates for the more general optimization.julia> @time mdl1 = fit!(GeneralizedLinearMixedModel(@formula(r2 ~ 1+a+g+b+s+(1|id)+(1|item)), \n        dat[:VerbAgg], Bernoulli()))\n 61.171357 seconds (63.04 M allocations: 548.014 MiB, 0.15% gc time)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  Formula: r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 8151.3997\n\nVariance components:\n          Column    Variance   Std.Dev. \n id   (Intercept)  1.79484880 1.3397197\n item (Intercept)  0.24532098 0.4952989\n\n Number of obs: 7584; levels of grouping factors: 316, 24\n\nFixed-effects parameters:\n              Estimate Std.Error  z value P(>|z|)\n(Intercept)   0.198989  0.405179 0.491114  0.6233\na            0.0574285 0.0167574  3.42705  0.0006\ng: M          0.320731   0.19126  1.67694  0.0936\nb: scold      -1.05884  0.256803 -4.12316   <1e-4\nb: shout      -2.10547  0.258526 -8.14412  <1e-15\ns: self       -1.05523    0.2103 -5.01774   <1e-6\n\nThis fit provided slightly better results (Laplace approximation to the deviance of 8151.400 versus 8151.583) but took 6 times as long. That is not terribly important when the times involved are a few seconds but can be important when the fit requires many hours or days of computing time.The comparison of the slow and fast fit is available in the optimization summary after the slow fit.julia> mdl1.LMM.optsum\nInitial parameter vector: [0.0543791, -1.01344, -1.0165, 0.304089, -2.0218, 0.208273, 1.33956, 0.496833]\nInitial objective value:  8151.583340131868\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [-Inf, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10]\ninitial_step:             [0.135142, 0.00558444, 0.0637411, 0.0858438, 0.0864116, 0.0702961, 0.05, 0.05]\nmaxfeval:                 -1\n\nFunction evaluations:     1100\nFinal parameter vector:   [0.0574285, -1.05523, -1.05884, 0.320731, -2.10547, 0.198989, 1.33972, 0.495299]\nFinal objective value:    8151.399721088063\nReturn code:              FTOL_REACHED\n\n"
},

{
    "location": "GaussHermite/#",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "Normalized Gauss-Hermite Quadrature",
    "category": "page",
    "text": ""
},

{
    "location": "GaussHermite/#Normalized-Gauss-Hermite-Quadrature-1",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "Normalized Gauss-Hermite Quadrature",
    "category": "section",
    "text": "Gaussian Quadrature rules provide sets of x values, called abscissae, and weights, w, to approximate an integral with respect to a weight function, g(x). For a kth order rule the approximation isint f(x)g(x)dx approx sum_i=1^k w_i f(x_i)For the Gauss-Hermite rule the weight function isg(x) = e^-x^2and the domain of integration is (-infty infty). A slight variation of this is the normalized Gauss-Hermite rule for which the weight function is the standard normal densityg(z) = phi(z) = frace^-z^22sqrt2piThus, the expected value of f(z), where mathcalZsimmathscrN(01), is approximated asmathbbEf=int_-infty^infty f(z) phi(z)dzapproxsum_i=1^k w_if(z_i) Naturally, there is a caveat. For the approximation to be accurate the function f(z) must behave like a low-order polynomial over the range of interest. More formally, a kth order rule is exact when f is a k-1 order polynomial."
},

{
    "location": "GaussHermite/#MixedModels.GHnorm",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "MixedModels.GHnorm",
    "category": "function",
    "text": "GHnorm(k::Int)\n\nReturn the (unique) GaussHermiteNormalized{k} object.\n\nThe values are memoized in GHnormd when first evaluated.  Subsequent evaluations for the same k have very low overhead.\n\n\n\n\n\n"
},

{
    "location": "GaussHermite/#Evaluating-the-weights-and-abscissae-1",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "Evaluating the weights and abscissae",
    "category": "section",
    "text": "In the Golub-Welsch algorithm the abscissae for a particular Gaussian quadrature rule are determined as the eigenvalues of a symmetric tri-diagonal matrix and the weights are derived from the squares of the first row of the matrix of eigenvectors. For a kth order normalized Gauss-Hermite rule the tridiagonal matrix has zeros on the diagonal and the square roots of 1:k-1 on the super- and sub-diagonal, e.g.julia> using LinearAlgebra, Gadfly\n\njulia> sym3 = SymTridiagonal(zeros(3), sqrt.(1:2))\n3×3 LinearAlgebra.SymTridiagonal{Float64,Array{Float64,1}}:\n 0.0  1.0       ⋅     \n 1.0  0.0      1.41421\n  ⋅   1.41421  0.0    \n\njulia> ev = eigen(sym3);\n\njulia> show(ev.values)\n[-1.73205, 1.11022e-15, 1.73205]\njulia> show(abs2.(ev.vectors[1,:]))\n[0.166667, 0.666667, 0.166667]As a function of k this can be written asfunction gausshermitenorm(k)\n    ev = eigen(SymTridiagonal(zeros(k), sqrt.(1:k-1)))\n    ev.values, abs2.(ev.vectors[1,:])\nendprovidingjulia> gausshermitenorm(3)\n([-1.73205, 1.11022e-15, 1.73205], [0.166667, 0.666667, 0.166667])\nThe weights and positions are often shown as a lollipop plot. For the 9th order rule these are (Image: Lollipop plot of 9th order normalized Gauss-Hermite rule)Notice that the magnitudes of the weights drop quite dramatically away from zero, even on a logarithmic scale (Image: Lollipop plot of 9th order normalized Gauss-Hermite rule (logarithmic scale)The definition of MixedModels.GHnorm is similar to the gausshermitenorm function with some extra provisions for ensuring symmetry of the abscissae and the weights and for caching values once they have been calculated.GHnormjulia> using MixedModels\n\njulia> GHnorm(3)\nMixedModels.GaussHermiteNormalized{3}([-1.73205, 0.0, 1.73205], [0.166667, 0.666667, 0.166667])\nBy the properties of the normal distribution, when mathcalXsimmathscrN(mu sigma^2)mathbbEg(x) approx sum_i=1^k g(mu + sigma z_i)w_iFor example, mathbbEmathcalX^2 where mathcalXsimmathcalN(2 3^2) isjulia> μ = 2; σ = 3; ghn3 = GHnorm(3);\n\njulia> sum(@. ghn3.w * abs2(μ + σ * ghn3.z))  # should be μ² + σ² = 13\n13.0\n(In general a dot, \'.\', after the function name in a function call, as in abs2.(...), or before an operator creates a fused vectorized evaluation in Julia. The macro @. has the effect of vectorizing all operations in the subsequent expression.)"
},

{
    "location": "GaussHermite/#Application-to-a-model-for-contraception-use-1",
    "page": "Normalized Gauss-Hermite Quadrature",
    "title": "Application to a model for contraception use",
    "category": "section",
    "text": "A binary response is a \"Yes\"/\"No\" type of answer. For example, in a 1989 fertility survey of women in Bangladesh (reported in Huq, N. M. and Cleland, J., 1990) one response of interest was whether the woman used artificial contraception. Several covariates were recorded including the woman\'s age (centered at the mean), the number of live children the woman has had (in 4 categories: 0, 1, 2, and 3 or more), whether she lived in an urban setting, and the district in which she lived. The version of the data used here is that used in review of multilevel modeling software conducted by the Center for Multilevel Modelling, currently at University of Bristol (http://www.bristol.ac.uk/cmm/learning/mmsoftware/data-rev.html). These data are available as the Contraception data frame in the test data for the MixedModels package.julia> using DataFrames, DataFramesMeta, RData\n\njulia> const dat = Dict(Symbol(k)=>v for (k,v) in \n    load(joinpath(dirname(pathof(MixedModels)), \"..\", \"test\", \"dat.rda\")));\n\njulia> const contra = @transform(dat[:Contraception],\n     a2 = abs2.(:a), urbdist = string.(:urb, :d));\n\njulia> describe(contra)\n8×8 DataFrames.DataFrame. Omitted printing of 2 columns\n│ Row │ variable │ mean       │ min    │ median  │ max     │ nunique │\n│     │ Symbol   │ Union…     │ Any    │ Union…  │ Any     │ Union…  │\n├─────┼──────────┼────────────┼────────┼─────────┼─────────┼─────────┤\n│ 1   │ w        │            │ 1      │         │ 1934    │ 1934    │\n│ 2   │ d        │            │ 1      │         │ 61      │ 60      │\n│ 3   │ use      │            │ N      │         │ Y       │ 2       │\n│ 4   │ l        │            │ 0      │         │ 3+      │ 4       │\n│ 5   │ a        │ 0.00219788 │ -13.56 │ -1.5599 │ 19.44   │         │\n│ 6   │ urb      │            │ N      │         │ Y       │ 2       │\n│ 7   │ a2       │ 81.1966    │ 0.1936 │ 55.3536 │ 377.914 │         │\n│ 8   │ urbdist  │            │ N1     │         │ Y9      │ 102     │\nBecause a smoothed scatterplot of contraception use versus age (Image: Scatterplot smooth of contraception use versus age)shows that the proportion of women using artificial contraception is approximately quadratic in age, a column named :a2, which is the square of the age, :a, is added to the data frame using the @transform macro.A model with fixed-effects for age, age squared, number of live children and urban location and with random effects for district, is fit asjulia> const form1 = @formula use ~ 1 + a + a2 + l + urb + (1|d);\n\njulia> m1 = fit!(GeneralizedLinearMixedModel(form1, contra,\n    Bernoulli()), fast=true)\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  Formula: use ~ 1 + a + a2 + l + urb + (1 | d)\n  Distribution: Distributions.Bernoulli{Float64}\n  Link: GLM.LogitLink()\n\n  Deviance: 2372.7844\n\nVariance components:\n       Column    Variance   Std.Dev.  \n d (Intercept)  0.22532962 0.47468897\n\n Number of obs: 1934; levels of grouping factors: 60\n\nFixed-effects parameters:\n               Estimate   Std.Error  z value P(>|z|)\n(Intercept)    -1.01528    0.173972 -5.83585   <1e-8\na            0.00351346  0.00921057 0.381459  0.7029\na2           -0.0044867 0.000722835 -6.20708   <1e-9\nl: 1           0.801881    0.161867  4.95396   <1e-6\nl: 2           0.901017    0.184771   4.8764   <1e-5\nl: 3+          0.899412    0.185401  4.85118   <1e-5\nurb: Y         0.684401    0.119684   5.7184   <1e-7\n\nFor a model such as m1, which has a single, scalar random-effects term, the unscaled conditional density of the spherical random effects variable, mathcalU, given the observed data, mathcalY=mathbfy_0, can be expressed as a product of scalar density functions, f_i(u_i) i=1dotsq. In the PIRLS algorithm, which determines the conditional mode vector, tildemathbfu, the optimization is performed on the deviance scale, D(mathbfu)=-2sum_i=1^q log(f_i(u_i))The objective, D, consists of two parts: the sum of the (squared) deviance residuals, measuring fidelity to the data, and the squared length of mathbfu, which is the penalty. In the PIRLS algorithm, only the sum of these components is needed. To use Gauss-Hermite quadrature the contributions of each of the u_ii=1dotsq should be separately evaluated.julia> const devc0 = map!(abs2, m1.devc0, m1.u[1]);  # start with uᵢ²\n\njulia> const devresid = m1.resp.devresid;   # n-dimensional vector of deviance residuals\n\njulia> const refs = m1.LMM.trms[1].refs;  # n-dimensional vector of indices in 1:q\n\njulia> for (dr, i) in zip(devresid, refs)\n    devc0[i] += dr\nend\n\njulia> show(devc0)\n[121.293, 22.0226, 2.91895, 30.7877, 47.5419, 69.5551, 23.4047, 46.279, 24.4528, 7.75949, 9.77364, 42.7589, 27.5526, 156.421, 26.1925, 27.4161, 24.5381, 57.5662, 31.1792, 22.3417, 27.478, 19.9885, 16.0108, 9.76147, 83.8633, 15.5687, 42.7598, 51.4686, 32.7332, 70.4157, 39.686, 27.544, 14.6975, 53.0474, 64.8499, 19.7439, 19.4156, 11.2423, 37.4169, 54.2651, 39.5826, 17.3984, 60.2275, 28.8192, 42.4441, 112.992, 17.2977, 51.5772, 2.18724, 22.9614, 47.4145, 87.2315, 25.9235, 9.47034, 61.1762, 27.1028, 48.0163, 8.46023, 30.3652, 47.3741]One thing to notice is that, even on the deviance scale, the contributions of different districts can be different magnitudes. This is primarily due to different sample sizes in the different districts.julia> using FreqTables\n\njulia> freqtable(contra, :d)\'\n1×60 Named LinearAlgebra.Adjoint{Int64,Array{Int64,1}}\n\' ╲ d │   1    2    3    4    5    6    7  …   55   56   57   58   59   60   61\n──────┼────────────────────────────────────────────────────────────────────────\n1     │ 117   20    2   30   39   65   18  …    6   45   27   33   10   32   42\nBecause the first district has one of the largest sample sizes and the third district has the smallest sample size, these two will be used for illustration. For a range of u values, evaluate the individual components of the deviance and store them in a matrix.const devc = m1.devc\nconst xvals = -5.0:2.0^(-4):5.0\nconst uv = vec(m1.u[1])\nconst u₀ = vec(m1.u₀[1])\nresults = zeros(length(devc0), length(xvals))\nfor (j, u) in enumerate(xvals)\n    fill!(devc, abs2(u))\n    fill!(uv, u)\n    MixedModels.updateη!(m1)\n    for (dr, i) in zip(devresid, refs)\n        devc[i] += dr\n    end\n    copyto!(view(results, :, j), devc)\nendA plot of the deviance contribution versus u_1 (Image: Deviance contribution of u₁)shows that the deviance contribution is very close to a quadratic. This is also true for u_3 (Image: Deviance contribution of u₃)The PIRLS algorithm provides the locations of the minima of these scalar functions, stored as julia> m1.u₀[1]\n1×60 Array{Float64,2}:\n -1.58477  -0.0727333  0.449062  0.341585  …  -0.767064  -0.90292  -1.06624\nthe minima themselves, evaluated as devc0 above, and a horizontal scale, which is the inverse of diagonal of the Cholesky factor. As shown below, this is an estimate of the conditional standard deviations of the components of mathcalU.julia> const s = inv.(m1.LMM.L.data[Block(1,1)].diag);\n\njulia> s\'\n1×60 LinearAlgebra.Adjoint{Float64,Array{Float64,1}}:\n 0.406889  0.713511  0.952164  0.627135  …  0.839679  0.654965  0.60326\nThe curves can be put on a common scale, corresponding to the standard normal, asfor (j, z) in enumerate(xvals)\n    @. uv = u₀ + z * s\n    MixedModels.updateη!(m1)\n    @. devc = abs2(uv) - devc0\n    for (dr, i) in zip(devresid, refs)\n        devc[i] += dr\n    end\n    copyto!(view(results, :, j), devc)\nend(Image: Scaled and shifted deviance contributions)(Image: Scaled and shifted deviance contributions)On the original density scale these becomefor (j, z) in enumerate(xvals)\n    @. uv = u₀ + z * s\n    MixedModels.updateη!(m1)\n    @. devc = abs2(uv) - devc0\n    for (dr, i) in zip(devresid, refs)\n        devc[i] += dr\n    end\n    copyto!(view(results, :, j), @. exp(-devc/2))\nend(Image: Scaled and shifted conditional density)(Image: Scaled and shifted conditional density)and the function to be integrated with the normalized Gauss-Hermite rule isfor (j, z) in enumerate(xvals)\n    @. uv = u₀ + z * s\n    MixedModels.updateη!(m1)\n    @. devc = abs2(uv) - devc0\n    for (dr, i) in zip(devresid, refs)\n        devc[i] += dr\n    end\n    copyto!(view(results, :, j), @. exp((abs2(z) - devc)/2))\nend(Image: Function to be integrated with normalized Gauss-Hermite rule)(Image: Function to be integrated with normalized Gauss-Hermite rule)"
},

{
    "location": "bootstrap/#",
    "page": "Parametric bootstrap for linear mixed-effects models",
    "title": "Parametric bootstrap for linear mixed-effects models",
    "category": "page",
    "text": ""
},

{
    "location": "bootstrap/#Parametric-bootstrap-for-linear-mixed-effects-models-1",
    "page": "Parametric bootstrap for linear mixed-effects models",
    "title": "Parametric bootstrap for linear mixed-effects models",
    "category": "section",
    "text": "Julia is well-suited to implementing bootstrapping and other simulation-based methods for statistical models. The bootstrap! function in the MixedModels package provides an efficient parametric bootstrap for linear mixed-effects models, assuming that the results of interest from each simulated response vector can be incorporated into a vector of floating-point values."
},

{
    "location": "bootstrap/#The-parametric-bootstrap-1",
    "page": "Parametric bootstrap for linear mixed-effects models",
    "title": "The parametric bootstrap",
    "category": "section",
    "text": "Bootstrapping is a family of procedures for generating sample values of a statistic, allowing for visualization of the distribution of the statistic or for inference from this sample of values.A parametric bootstrap is used with a parametric model, m, that has been fitted to data. The procedure is to simulate n response vectors from m using the estimated parameter values and refit m to these responses in turn, accumulating the statistics of interest at each iteration.The parameters of a linear mixed-effects model as fit by the lmm function are the fixed-effects parameters, β, the standard deviation, σ, of the per-observation noise, and the covariance parameter, θ, that defines the variance-covariance matrices of the random effects.For example, a simple linear mixed-effects model for the Dyestuff data in the lme4 package for R is fit byjulia> using DataFrames, MixedModels, RData, Gadfly\njulia> ds = names!(dat[:Dyestuff], [:Batch, :Yield])\n30×2 DataFrames.DataFrame\n│ Row │ Batch        │ Yield   │\n│     │ Categorical… │ Float64 │\n├─────┼──────────────┼─────────┤\n│ 1   │ A            │ 1545.0  │\n│ 2   │ A            │ 1440.0  │\n│ 3   │ A            │ 1440.0  │\n│ 4   │ A            │ 1520.0  │\n│ 5   │ A            │ 1580.0  │\n│ 6   │ B            │ 1540.0  │\n│ 7   │ B            │ 1555.0  │\n⋮\n│ 23  │ E            │ 1515.0  │\n│ 24  │ E            │ 1635.0  │\n│ 25  │ E            │ 1625.0  │\n│ 26  │ F            │ 1520.0  │\n│ 27  │ F            │ 1455.0  │\n│ 28  │ F            │ 1450.0  │\n│ 29  │ F            │ 1480.0  │\n│ 30  │ F            │ 1445.0  │\n\njulia> m1 = fit(LinearMixedModel, @formula(Yield ~ 1 + (1 | Batch)), ds)\nLinear mixed model fit by maximum likelihood\n Formula: Yield ~ 1 + (1 | Batch)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n              Column    Variance  Std.Dev. \n Batch    (Intercept)  1388.3333 37.260345\n Residual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)    1527.5   17.6946  86.326  <1e-99\n\nNow bootstrap the model parametersjulia> results = bootstrap(100_000, m1);\n\njulia> show(names(results))\nSymbol[:obj, :σ, :β₁, :θ₁, :σ₁]The results for each bootstrap replication are stored in the columns of the matrix passed in as the first argument.  A density plot of the bootstrapped values of σ is created as\nplot(results, x = :σ, Geom.density, Guide.xlabel(\"Parametric bootstrap estimates of σ\"))(Image: Density of parametric bootstrap estimates of σ from model m1)(Image: Density of parametric bootstrap estimates of σ₁ from model m1)(Image: Histogram of parametric bootstrap estimates of σ₁ from model m1)The distribution of the bootstrap samples of σ is a bit skewed but not terribly so.  However, the distribution of the bootstrap samples of the estimate of σ₁ is highly skewed and has a spike at zero."
},

{
    "location": "SimpleLMM/#",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "A Simple, Linear, Mixed-effects Model",
    "category": "page",
    "text": ""
},

{
    "location": "SimpleLMM/#A-Simple,-Linear,-Mixed-effects-Model-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "A Simple, Linear, Mixed-effects Model",
    "category": "section",
    "text": "In this book we describe the theory behind a type of statistical model called mixed-effects models and the practice of fitting and analyzing such models using the MixedModels package for Julia. These models are used in many different disciplines. Because the descriptions of the models can vary markedly between disciplines, we begin by describing what mixed-effects models are and by exploring a very simple example of one type of mixed model, the linear mixed model.This simple example allows us to illustrate the use of the lmm function in the MixedModels package for fitting such models and other functions  for analyzing the fitted model. We also describe methods of assessing the precision of the parameter estimates and of visualizing the conditional distribution of the random effects, given the observed data."
},

{
    "location": "SimpleLMM/#Mixed-effects-Models-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Mixed-effects Models",
    "category": "section",
    "text": "Mixed-effects models, like many other types of statistical models, describe a relationship between a response variable and some of the covariates that have been measured or observed along with the response. In mixed-effects models at least one of the covariates is a categorical covariate representing experimental or observational “units” in the data set. In the example from the chemical industry discussed in this chapter, the observational unit is the batch of an intermediate product used in production of a dye. In medical and social sciences the observational units are often the human or animal subjects in the study. In agriculture the experimental units may be the plots of land or the specific plants being studied.In all of these cases the categorical covariate or covariates are observed at a set of discrete levels. We may use numbers, such as subject identifiers, to designate the particular levels that we observed but these numbers are simply labels. The important characteristic of a categorical covariate is that, at each observed value of the response, the covariate takes on the value of one of a set of distinct levels.Parameters associated with the particular levels of a covariate are sometimes called the “effects” of the levels. If the set of possible levels of the covariate is fixed and reproducible we model the covariate using fixed-effects parameters. If the levels that were observed represent a random sample from the set of all possible levels we incorporate random effects in the model.There are two things to notice about this distinction between fixed-effects parameters and random effects. First, the names are misleading because the distinction between fixed and random is more a property of the levels of the categorical covariate than a property of the effects associated with them. Secondly, we distinguish between “fixed-effects parameters”, which are indeed parameters in the statistical model, and “random effects”, which, strictly speaking, are not parameters. As we will see shortly, random effects are unobserved random variables.To make the distinction more concrete, suppose the objective is to model the annual reading test scores for students in a school district and that the covariates recorded with the score include a student identifier and the student’s gender. Both of these are categorical covariates. The levels of the gender covariate, male and female, are fixed. If we consider data from another school district or we incorporate scores from earlier tests, we will not change those levels. On the other hand, the students whose scores we observed would generally be regarded as a sample from the set of all possible students whom we could have observed. Adding more data, either from more school districts or from results on previous or subsequent tests, will increase the number of distinct levels of the student identifier.Mixed-effects models or, more simply, mixed models are statistical models that incorporate both fixed-effects parameters and random effects. Because of the way that we will define random effects, a model with random effects always includes at least one fixed-effects parameter. Thus, any model with random effects is a mixed model.We characterize the statistical model in terms of two random variables: a q-dimensional vector of random effects represented by the random variable mathcalB and an n-dimensional response vector represented by the random variable mathcalY. (We use upper-case “script” characters to denote random variables. The corresponding lower-case upright letter denotes a particular value of the random variable.) We observe the value, bfy, of mathcalY. We do not observe the value, bfb, of mathcalB.When formulating the model we describe the unconditional distribution of mathcalB and the conditional distribution, (mathcalYmathcalB=bfb). The descriptions of the distributions involve the form of the distribution and the values of certain parameters. We use the observed values of the response and the covariates to estimate these parameters and to make inferences about them.That’s the big picture. Now let’s make this more concrete by describing a particular, versatile class of mixed models called linear mixed models and by studying a simple example of such a model. First we describe the data in the example."
},

{
    "location": "SimpleLMM/#The-Dyestuff-and-Dyestuff2-Data-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "The Dyestuff and Dyestuff2 Data",
    "category": "section",
    "text": "Models with random effects have been in use for a long time. The first edition of the classic book, Statistical Methods in Research and Production, edited by O.L. Davies, was published in 1947 and contained examples of the use of random effects to characterize batch-to-batch variability in chemical processes. The data from one of these examples are available as Dyestuff in the MixedModels package. In this section we describe and plot these data and introduce a second example, the Dyestuff2 data, described in Box and Tiao (1973)."
},

{
    "location": "SimpleLMM/#The-Dyestuff-Data-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "The Dyestuff Data",
    "category": "section",
    "text": "The data are described in Davies (), the fourth edition of the book mentioned above, as coming froman investigation to find out how much the variation from batch to batch in the quality of an intermediate product (H-acid) contributes to the variation in the yield of the dyestuff (Naphthalene Black 12B) made from it. In the experiment six samples of the intermediate, representing different batches of works manufacture, were obtained, and five preparations of the dyestuff were made in the laboratory from each sample. The equivalent yield of each preparation as grams of standard colour was determined by dye-trial.First attach the packages to be usedjulia> using DataFrames, Distributions, GLM, LinearAlgebra, MixedModels\n\njulia> using Random, RCall, RData, SparseArrays\n\njulia> using Gadfly\nand allow for unqualified names for some graphics functionsjulia> using Gadfly.Geom: point, line, histogram, density, vline\n\njulia> using Gadfly.Guide: xlabel, ylabel, yticks\nThe Dyestuff data are available in the lme4 package for R. This data frame and others have been stored in saved RData format in the test directory within the MixedModels package.Access the Dyestuff datajulia> const dat = Dict(Symbol(k)=>v for (k,v) in \n    load(joinpath(dirname(pathof(MixedModels)), \"..\", \"test\", \"dat.rda\")));\n\njulia> dyestuff = dat[:Dyestuff];\n\njulia> describe(dyestuff)\n2×8 DataFrames.DataFrame. Omitted printing of 1 columns\n│ Row │ variable │ mean   │ min    │ median │ max    │ nunique │ nmissing │\n│     │ Symbol   │ Union… │ Any    │ Union… │ Any    │ Union…  │ Nothing  │\n├─────┼──────────┼────────┼────────┼────────┼────────┼─────────┼──────────┤\n│ 1   │ G        │        │ A      │        │ F      │ 6       │          │\n│ 2   │ Y        │ 1527.5 │ 1440.0 │ 1530.0 │ 1635.0 │         │          │\nand plot it (Image: Yield versus Batch for the Dyestuff data)In the dotplot we can see that there is considerable variability in yield, even for preparations from the same batch, but there is also noticeable batch-to-batch variability. For example, four of the five preparations from batch F provided lower yields than did any of the preparations from batches B, C, and E.Recall that the labels for the batches are just labels and that their ordering is arbitrary. In a plot, however, the order of the levels influences the perception of the pattern. Rather than providing an arbitrary pattern it is best to order the levels according to some criterion for the plot. In this case a good choice is to order the batches by increasing mean yield, which can be easily done in R.julia> dyestuffR = rcopy(R\"within(lme4::Dyestuff, Batch <- reorder(Batch, Yield, mean))\");\n\njulia> plot(dyestuffR, x = :Yield, y = :Batch, point, xlabel(\"Yield of dyestuff (g)\"), ylabel(\"Batch\"))\nPlot(...)\nIn Sect. [sec:DyestuffLMM] we will use mixed models to quantify the variability in yield between batches. For the time being let us just note that the particular batches used in this experiment are a selection or sample from the set of all batches that we wish to consider. Furthermore, the extent to which one particular batch tends to increase or decrease the mean yield of the process — in other words, the “effect” of that particular batch on the yield — is not as interesting to us as is the extent of the variability between batches. For the purposes of designing, monitoring and controlling a process we want to predict the yield from future batches, taking into account the batch-to-batch variability and the within-batch variability. Being able to estimate the extent to which a particular batch in the past increased or decreased the yield is not usually an important goal for us. We will model the effects of the batches as random effects rather than as fixed-effects parameters."
},

{
    "location": "SimpleLMM/#The-Dyestuff2-Data-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "The Dyestuff2 Data",
    "category": "section",
    "text": "The data are simulated data presented in Box and Tiao (1973), where the authors stateThese data had to be constructed for although examples of this sort undoubtedly occur in practice they seem to be rarely published.The structure and summary are intentionally similar to those of the Dyestuff data.julia> dyestuff2 = dat[:Dyestuff2];\n\njulia> describe(dyestuff2)\n2×8 DataFrames.DataFrame. Omitted printing of 1 columns\n│ Row │ variable │ mean   │ min    │ median │ max    │ nunique │ nmissing │\n│     │ Symbol   │ Union… │ Any    │ Union… │ Any    │ Union…  │ Nothing  │\n├─────┼──────────┼────────┼────────┼────────┼────────┼─────────┼──────────┤\n│ 1   │ G        │        │ A      │        │ F      │ 6       │          │\n│ 2   │ Y        │ 5.6656 │ -0.892 │ 5.365  │ 13.434 │         │          │\nAs can be seen in a data plot (Image: )the batch-to-batch variability in these data is small compared to the within-batch variability. In some approaches to mixed models it can be difficult to fit models to such data. Paradoxically, small “variance components” can be more difficult to estimate than large variance components.The methods we will present are not compromised when estimating small variance components."
},

{
    "location": "SimpleLMM/#Fitting-Linear-Mixed-Models-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Fitting Linear Mixed Models",
    "category": "section",
    "text": "Before we formally define a linear mixed model, let’s go ahead and fit models to these data sets using lmm which takes, as its first two arguments, a formula specifying the model and the data with which to evaluate the formula. The structure of the formula will be explained after showing the example."
},

{
    "location": "SimpleLMM/#A-Model-For-the-Dyestuff-Data-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "A Model For the Dyestuff Data",
    "category": "section",
    "text": "A model allowing for an overall level of the Yield and for an additive random effect for each level of Batch can be fit asjulia> mm1 = fit(LinearMixedModel, @formula(Y ~ 1 + (1 | G)), dyestuff)\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -163.66353  327.32706  333.32706  337.53065\n\nVariance components:\n              Column    Variance  Std.Dev. \n G        (Intercept)  1388.3333 37.260345\n Residual              2451.2500 49.510100\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)    1527.5   17.6946  86.326  <1e-99\n\nAs shown in the summary of the model fit, the default estimation criterion is maximum likelihood. The summary provides several other model-fit statistics such as Akaike’s Information Criterion (AIC), Schwarz’s Bayesian Information Criterion (BIC), the log-likelihood at the parameter estimates, and the objective function (negative twice the log-likelihood) at the parameter estimates. These are all statistics related to the model fit and are used to compare different models fit to the same data.The third section is the table of estimates of parameters associated with the random effects. There are two sources of variability in this model, a batch-to-batch variability in the level of the response and the residual or per-observation variability — also called the within-batch variability. The name “residual” is used in statistical modeling to denote the part of the variability that cannot be explained or modeled with the other terms. It is the variation in the observed data that is “left over” after determining the estimates of the parameters in the other parts of the model.Some of the variability in the response is associated with the fixed-effects terms. In this model there is only one such term, labeled the (Intercept). The name “intercept”, which is better suited to models based on straight lines written in a slope/intercept form, should be understood to represent an overall “typical” or mean level of the response in this case. (For those wondering about the parentheses around the name, they are included so that a user cannot accidentally name a variable in conflict with this name.) The line labeled Batch in the random effects table shows that the random effects added to the intercept term, one for each level of the factor, are modeled as random variables whose unconditional variance is estimated as 1388.33 g^2. The corresponding standard deviations is 37.26 g for the ML fit.Note that the last column in the random effects summary table is the estimate of the variability expressed as a standard deviation rather than as a variance. These are provided because it is usually easier to visualize the variability in standard deviations, which are on the scale of the response, than it is to visualize the magnitude of a variance. The values in this column are a simple re-expression (the square root) of the estimated variances. Do not confuse them with the standard errors of the variance estimators, which are not given here. As described in later sections, standard errors of variance estimates are generally not useful because the distribution of the estimator of a variance is skewed - often badly skewed.The line labeled Residual in this table gives the estimate, 2451.25 g^2, of the variance of the residuals and the corresponding standard deviation, 49.51 g. In written descriptions of the model the residual variance parameter is written sigma^2 and the variance of the random effects is sigma_1^2. Their estimates are widehatsigma^2 and widehatsigma_1^2The last line in the random effects table states the number of observations to which the model was fit and the number of levels of any “grouping factors” for the random effects. In this case we have a single random effects term, (1 | Batch), in the model formula and the grouping factor for that term is Batch. There will be a total of six random effects, one for each level of Batch.The final part of the printed display gives the estimates and standard errors of any fixed-effects parameters in the model. The only fixed-effects term in the model formula is the (Intercept). The estimate of this parameter is 1527.5 g. The standard error of the intercept estimate is 17.69 g."
},

{
    "location": "SimpleLMM/#A-Model-For-the-Dyestuff2-Data-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "A Model For the Dyestuff2 Data",
    "category": "section",
    "text": "Fitting a similar model to the dyestuff2 data produces an estimate widehatsigma_1^2=0.julia> mm2 = fit(LinearMixedModel, @formula(Y ~ 1 + (1 | G)), dyestuff2)\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -81.436518 162.873037 168.873037 173.076629\n\nVariance components:\n              Column    Variance  Std.Dev. \n G        (Intercept)   0.000000 0.0000000\n Residual              13.346099 3.6532314\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)    5.6656  0.666986 8.49433  <1e-16\n\nAn estimate of 0 for sigma_1 does not mean that there is no variation between the groups. Indeed Fig. [fig:Dyestuff2dot] shows that there is some small amount of variability between the groups. The estimate, widehatsigma_1, is a measure of the “between-group” variability that is in excess of the variability induced by the \"within-group\" or residual variability in the responses.  If 30 observations were simulated from a \"normal\" (also called \"Gaussian\") distribution and divided arbitrarily into 6 groups of 5, a plot of the data would look very much like Fig. [fig:Dyestuff2dot].  (In fact, it is likely that this is how that data set was generated.) It is only where there is excess variability between the groups that widehatsigma_10. Obtaining widehatsigma_1=0 is not a mistake; it is simply a statement about the data and the model.The important point to take away from this example is the need to allow for the estimates of variance components that are zero. Such a model is said to be singular, in the sense that it corresponds to a linear model in which we have removed the random effects associated with Batch. Singular models can and do occur in practice. Even when the final fitted model is not singular, we must allow for such models to be expressed when determining the parameter estimates through numerical optimization.It happens that this model corresponds to the linear model (i.e. a model with fixed-effects only)julia> lm1 = lm(@formula(Y ~ 1), dyestuff2)\nStatsModels.DataFrameRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float64,2}}\n\nFormula: Y ~ +1\n\nCoefficients:\n             Estimate Std.Error t value Pr(>|t|)\n(Intercept)    5.6656  0.678388 8.35156    <1e-8\n\nThe log-likelihood for this modeljulia> loglikelihood(lm1)\n-81.43651832691287\nis the same as that of fm2. The standard error of the intercept in lm1 is a bit larger than that of fm2 because the estimate of the residual variance is evaluated differently in the linear model."
},

{
    "location": "SimpleLMM/#Further-Assessment-of-the-Fitted-Models-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Further Assessment of the Fitted Models",
    "category": "section",
    "text": "The parameter estimates in a statistical model represent our “best guess” at the unknown values of the model parameters and, as such, are important results in statistical modeling. However, they are not the whole story. Statistical models characterize the variability in the data and we must assess the effect of this variability on the parameter estimates and on the precision of predictions made from the model.In Sect. [sec:variability] we introduce a method of assessing variability in parameter estimates using the “profiled log-likelihood” and in Sect. [sec:assessRE] we show methods of characterizing the conditional distribution of the random effects given the data. Before we get to these sections, however, we should state in some detail the probability model for linear mixed-effects and establish some definitions and notation. In particular, before we can discuss profiling the log-likelihood, we should define the log-likelihood. We do that in the next section."
},

{
    "location": "SimpleLMM/#The-Linear-Mixed-effects-Probability-Model-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "The Linear Mixed-effects Probability Model",
    "category": "section",
    "text": "In explaining some of parameter estimates related to the random effects we have used terms such as “unconditional distribution” from the theory of probability. Before proceeding further we clarify the linear mixed-effects probability model and define several terms and concepts that will be used throughout the book. Readers who are more interested in practical results than in the statistical theory should feel free to skip this section."
},

{
    "location": "SimpleLMM/#Definitions-and-Results-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Definitions and Results",
    "category": "section",
    "text": "In this section we provide some definitions and formulas without derivation and with minimal explanation, so that we can use these terms in what follows. In Chap. [chap:computational] we revisit these definitions providing derivations and more explanation.As mentioned in Sect. [sec:memod], a mixed model incorporates two random variables: mathcalB, the q-dimensional vector of random effects, and mathcalY, the n-dimensional response vector. In a linear mixed model the unconditional distribution of mathcalB and the conditional distribution, (mathcalY  mathcalB=bfb), are both multivariate Gaussian distributions,\\begin{aligned}   (\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\beta + Z b},\\sigma^2\\bf{I})\\\\\n  \\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_\\theta) . \\end{aligned}The conditional mean of mathcal Y, given mathcal B=bf b, is the linear predictor, bf Xbfbeta+bf Zbf b, which depends on the p-dimensional fixed-effects parameter, bf beta, and on bf b. The model matrices, bf X and bf Z, of dimension ntimes p and ntimes q, respectively, are determined from the formula for the model and the values of covariates. Although the matrix bf Z can be large (i.e. both n and q can be large), it is sparse (i.e. most of the elements in the matrix are zero).The relative covariance factor, Lambda_theta, is a qtimes q lower-triangular matrix, depending on the variance-component parameter, bftheta, and generating the symmetric qtimes q variance-covariance matrix, Sigma_theta, as\\begin{equation}   \\Sigma\\theta=\\sigma^2\\Lambda\\theta\\Lambda_\\theta\' \\end{equation}The spherical random effects, mathcalUsimmathcalN(bf 0sigma^2bf I_q), determine mathcal B according to\\begin{equation}   \\mathcal{B}=\\Lambda_\\theta\\mathcal{U}. \\end{equation}The penalized residual sum of squares (PRSS),\\begin{equation}   r^2(\\theta,\\beta,{\\bf u})=\\|{\\bf y} -{\\bf X}\\beta -{\\bf Z}\\Lambda_\\theta{\\bf u}\\|^2+\\|{\\bf u}\\|^2, \\end{equation}is the sum of the residual sum of squares, measuring fidelity of the model to the data, and a penalty on the size of bf u, measuring the complexity of the model. Minimizing r^2 with respect to bf u, \\begin{equation}   r^2{\\beta,\\theta} =\\min{\\bf u}\\left{\\|{\\bf y} -{\\bf X}{\\beta} -{\\bf Z}\\Lambda\\theta{\\bf u}\\|^2+\\|{\\bf u}\\|^2\\right} \\end{equation} is a direct (i.e. non-iterative) computation. The particular method used to solve this generates a blocked Choleksy factor, ${\\bf L}\\theta which is an lower triangular qtimes q matrix satisfying \\begin{equation}   {\\bf L}\\theta{\\bf L}\\theta\'=\\Lambda\\theta\'{\\bf Z}\'{\\bf Z}\\Lambda\\theta+{\\bf I}q . \\end{equation} where ${\\bf I}q$ is the qtimes q identity matrix.Negative twice the log-likelihood of the parameters, given the data, bf y, is\\begin{equation} d({\\bf\\theta},{\\bf\\beta},\\sigma|{\\bf y}) =n\\log(2\\pi\\sigma^2)+\\log(|{\\bf L}\\theta|^2)+\\frac{r^2{\\beta,\\theta}}{\\sigma^2}. \\end{equation}where bf L_theta denotes the determinant of bf L_theta. Because bf L_theta is triangular, its determinant is the product of its diagonal elements.Negative twice the log-likelihood will be called the objective in what follows. It is the value to be minimized by the parameter estimates.  It is, up to an additive factor, the deviance of the parameters.  Unfortunately, it is not clear what the additive factor should be in the case of linear mixed models.  In many applications, however, it is not the deviance of the model that is of interest as much the change in the deviance between two fitted models.  When calculating the change in the deviance the additive factor will cancel out so the change in the deviance when comparing models is the same as the change in this objective.Because the conditional mean, bfmu_mathcal Ymathcal B=bf b=bf Xbfbeta+bf ZLambda_thetabf u, is a linear function of both bfbeta and bf u, minimization of the PRSS with respect to both bfbeta and bf u to produce\\begin{equation} r^2\\theta =\\min{{\\bf\\beta},{\\bf u}}\\left{\\|{\\bf y} -{\\bf X}{\\bf\\beta} -{\\bf Z}\\Lambda_\\theta{\\bf u}\\|^2+\\|{\\bf u}\\|^2\\right} \\end{equation}is also a direct calculation. The values of bf u and bfbeta that provide this minimum are called, respectively, the conditional mode, tildebf u_theta, of the spherical random effects and the conditional estimate, widehatbfbeta_theta, of the fixed effects. At the conditional estimate of the fixed effects the objective is\\begin{equation} d({\\bf\\theta},\\widehat{\\beta}\\theta,\\sigma|{\\bf y}) =n\\log(2\\pi\\sigma^2)+\\log(|{\\bf L}\\theta|^2)+\\frac{r^2_\\theta}{\\sigma^2}. \\end{equation}Minimizing this expression with respect to sigma^2 produces the conditional estimate\\begin{equation} \\widehat{\\sigma^2}\\theta=\\frac{r^2\\theta}{n} \\end{equation}which provides the profiled log-likelihood on the deviance scale as\\begin{equation} \\tilde{d}(\\theta|{\\bf y})=d(\\theta,\\widehat{\\beta}\\theta,\\widehat{\\sigma}\\theta|{\\bf y}) =\\log(|{\\bf L}\\theta|^2)+n\\left[1+\\log\\left(\\frac{2\\pi r^2\\theta}{n}\\right)\\right], \\end{equation} a function of bftheta alone.The MLE of bftheta, written widehatbftheta, is the value that minimizes this profiled objective. We determine this value by numerical optimization. In the process of evaluating tilded(widehatthetabf y) we determine widehatbeta=widehatbeta_widehattheta, tildebf u_widehattheta and r^2_widehattheta, from which we can evaluate widehatsigma=sqrtr^2_widehatthetan.The elements of the conditional mode of mathcal B, evaluated at the parameter estimates,\\begin{equation} \\tilde{\\bf b}{\\widehat{\\theta}}= \\Lambda{\\widehat{\\theta}}\\tilde{\\bf u}_{\\widehat{\\theta}} \\end{equation}are sometimes called the best linear unbiased predictors or BLUPs of the random effects. Although BLUPs an appealing acronym, I don’t find the term particularly instructive (what is a “linear unbiased predictor” and in what sense are these the “best”?) and prefer the term “conditional modes”, because these are the values of bf b that maximize the density of the conditional distribution mathcalB  mathcalY = bf y. For a linear mixed model, where all the conditional and unconditional distributions are Gaussian, these values are also the conditional means."
},

{
    "location": "SimpleLMM/#Fields-of-a-LinearMixedModel-object-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Fields of a LinearMixedModel object",
    "category": "section",
    "text": "The optional second argument, verbose, in a call to fit! of a LinearMixedModel object produces output showing the progress of the iterative optimization of tilded(bfthetabf y).julia> mm1 = fit!(LinearMixedModel(@formula(Y ~ 1 + (1 | G)), dyestuff), verbose = true);\nf_1: 327.76702 [1.0]\nf_2: 331.03619 [1.75]\nf_3: 330.64583 [0.25]\nf_4: 327.69511 [0.97619]\nf_5: 327.56631 [0.928569]\nf_6: 327.3826 [0.833327]\nf_7: 327.35315 [0.807188]\nf_8: 327.34663 [0.799688]\nf_9: 327.341 [0.792188]\nf_10: 327.33253 [0.777188]\nf_11: 327.32733 [0.747188]\nf_12: 327.32862 [0.739688]\nf_13: 327.32706 [0.752777]\nf_14: 327.32707 [0.753527]\nf_15: 327.32706 [0.752584]\nf_16: 327.32706 [0.752509]\nf_17: 327.32706 [0.752591]\nf_18: 327.32706 [0.752581]\nThe algorithm converges after 18 function evaluations to a profiled deviance of 327.32706 at theta=0752581. In this model the parameter theta is of length 1, the single element being the ratio sigma_1sigma.Whether or not verbose output is requested, the optsum field of a LinearMixedModel contains information on the optimization.  The various tolerances or the optimizer name can be changed between creating a LinearMixedModel and calling fit! on it to exert finer control on the optimization process.julia> mm1.optsum\nInitial parameter vector: [1.0]\nInitial objective value:  327.76702162461663\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10]\ninitial_step:             [0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     18\nFinal parameter vector:   [0.752581]\nFinal objective value:    327.3270598811373\nReturn code:              FTOL_REACHED\n\nThe full list of fields in a LinearMixedModel object isjulia> fieldnames(LinearMixedModel)\n(:formula, :trms, :sqrtwts, :A, :L, :optsum)\nThe formula field is a copy of the model formulajulia> mm1.formula\nFormula: Y ~ 1 + (1 | G)\nThe mf field is a ModelFrame constructed from the formula and data arguments to lmm. It contains a DataFrame formed by reducing the data argument to only those columns needed to evaluate the formula and only those rows that have no missing data. It also contains information on the terms in the model formula and on any \"contrasts\" associated with categorical variables in the fixed-effects terms.The trms field is a vector of numerical objects representing the terms in the model, including the response vector. As the names imply, the sqrtwts and wttrms fields are for incorporating case weights. These fields are not often used when fitting linear mixed models but are vital to the process of fitting a generalized linear mixed model, described in Chapter [sec:glmm].  When used, sqrtwts is a diagonal matrix of size n. A size of 0 indicates weights are not used.julia> mm1.sqrtwts\n0-element Array{Float64,1}\nThe trms field is a vector of length ge 3.julia> length(mm1.trms)\n3\nThe last two elements are bf X, the ntimes p model matrix for the fixed-effects parameters, bfbeta, and bf y, the response vector stored as a matrix of size ntimes 1.  In mm1, bf X consists of a single column of 1\'sjulia> mm1.trms[end - 1]\nMixedModels.MatrixTerm{Float64,Array{Float64,2}}([1.0; 1.0; … ; 1.0; 1.0], [1.0; 1.0; … ; 1.0; 1.0], [1], 1, [\"(Intercept)\"])\njulia> mm1.trms[end]\nMixedModels.MatrixTerm{Float64,Array{Float64,2}}([1545.0; 1440.0; … ; 1480.0; 1445.0], [1545.0; 1440.0; … ; 1480.0; 1445.0], [1], 0, [\"\"])\nThe elements of trms before the last two represent vertical sections of bf Z associated with the random effects terms in the model.  In mm1 there is only one random effects term, (1 | Batch), and bf Z has only one section, the one generated by this term, of type ScalarReMat.julia> mm1.trms[1]\nMixedModels.ScalarFactorReTerm{Float64,UInt8}(UInt8[0x01, 0x01, 0x01, 0x01, 0x01, 0x02, 0x02, 0x02, 0x02, 0x02  …  0x05, 0x05, 0x05, 0x05, 0x05, 0x06, 0x06, 0x06, 0x06, 0x06], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], :G, [\"(Intercept)\"], 0.7525806932030558)\nIn practice these matrices are stored in a highly condensed form because, in some models, they can be very large. In small examples the structure is more obvious when the ScalarReMat is converted to a sparse or a dense matrix.julia> sparse(mm1.trms[1])\n30×6 SparseArrays.SparseMatrixCSC{Float64,Int32} with 30 stored entries:\n  [1 ,  1]  =  1.0\n  [2 ,  1]  =  1.0\n  [3 ,  1]  =  1.0\n  [4 ,  1]  =  1.0\n  [5 ,  1]  =  1.0\n  [6 ,  2]  =  1.0\n  [7 ,  2]  =  1.0\n  [8 ,  2]  =  1.0\n  [9 ,  2]  =  1.0\n  ⋮\n  [22,  5]  =  1.0\n  [23,  5]  =  1.0\n  [24,  5]  =  1.0\n  [25,  5]  =  1.0\n  [26,  6]  =  1.0\n  [27,  6]  =  1.0\n  [28,  6]  =  1.0\n  [29,  6]  =  1.0\n  [30,  6]  =  1.0\njulia> Matrix(mm1.trms[1])\n30×6 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0\n ⋮                        ⋮  \n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  1.0\nThe A field is a representation of the blocked, square, symmetric matrix bf A = Z  X  yZ  X  y. Only the upper triangle of A is stored. The number of blocks of the rows and columns of A is the number of vertical sections of bf Z (i.e. the number of random-effects terms) plus 2.julia> nblocks(mm1.A)\n(3, 3)\njulia> mm1.A[Block(1, 1)]\n6×6 LinearAlgebra.Diagonal{Float64,Array{Float64,1}}:\n 5.0   ⋅    ⋅    ⋅    ⋅    ⋅ \n  ⋅   5.0   ⋅    ⋅    ⋅    ⋅ \n  ⋅    ⋅   5.0   ⋅    ⋅    ⋅ \n  ⋅    ⋅    ⋅   5.0   ⋅    ⋅ \n  ⋅    ⋅    ⋅    ⋅   5.0   ⋅ \n  ⋅    ⋅    ⋅    ⋅    ⋅   5.0\njulia> mm1.A[Block(2, 1)]\n1×6 Array{Float64,2}:\n 5.0  5.0  5.0  5.0  5.0  5.0\njulia> mm1.A[Block(2, 2)]\n1×1 Array{Float64,2}:\n 30.0\njulia> mm1.A[Block(3, 1)]\n1×6 Array{Float64,2}:\n 7525.0  7640.0  7820.0  7490.0  8000.0  7350.0\njulia> mm1.A[Block(3, 2)]\n1×1 Array{Float64,2}:\n 45825.0\njulia> mm1.A[Block(3, 3)]\n1×1 Array{Float64,2}:\n 7.0112875e7\n"
},

{
    "location": "SimpleLMM/#Fields-modified-during-the-optimization-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Fields modified during the optimization",
    "category": "section",
    "text": "Changing the value of theta changes the Lambda field. (Note: to input a symbol like Lambda in a Jupyter code cell or in the Julia read-eval-print loop (REPL), type \\Lambda followed by a tab character.  Such \"latex completions\" are available for many UTF-8 characters used in Julia.)  The matrix Lambda has a special structure.  It is a block diagonal matrix where the diagonal blocks are Kronecker products of an identity matrix and a (small) lower triangular matrix.  The diagonal blocks correspond to the random-effects terms.  For a scalar random-effects term, like (1 | Batch) the diagonal block is the Kronecker product of an identity matrix and a 1times 1 matrix.  This result in this case is just a multiple of the identity matrix.It is not necessary to store the full Lambda matrix.  Storing the small lower-triangular matrices is sufficient.julia> getΛ(mm1)\n1-element Array{Float64,1}:\n 0.7525806932030558\nThe L field is a blocked matrix like the A field containing the upper Cholesky factor of\\begin{bmatrix}   \\bf{\\Lambda\'Z\'Z\\Lambda + I} & \\bf{\\Lambda\'Z\'X} & \\bf{\\Lambda\'Z\'y} \\\n  \\bf{X\'Z\\Lambda} & \\bf{X\'X} & \\bf{X\'y} \\\n  \\bf{y\'Z\\Lambda} & \\bf{y\'Z} & \\bf{y\'y} \\end{bmatrix}   julia> mm1.L\n8×8 LinearAlgebra.LowerTriangular{Float64,BlockArrays.BlockArray{Float64,2,AbstractArray{Float64,2}}}:\n    1.95752      ⋅           ⋅       …      ⋅           ⋅          ⋅   \n    0.0         1.95752      ⋅              ⋅           ⋅          ⋅   \n    0.0         0.0         1.95752         ⋅           ⋅          ⋅   \n    0.0         0.0         0.0             ⋅           ⋅          ⋅   \n    0.0         0.0         0.0             ⋅           ⋅          ⋅   \n    0.0         0.0         0.0    \n ──────────────────────────────────  …     1.95752      ⋅          ⋅   \n ───────────────────────────────\n    1.92228     1.92228     1.92228\n ──────────────────────────────────        1.92228     2.79804     ⋅   \n ───────────────────────────────\n 2893.03     2937.24     3006.45        2825.75     4274.01     271.178\njulia> mm1.L.data[Block(1, 1)]\n6×6 LinearAlgebra.Diagonal{Float64,Array{Float64,1}}:\n 1.95752   ⋅        ⋅        ⋅        ⋅        ⋅     \n  ⋅       1.95752   ⋅        ⋅        ⋅        ⋅     \n  ⋅        ⋅       1.95752   ⋅        ⋅        ⋅     \n  ⋅        ⋅        ⋅       1.95752   ⋅        ⋅     \n  ⋅        ⋅        ⋅        ⋅       1.95752   ⋅     \n  ⋅        ⋅        ⋅        ⋅        ⋅       1.95752\njulia> mm1.L.data[Block(2, 1)]\n1×6 Array{Float64,2}:\n 1.92228  1.92228  1.92228  1.92228  1.92228  1.92228\njulia> mm1.L.data[Block(2, 2)]\n1×1 Array{Float64,2}:\n 2.798041784037937\njulia> mm1.L.data[Block(3, 1)]\n1×6 Array{Float64,2}:\n 2893.03  2937.24  3006.45  2879.58  3075.65  2825.75\njulia> mm1.L.data[Block(3, 2)]\n1×1 Array{Float64,2}:\n 4274.008825117949\njulia> mm1.L.data[Block(3, 3)]\n1×1 Array{Float64,2}:\n 271.17798578517414\nAll the information needed to evaluate the profiled log-likelihood is available in the R field; log(bf R_theta^2) isjulia> 2 * sum(log.(diag(mm1.L.data[Block(1,1)])))\n8.060146573941458\nIt can also be evaluated as logdet(mm1) or 2 * logdet(mm1.R[1, 1])julia> logdet(mm1) == (2*logdet(mm1.L.data[Block(1, 1)])) == (2*sum(log.(diag(mm1.L.data[Block(1, 1)]))))\ntrue\nThe penalized residual sum of squares is the square of the single element of the lower-right block, R[3, 3] in this casejulia> abs2(mm1.L.data[Block(3, 3)][1, 1])\n73537.49997450411\njulia> pwrss(mm1)\n73537.49997450411\nThe objective isjulia> logdet(mm1) + nobs(mm1) * (1 + log(2π * pwrss(mm1) / nobs(mm1)))\n327.3270598811373\n"
},

{
    "location": "SimpleLMM/#Assessing-variability-of-parameter-estimates-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Assessing variability of parameter estimates",
    "category": "section",
    "text": ""
},

{
    "location": "SimpleLMM/#Parametric-bootstrap-samples-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Parametric bootstrap samples",
    "category": "section",
    "text": "One way to assess the variability of the parameter estimates is to generate a parametric bootstrap sample from the model.  The technique is to simulate response vectors from the model at the estimated parameter values and refit the model to each of these simulated responses, recording the values of the parameters.  The bootstrap method for these models performs these simulations and returns 4 arrays: a vector of objective (negative twice the log-likelihood) values, a vector of estimates of sigma^2, a matrix of estimates of the fixed-effects parameters and a matrix of the estimates of the relative covariance parameters.  In this case there is only one fixed-effects parameter and one relative covariance parameter, which is the ratio of the standard deviation of the random effects to the standard deviation of the per-sample noise.First set the random number seed for reproducibility.julia> Random.seed!(1234321);\n\njulia> mm1bstp = bootstrap(100000, mm1);\n\njulia> size(mm1bstp)\n(100000, 5)\njulia> describe(mm1bstp)\n5×8 DataFrames.DataFrame. Omitted printing of 1 columns\n│ Row │ variable │ mean     │ min     │ median  │ max     │ nunique │ nmissing │\n│     │ Symbol   │ Float64  │ Float64 │ Float64 │ Float64 │ Nothing │ Nothing  │\n├─────┼──────────┼──────────┼─────────┼─────────┼─────────┼─────────┼──────────┤\n│ 1   │ obj      │ 324.03   │ 284.104 │ 324.346 │ 353.114 │         │          │\n│ 2   │ σ        │ 48.8259  │ 23.0686 │ 48.6577 │ 81.1291 │         │          │\n│ 3   │ β₁       │ 1527.41  │ 1442.75 │ 1527.47 │ 1598.74 │         │          │\n│ 4   │ θ₁       │ 0.611039 │ 0.0     │ 0.61079 │ 2.9935  │         │          │\n│ 5   │ σ₁       │ 28.9064  │ 0.0     │ 29.6869 │ 94.001  │         │          │\n"
},

{
    "location": "SimpleLMM/#Histograms,-kernel-density-plots-and-quantile-quantile-plots-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Histograms, kernel density plots and quantile-quantile plots",
    "category": "section",
    "text": "I am a firm believer in the value of plotting results before summarizing them. Well chosen plots can provide insights not available from a simple numerical summary. It is common to visualize the distribution of a sample using a histogram, which approximates the shape of the probability density function. The density can also be approximated more smoothly using a kernel density plot. Finally, the extent to which the distribution of a sample can be approximated by a particular distribution or distribution family can be assessed by a quantile-quantile (qq) plot, the most common of which is the normal probability plot.The Gadfly package for Julia uses a \"grammar of graphics\" specification, similar to the ggplot2 package for R.  A histogram or a kernel density plot are describes as geometries and specified by Geom.histogram and Geom.density, respectively.julia> plot(mm1bstp, x = :β₁, histogram)\nPlot(...)\njulia> plot(mm1bstp, x = :σ, histogram)\nPlot(...)\njulia> plot(mm1bstp, x = :σ₁, histogram)\nPlot(...)\nThe last two histograms show that, even if the models are defined in terms of variances, the variance is usually not a good scale on which to assess the variability of the parameter estimates.  The standard deviation or, in some cases, the logarithm of the standard deviation is a more suitable scale.The histogram of sigma_1^2 has a \"spike\" at zero.  Because the value of sigma^2 is never zero, a value of sigma_1^2=0 must correspond to theta=0.  There are several ways to count the zeros in theta1.  The simplest is to use count on the nonzeros, and subtrack that value from the total number of values in theta1.julia> length(mm1bstp[:θ₁]) - count(!iszero, mm1bstp[:θ₁])\n10090\nThat is, nearly 1/10 of the theta1 values are zeros.  Because such a spike or pulse will be spread out or diffused in a kernel density plot,julia> plot(mm1bstp, density, x = :θ₁)\nPlot(...)\nsuch a plot is not suitable for a sample of a bounded parameter that includes values on the boundary.The density of the estimates of the other two parameters, beta_1 and sigma, are depicted well in kernel density plots.julia> plot(mm1bstp, density, x = :β₁)\nPlot(...)\njulia> plot(mm1bstp, density, x = :σ)\nPlot(...)\nThe standard approach of summarizing a sample by its mean and standard deviation, or of constructing a confidence interval using the sample mean, the standard error of the mean and quantiles of a t or normal distribution, are based on the assumption that the sample is approximately normal (also called Gaussian) in shape.  A normal probability plot, which plots sample quantiles versus quantiles of the standard normal distribution, mathcalN(01), can be used to assess the validity of this assumption.  If the points fall approximately along a straight line, the assumption of normality should be valid.  Systematic departures from a straight line are cause for concern.In Gadfly a normal probability plot can be  constructed by specifying the statistic to be generated as Stat.qq and either x or y as the distribution Normal(). For the present purposes it is an advantage to put the theoretical quantiles on the x axis.This approach is suitable for small to moderate sample sizes, but not for sample sizes of 10,000.  To smooth the plot and to reduce the size of the plot files, we plot quantiles defined by a sequence of n \"probability points\".  These are constructed by partitioning the interval (0, 1) into n equal-width subintervals and returning the midpoint of each of the subintervals.julia> function ppoints(n)\n    if n ≤ 0\n        throw(ArgumentError(\"n = $n should be positive\"))\n    end\n    width = inv(n)\n    width / 2 : width : one(width)\nend\nppoints (generic function with 1 method)\n\njulia> const ppt250 = ppoints(250)\n0.002:0.004:0.998\nThe kernel density estimate of sigma is more symmetric(Image: )and the normal probability plot of sigma is also reasonably straight.(Image: )The normal probability plot of sigma_1 has a flat section at sigma_1 = 0.(Image: )In terms of the variances, sigma^2 and sigma_1^2, the normal probability plots are(Image: )(Image: )"
},

{
    "location": "SimpleLMM/#Confidence-intervals-based-on-bootstrap-samples-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Confidence intervals based on bootstrap samples",
    "category": "section",
    "text": "When the distribution of a parameter estimator is close to normal or to a T distribution, symmetric confidence intervals are an appropriate representation of the uncertainty in the parameter estimates.  However, they are not appropriate for skewed and/or bounded estimator distributions, such as those for sigma^2 and sigma_2^1 shown above.The fact that a symmetric confidence interval is not appropriate for sigma^2 should not be surprising.  In an introductory statistics course the calculation of a confidence interval on sigma^2 from a random sample of a normal distribution using quantiles of a chi^2 distribution is often introduced. So a symmetric confidence interval on sigma^2 is inappropriate in the simplest case but is often expected to be appropriate in much more complicated cases, as shown by the fact that many statistical software packages include standard errors of variance component estimates in the output from a mixed model fitting procedure.  Creating confidence intervals in this way is optimistic at best. Completely nonsensical would be another way of characterizing this approach.A more reasonable alternative for constructing a 1 - alpha confidence interval from a bootstrap sample is to report a contiguous interval that contains a 1 - alpha proportion of the sample values.But there are many such intervals.  Suppose that a 95% confidence interval was to be constructed from one of the samples of size 10,000 of bootstrapped values.  To get a contigous interval the sample should be sorted. The sorted sample values, also called the order statistics of the sample, are denoted by a bracketed subscript.  That is, sigma_1 is the smallest value in the sample, sigma_2 is the second smallest, up to sigma_10000, which is the largest.One possible interval containing 95% of the sample is (sigma_1 sigma_9500).  Another is (sigma_2 sigma_9501) and so on up to (sigma_501sigma_10000).  There needs to be a method of choosing one of these intervals.  On approach would be to always choose the central 95% of the sample.  That is, cut off 2.5% of the sample on the left side and 2.5% on the right side.  julia> sigma95 = quantile(mm1bstp[:σ], [0.025, 0.975])\n2-element Array{Float64,1}:\n 35.583660074554146\n 63.09896877865457 \nThis approach has the advantage that the endpoints of a 95% interval on sigma^2 are the squares of the endpoints of a 95% interval on sigma.julia> isapprox(abs2.(sigma95), quantile(abs2.(mm1bstp[:σ]), [0.025, 0.975]))\ntrue\nThe intervals are compared with isapprox rather than exact equality because, in floating point arithmetic, it is not always the case that left(sqrtxright)^2 = x.  This comparison can also be expressed in Julia asjulia> abs2.(sigma95) ≈ quantile(abs2.(mm1bstp[:σ]), [0.025, 0.975])\ntrue\nAn alternative approach is to evaluate all of the contiguous intervals containing, say, 95% of the sample and return the shortest shortest such interval.  This is the equivalent of a Highest Posterior Density (HPD) interval sometimes used in Bayesian analysis.  If the procedure is applied to a unimodal (i.e. one that has only one peak or mode) theoretical probability density the resulting interval has the property that the density at the left endpoint is equal to the density at the right endpoint and that the density at any point outside the interval is less than the density at any point inside the interval.  Establishing this equivalence is left as an exercise for the mathematically inclined reader.  (Hint: Start with the interval defined by the \"equal density at the endpoints\" property and consider what happens if you shift that interval while maintaining the same area under the density curve.  You will be replacing a region of higher density by one with a lower density and the interval must become wider to maintain the same area.)With large samples a brute-force enumeration approach works.julia> function hpdinterval(v, level=0.95)\n    n = length(v)\n    if !(0 < level < 1)\n        throw(ArgumentError(\"level = $level must be in (0, 1)\"))\n    end\n    if (lbd = floor(Int, (1 - level) * n)) < 2\n        throw(ArgumentError(\n            \"level = $level is too large from sample size $n\"))\n    end\n    ordstat = sort(v)\n    leftendpts = ordstat[1:lbd]\n    rtendpts = ordstat[(1 + n - lbd):n]\n    (w, ind) = findmin(rtendpts - leftendpts)\n    return [leftendpts[ind], rtendpts[ind]]\nend\nhpdinterval (generic function with 2 methods)\nFor example, the 95% HPD interval calculated from the sample of beta_1 values isjulia> hpdinterval(mm1bstp[:β₁])\n2-element Array{Float64,1}:\n 1493.009507788377 \n 1562.0769558913405\nwhich is very close to the central probability interval ofjulia> quantile(mm1bstp[:β₁], [0.025, 0.975])\n2-element Array{Float64,1}:\n 1492.8482234420492\n 1561.9248091441677\nbecause the empirical distribution of the beta_1 sample is very similar to a normal distribution.  In particular, it is more-or-less symmetric and also unimodal.The HPD interval on sigma^2 is julia> hpdinterval(abs2.(mm1bstp[:σ]))\n2-element Array{Float64,1}:\n 1162.812380205002 \n 3834.3190914091965\nwhich is shifted to the left relative to the central probability intervaljulia> quantile(abs2.(mm1bstp[:σ]), [0.025, 0.975])\n2-element Array{Float64,1}:\n 1266.1968643014757\n 3981.4798610328708\nbecause the distribution of the sigma^2 sample is skewed to the right.  The HPD interval will truncate the lower density, long, right tail and include more of the higher density, short, left tail.The HPD interval does not have the property that the endpoints of the interval on sigma^2 are the squares of the endpoints of the intervals on sigma, because \"shorter\" on the scale of sigma does not necessarily correspond to shorter on the scale of sigma^2.julia> sigma95hpd = hpdinterval(mm1bstp[:σ])\n2-element Array{Float64,1}:\n 35.42542537228528\n 62.88749001676501\njulia> abs2.(sigma95hpd)\n2-element Array{Float64,1}:\n 1254.9607628073538\n 3954.8364006087186\nFinally, a 95% HPD interval on sigma_1 includes the boundary value sigma_1=0.julia> hpdinterval(mm1bstp[:σ₁])\n2-element Array{Float64,1}:\n  0.0             \n 54.59856859889488\nIn fact, the confidence level or coverage probability must be rather small before the boundary value is excludedjulia> hpdinterval(mm1bstp[:σ₁], 0.798)\n2-element Array{Float64,1}:\n  0.0             \n 42.33710948814182\njulia> hpdinterval(mm1bstp[:σ₁], 0.799)\n2-element Array{Float64,1}:\n  0.0              \n 42.394347640663185\n"
},

{
    "location": "SimpleLMM/#Empirical-cumulative-distribution-function-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Empirical cumulative distribution function",
    "category": "section",
    "text": "The empirical cumulative distribution function (ecdf) of a sample maps the range of the sample onto [0,1] by x → proportion of sample ≤ x.  In general this is a \"step function\", which takes jumps of size 1/length(samp) at each observed sample value.  For large samples, we can plot it as a qq plot where the theoretical quantiles are the probability points and are on the vertical axis.(Image: )The orange lines added to the plot show the construction of the central probability 80% confidence interval on sigma_1 and the red lines show the 80% HPD interval.  Comparing the spacing of the left end points to that of the right end points shows that the HPD interval is shorter, because, in switching from the orange to the red lines, the right end point moves further to the left than does the left end point.The differences in the widths becomes more dramatic on the scale of sigma_1^2(Image: )ADD: similar analysis for mm2"
},

{
    "location": "SimpleLMM/#Assessing-the-Random-Effects-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Assessing the Random Effects",
    "category": "section",
    "text": "In Sect. [sec:definitions] we mentioned that what are sometimes called the BLUPs (or best linear unbiased predictors) of the random effects, mathcal B, are the conditional modes evaluated at the parameter estimates, calculated as tildeb_widehattheta=Lambda_widehatthetatildeu_widehattheta.These values are often considered as some sort of “estimates” of the random effects. It can be helpful to think of them this way but it can also be misleading. As we have stated, the random effects are not, strictly speaking, parameters—they are unobserved random variables. We don’t estimate the random effects in the same sense that we estimate parameters. Instead, we consider the conditional distribution of mathcal B given the observed data, (mathcal Bmathcal Y=mathbf  y).Because the unconditional distribution, mathcal BsimmathcalN(mathbf  0Sigma_theta) is continuous, the conditional distribution, (mathcal Bmathcal Y=mathbf  y) will also be continuous. In general, the mode of a probability density is the point of maximum density, so the phrase “conditional mode” refers to the point at which this conditional density is maximized. Because this definition relates to the probability model, the values of the parameters are assumed to be known. In practice, of course, we don’t know the values of the parameters (if we did there would be no purpose in forming the parameter estimates), so we use the estimated values of the parameters to evaluate the conditional modes.Those who are familiar with the multivariate Gaussian distribution may recognize that, because both mathcal B and (mathcal Ymathcal B=mathbf  b) are multivariate Gaussian, (mathcal Bmathcal Y=mathbf  y) will also be multivariate Gaussian and the conditional mode will also be the conditional mean of mathcal B, given mathcal Y=mathbf  y. This is the case for a linear mixed model but it does not carry over to other forms of mixed models. In the general case all we can say about tildemathbf    u or tildemathbf  b is that they maximize a conditional density, which is why we use the term “conditional mode” to describe these values. We will only use the term “conditional mean” and the symbol, mathbf mu, in reference to mathrmE(mathcal Ymathcal B=mathbf  b), which is the conditional mean of mathcal Y given mathcal B, and an important part of the formulation of all types of mixed-effects models.The ranef extractor returns the conditional modes.julia> ranef(mm1)  # FIXME return an ordered dict\n1-element Array{Array{Float64,2},1}:\n [-16.6282 0.369516 … 53.5798 -42.4943]\nThe result is an array of matrices, one for each random effects term in the model.  In this case there is only one matrix because there is only one random-effects term, (1 | Batch), in the model. There is only one row in this matrix because the random-effects term, (1 | Batch), is a simple, scalar term.To make this more explicit, random-effects terms in the model formula are those that contain the vertical bar () character. The variable is the grouping factor for the random effects generated by this term. An expression for the grouping factor, usually just the name of a variable, occurs to the right of the vertical bar. If the expression on the left of the vertical bar is , as it is here, we describe the term as a simple, scalar, random-effects term. The designation “scalar” means there will be exactly one random effect generated for each level of the grouping factor. A simple, scalar term generates a block of indicator columns — the indicators for the grouping factor — in mathbf Z. Because there is only one random-effects term in this model and because that term is a simple, scalar term, the model matrix, 𝐙, for this model is the indicator matrix for the levels of Batch.In the next chapter we fit models with multiple simple, scalar terms and, in subsequent chapters, we extend random-effects terms beyond simple, scalar terms. When we have only simple, scalar terms in the model, each term has a unique grouping factor and the elements of the list returned by can be considered as associated with terms or with grouping factors. In more complex models a particular grouping factor may occur in more than one term, in which case the elements of the list are associated with the grouping factors, not the terms.Given the data, 𝐲, and the parameter estimates, we can evaluate a measure of the dispersion of (mathcal Bmathcal Y=mathbf y). In the case of a linear mixed model, this is the conditional standard deviation, from which we can obtain a prediction interval. The extractor is named condVar.julia> condVar(mm1)\n1-element Array{Array{Float64,3},1}:\n [362.31]\n\n[362.31]\n\n[362.31]\n\n[362.31]\n\n[362.31]\n\n[362.31]\n"
},

{
    "location": "SimpleLMM/#Chapter-Summary-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Chapter Summary",
    "category": "section",
    "text": "A considerable amount of material has been presented in this chapter, especially considering the word “simple” in its title (it’s the model that is simple, not the material). A summary may be in order.A mixed-effects model incorporates fixed-effects parameters and random effects, which are unobserved random variables, mathcal B. In a linear mixed model, both the unconditional distribution of mathcal B and the conditional distribution, (mathcal Ymathcal B=mathbf b), are multivariate Gaussian distributions. Furthermore, this conditional distribution is a spherical Gaussian with mean, mathbfmu, determined by the linear predictor, mathbf Zmathbf b+mathbf Xmathbfbeta. That is,\\begin{equation}(\\mathcal Y|\\mathcal B=\\mathbf b)\\sim   \\mathcal{N}(\\mathbf Z\\mathbf b+\\mathbf X\\mathbf\\beta, \\sigma^2\\mathbf I_n) .\\end{equation}The unconditional distribution of mathcal B has mean mathbf 0 and a parameterized qtimes q variance-covariance matrix, Sigma_theta.In the models we considered in this chapter, Sigma_theta, is a simple multiple of the identity matrix, mathbf I_6. This matrix is always a multiple of the identity in models with just one random-effects term that is a simple, scalar term. The reason for introducing all the machinery that we did is to allow for more general model specifications.The maximum likelihood estimates of the parameters are obtained by minimizing the deviance. For linear mixed models we can minimize the profiled deviance, which is a function of mathbftheta only, thereby considerably simplifying the optimization problem.To assess the precision of the parameter estimates, we profile the deviance function with respect to each parameter and apply a signed square root transformation to the likelihood ratio test statistic, producing a profile zeta function for each parameter. These functions provide likelihood-based confidence intervals for the parameters. Profile zeta plots allow us to visually assess the precision of individual parameters. Density plots derived from the profile zeta function provide another way of examining the distribution of the estimators of the parameters.Prediction intervals from the conditional distribution of the random effects, given the observed data, allow us to assess the precision of the random effects."
},

{
    "location": "SimpleLMM/#Notation-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Notation",
    "category": "section",
    "text": ""
},

{
    "location": "SimpleLMM/#Random-Variables-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Random Variables",
    "category": "section",
    "text": "mathcal Y\n: The responses (n-dimensional Gaussian)\nmathcal B\n: The random effects on the original scale (q-dimensional Gaussian with mean mathbf 0)\nmathcal U\n: The orthogonal random effects (q-dimensional spherical Gaussian)Values of these random variables are denoted by the corresponding bold-face, lower-case letters: mathbf y, mathbf b and mathbf u. We observe mathbf y. We do not observe mathbf b or mathbf u."
},

{
    "location": "SimpleLMM/#Parameters-in-the-Probability-Model-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Parameters in the Probability Model",
    "category": "section",
    "text": "mathbfbeta\n: The p-dimension fixed-effects parameter vector.\nmathbftheta\n: The variance-component parameter vector. Its (unnamed) dimension is typically very small. Dimensions of 1, 2 or 3 are common in practice.\nsigma\n: The (scalar) common scale parameter, sigma0. It is called the common scale parameter because it is incorporated in the variance-covariance matrices of both mathcal Y and mathcal U.\nmathbftheta\nThe \"covariance\" parameter vector which determines the qtimes q lower triangular matrix Lambda_theta, called the relative covariance factor, which, in turn, determines the qtimes q sparse, symmetric semidefinite variance-covariance matrix Sigma_theta=sigma^2Lambda_thetaLambda_theta that defines the distribution of mathcal B."
},

{
    "location": "SimpleLMM/#Model-Matrices-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Model Matrices",
    "category": "section",
    "text": "mathbf X\n: Fixed-effects model matrix of size ntimes p.\nmathbf Z\n: Random-effects model matrix of size ntimes q."
},

{
    "location": "SimpleLMM/#Derived-Matrices-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Derived Matrices",
    "category": "section",
    "text": "mathbf L_theta\n: The sparse, lower triangular Cholesky factor of Lambda_thetamathbf Zmathbf ZLambda_theta+mathbf I_q"
},

{
    "location": "SimpleLMM/#Vectors-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Vectors",
    "category": "section",
    "text": "In addition to the parameter vectors already mentioned, we definemathbf y\n: the n-dimensional observed response vector\nmathbfeta\n: the n-dimension linear predictor,\\begin{equation}   \\mathbf{\\eta=X\\beta+Zb=Z\\Lambda_\\theta u+X\\beta} \\end{equation}mathbfmu\n: the n-dimensional conditional mean of mathcal Y given mathcal B=mathbf b (or, equivalently, given mathcal U=mathbf u)\\begin{equation}   \\mathbf\\mu=\\mathrm{E}[\\mathcal Y|\\mathcal B=\\mathbf b]=\\mathrm{E}[\\mathcal Y|\\mathcal U=\\mathbf u] \\end{equation}tildeu_theta\n: the q-dimensional conditional mode (the value at which the conditional density is maximized) of mathcal U given mathcal Y=mathbf y."
},

{
    "location": "SimpleLMM/#Exercises-1",
    "page": "A Simple, Linear, Mixed-effects Model",
    "title": "Exercises",
    "category": "section",
    "text": "These exercises and several others in this book use data sets from the package for . You will need to ensure that this package is installed before you can access the data sets.To load a particular data set,or load just the one data setCheck the documentation, the structure () and a summary of the Rail data (Fig. [fig:Raildot]).\nFit a model with as the response and a simple, scalar random-effects term for the variable Rail. Create a dotplot of the conditional modes of the random effects.\nCreate a bootstrap simulation from the model and construct 95% bootstrap-based confidence intervals on the parameters. Is the confidence interval on sigma_1 close to being symmetric about the estimate? Is the corresponding interval on log(sigma_1) close to being symmetric about its estimate?\nCreate the profile zeta plot for this model. For which parameters are there good normal approximations?\nPlot the prediction intervals on the random effects from this model. Do any of these prediction intervals contain zero? Consider the relative magnitudes of widehatsigma_1 and widehatsigma in this model compared to those in model for the data. Should these ratios of sigma_1sigma lead you to expect a different pattern of prediction intervals in this plot than those in Fig. [fig:fm01preddot]?"
},

{
    "location": "MultipleTerms/#",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Models With Multiple Random-effects Terms",
    "category": "page",
    "text": ""
},

{
    "location": "MultipleTerms/#Models-With-Multiple-Random-effects-Terms-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Models With Multiple Random-effects Terms",
    "category": "section",
    "text": "The mixed models considered in the previous chapter had only one random-effects term, which was a simple, scalar random-effects term, and a single fixed-effects coefficient. Although such models can be useful, it is with the facility to use multiple random-effects terms and to use random-effects terms beyond a simple, scalar term that we can begin to realize the flexibility and versatility of mixed models.In this chapter we consider models with multiple simple, scalar random-effects terms, showing examples where the grouping factors for these terms are in completely crossed or nested or partially crossed configurations. For ease of description we will refer to the random effects as being crossed or nested although, strictly speaking, the distinction between nested and non-nested refers to the grouping factors, not the random effects.julia> using DataFrames, Distributions, FreqTables, MixedModels, RData, Random\n\njulia> using Gadfly\n\njulia> using Gadfly.Geom: density, histogram, line, point\n\njulia> using Gadfly.Guide: xlabel, ylabel\n\njulia> const dat = Dict(Symbol(k)=>v for (k,v) in \n    load(joinpath(dirname(pathof(MixedModels)), \"..\", \"test\", \"dat.rda\")));\n\njulia> const ppt250 = inv(500) : inv(250) : 1.;\n\njulia> const zquantiles = quantile.(Normal(), ppt250);\n\njulia> function hpdinterval(v, level=0.95)\n    n = length(v)\n    if !(0 < level < 1)\n        throw(ArgumentError(\"level = $level must be in (0, 1)\"))\n    end\n    if (lbd = floor(Int, (1 - level) * n)) < 2\n        throw(ArgumentError(\n            \"level = $level is too large from sample size $n\"))\n    end\n    ordstat = sort(v)\n    leftendpts = ordstat[1:lbd]\n    rtendpts = ordstat[(1 + n - lbd):n]\n    (w, ind) = findmin(rtendpts - leftendpts)\n    return [leftendpts[ind], rtendpts[ind]]\nend\nhpdinterval (generic function with 2 methods)\n"
},

{
    "location": "MultipleTerms/#A-Model-With-Crossed-Random-Effects-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "A Model With Crossed Random Effects",
    "category": "section",
    "text": "One of the areas in which the methods in the package for are particularly effective is in fitting models to cross-classified data where several factors have random effects associated with them. For example, in many experiments in psychology the reaction of each of a group of subjects to each of a group of stimuli or items is measured. If the subjects are considered to be a sample from a population of subjects and the items are a sample from a population of items, then it would make sense to associate random effects with both these factors.In the past it was difficult to fit mixed models with multiple, crossed grouping factors to large, possibly unbalanced, data sets. The methods in the package are able to do this. To introduce the methods let us first consider a small, balanced data set with crossed grouping factors."
},

{
    "location": "MultipleTerms/#The-Penicillin-Data-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "The Penicillin Data",
    "category": "section",
    "text": "The data are derived from Table 6.6, p. 144 of Davies (), where they are described as coming from an investigation toassess the variability between samples of penicillin by the B. subtilis method. In this test method a bulk-innoculated nutrient agar medium is poured into a Petri dish of approximately 90 mm. diameter, known as a plate. When the medium has set, six small hollow cylinders or pots (about 4 mm. in diameter) are cemented onto the surface at equally spaced intervals. A few drops of the penicillin solutions to be compared are placed in the respective cylinders, and the whole plate is placed in an incubator for a given time. Penicillin diffuses from the pots into the agar, and this produces a clear circular zone of inhibition of growth of the organisms, which can be readily measured. The diameter of the zone is related in a known way to the concentration of penicillin in the solution.julia> describe(dat[:Penicillin])\n3×8 DataFrames.DataFrame. Omitted printing of 1 columns\n│ Row │ variable │ mean    │ min  │ median │ max  │ nunique │ nmissing │\n│     │ Symbol   │ Union…  │ Any  │ Union… │ Any  │ Union…  │ Nothing  │\n├─────┼──────────┼─────────┼──────┼────────┼──────┼─────────┼──────────┤\n│ 1   │ Y        │ 22.9722 │ 18.0 │ 23.0   │ 27.0 │         │          │\n│ 2   │ G        │         │ a    │        │ x    │ 24      │          │\n│ 3   │ H        │         │ A    │        │ F    │ 6       │          │\nof the data, then plot itThe variation in the diameter is associated with the plates and with the samples. Because each plate is used only for the six samples shown here we are not interested in the contributions of specific plates as much as we are interested in the variation due to plates and in assessing the potency of the samples after accounting for this variation. Thus, we will use random effects for the factor. We will also use random effects for the factor because, as in the Dyestuff example, we are more interested in the sample-to-sample variability in the penicillin samples than in the potency of a particular sample.In this experiment each sample is used on each plate. We say that the and factors are crossed, as opposed to nested factors, which we will describe in the next section. By itself, the designation “crossed” just means that the factors are not nested. If we wish to be more specific, we could describe these factors as being completely crossed, which means that we have at least one observation for each combination of a level of G and a level of H. We can see this in Fig. [fig:Penicillindot] and, because there are moderate numbers of levels in these factors, we can check it in a cross-tabulationjulia> freqtable(dat[:Penicillin][:H], dat[:Penicillin][:G])\n6×24 Named Array{Int64,2}\nDim1 ╲ Dim2 │ a  b  c  d  e  f  g  h  i  j  …  o  p  q  r  s  t  u  v  w  x\n────────────┼──────────────────────────────────────────────────────────────\nA           │ 1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1\nB           │ 1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1\nC           │ 1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1\nD           │ 1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1\nE           │ 1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1\nF           │ 1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1\nLike the Dyestuff data, the factors in the Penicillin data are balanced. That is, there are exactly the same number of observations on each plate and for each sample and, furthermore, there is the same number of observations on each combination of levels. In this case there is exactly one observation for each combination of G, the plate, and H, the sample. We would describe the configuration of these two factors as an unreplicated, completely balanced, crossed design.In general, balance is a desirable but precarious property of a data set. We may be able to impose balance in a designed experiment but we typically cannot expect that data from an observation study will be balanced. Also, as anyone who analyzes real data soon finds out, expecting that balance in the design of an experiment will produce a balanced data set is contrary to “Murphy’s Law”. That’s why statisticians allow for missing data. Even when we apply each of the six samples to each of the 24 plates, something could go wrong for one of the samples on one of the plates, leaving us without a measurement for that combination of levels and thus an unbalanced data set."
},

{
    "location": "MultipleTerms/#A-Model-For-the-Penicillin-Data-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "A Model For the Penicillin Data",
    "category": "section",
    "text": "A model incorporating random effects for both the plate and the sample is straightforward to specify — we include simple, scalar random effects terms for both these factors.julia> penm = fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G) + (1|H)), dat[:Penicillin]))\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G) + (1 | H)\n   logLik   -2 logLik     AIC        BIC    \n -166.09417  332.18835  340.18835  352.06760\n\nVariance components:\n              Column    Variance   Std.Dev. \n G        (Intercept)  0.71497949 0.8455646\n H        (Intercept)  3.13519326 1.7706477\n Residual              0.30242640 0.5499331\n Number of obs: 144; levels of grouping factors: 24, 6\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   22.9722  0.744596 30.8519  <1e-99\n\nThis model display indicates that the sample-to-sample variability has the greatest contribution, then plate-to-plate variability and finally the “residual” variability that cannot be attributed to either the sample or the plate. These conclusions are consistent with what we see in the data plot (Fig. [fig:Penicillindot]).The prediction intervals on the random effects (Fig. [fig:fm03ranef])confirm that the conditional distribution of the random effects for has much less variability than does the conditional distribution of the random effects for , in the sense that the dots in the bottom panel have less variability than those in the top panel. (Note the different horizontal axes for the two panels.) However, the conditional distribution of the random effect for a particular , say sample F, has less variability than the conditional distribution of the random effect for a particular plate, say plate m. That is, the lines in the bottom panel are wider than the lines in the top panel, even after taking the different axis scales into account. This is because the conditional distribution of the random effect for a particular sample depends on 24 responses while the conditional distribution of the random effect for a particular plate depends on only 6 responses.In chapter [chap:ExamLMM] we saw that a model with a single, simple, scalar random-effects term generated a random-effects model matrix, mathbf Z, that is the matrix of indicators of the levels of the grouping factor. When we have multiple, simple, scalar random-effects terms, as in model , each term generates a matrix of indicator columns and these sets of indicators are concatenated to form the model matrix mathbf Z. The transpose of this matrix, shown in Fig. [fig:fm03Ztimage], contains rows of indicators for each factor.The relative covariance factor, Lambda_theta, (Fig. [fig:fm03LambdaLimage], left panel) is no longer a multiple of the identity. It is now block diagonal, with two blocks, one of size 24 and one of size 6, each of which is a multiple of the identity. The diagonal elements of the two blocks are theta_1 and theta_2, respectively. The numeric values of these parameters can be obtained asjulia> show(penm.θ)\n[1.53758, 3.21975]or as the Final parameter vector in the opsum field of penmjulia> penm.optsum\nInitial parameter vector: [1.0, 1.0]\nInitial objective value:  364.62677981659544\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [0.0, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     44\nFinal parameter vector:   [1.53758, 3.21975]\nFinal objective value:    332.18834867237246\nReturn code:              FTOL_REACHED\n\nThe first parameter is the relative standard deviation of the random effects for plate, which has the value 0.8455646/0.5499331 = 1.53758 at convergence, and the second is the relative standard deviation of the random effects for sample (1.7706475/0.5499331 = 3.21975).Because Lambda_theta is diagonal, the pattern of non-zeros in Lambda_thetamathbf Zmathbf ZLambda_theta+mathbf I will be the same as that in mathbf Zmathbf Z, shown in the middle panel of Fig. [fig:fm03LambdaLimage]. The sparse Cholesky factor, mathbf L, shown in the right panel, is lower triangular and has non-zero elements in the lower right hand corner in positions where mathbf Zmathbf Z has systematic zeros. We say that “fill-in” has occurred when forming the sparse Cholesky decomposition. In this case there is a relatively minor amount of fill but in other cases there can be a substantial amount of fill and we shall take precautions so as to reduce this, because fill-in adds to the computational effort in determining the MLEs or the REML estimates.A bootstrap simulation of the modeljulia> @time penmbstp = bootstrap(10000, penm);\n 26.932617 seconds (77.19 M allocations: 2.076 GiB, 3.17% gc time)\nprovides the density plots(Image: )(Image: )(Image: )julia> plot(penmbstp, x = :σ₂, density, xlabel(\"σ₂\"))\nPlot(...)\nA profile zeta plot (Fig. [fig:fm03prplot]) for the parameters in model(Image: Profile zeta plot of the parameters in model .)[fig:fm03prplot]leads to conclusions similar to those from Fig. [fig:fm1prof] for model in the previous chapter. The fixed-effect parameter, beta_1, for the constant term has symmetric intervals and is over-dispersed relative to the normal distribution. The logarithm of sigma has a good normal approximation but the standard deviations of the random effects, sigma_1 and sigma_2, are skewed. The skewness for sigma_2 is worse than that for sigma_1, because the estimate of sigma_2 is less precise than that of sigma_1, in both absolute and relative senses. For an absolute comparison we compare the widths of the confidence intervals for these parameters.                     2.5 %     97.5 %\n    .sig01       0.6335658  1.1821040\n    .sig02       1.0957893  3.5562919\n    .sigma       0.4858454  0.6294535\n    (Intercept) 21.2666274 24.6778176In a relative comparison we examine the ratio of the endpoints of the interval divided by the estimate.\n               2.5 %   97.5 %\n    .sig01 0.7492746 1.397993\n    .sig02 0.6188634 2.008469The lack of precision in the estimate of sigma_2 is a consequence of only having 6 distinct levels of the factor. The factor, on the other hand, has 24 distinct levels. In general it is more difficult to estimate a measure of spread, such as the standard deviation, than to estimate a measure of location, such as a mean, especially when the number of levels of the factor is small. Six levels are about the minimum number required for obtaining sensible estimates of standard deviations for simple, scalar random effects terms.shows patterns similar to those in Fig. [fig:fm1profpair] for pairs of parameters in model fit to the data. On the zeta scale (panels below the diagonal) the profile traces are nearly straight and orthogonal with the exception of the trace of zeta(sigma_2) on zeta(beta_0) (the horizontal trace for the panel in the (42) position). The pattern of this trace is similar to the pattern of the trace of zeta(sigma_1) on zeta(beta_1) in Fig. [fig:fm1profpair]. Moving beta_0 from its estimate, widehatbeta_0, in either direction will increase the residual sum of squares. The increase in the residual variability is reflected in an increase of one or more of the dispersion parameters. The balanced experimental design results in a fixed estimate of sigma and the extra apparent variability must be incorporated into sigma_1 or sigma_2.Contours in panels of parameter pairs on the original scales (i.e. panels above the diagonal) can show considerable distortion from the ideal elliptical shape. For example, contours in the sigma_2 versus sigma_1 panel (the (12) position) and the log(sigma) versus sigma_2 panel (in the (23) position) are dramatically non-elliptical. However, the distortion of the contours is not due to these parameter estimates depending strongly on each other. It is almost entirely due to the choice of scale for sigma_1 and sigma_2. When we plot the contours on the scale of log(sigma_1) and log(sigma_2) instead (Fig. [fig:lpr03pairs])(Image: Profile pairs plot for the parameters in model fit to the data. In this plot the parameters $\\sigma_1$ and $\\sigma_2$ are on the scale of the natural logarithm, as is the parameter $\\sigma$ in this and other profile pairs plots.)[fig:fm03lprpairs]they are much closer to the elliptical pattern.Conversely, if we tried to plot contours on the scale of sigma_1^2 and sigma_2^2 (not shown), they would be hideously distorted."
},

{
    "location": "MultipleTerms/#A-Model-With-Nested-Random-Effects-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "A Model With Nested Random Effects",
    "category": "section",
    "text": "In this section we again consider a simple example, this time fitting a model with nested grouping factors for the random effects."
},

{
    "location": "MultipleTerms/#The-Pastes-Data-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "The Pastes Data",
    "category": "section",
    "text": "The third example from Davies (1972) is described as coming fromdeliveries of a chemical paste product contained in casks where, in addition to sampling and testing errors, there are variations in quality between deliveries …As a routine, three casks selected at random from each delivery were sampled and the samples were kept for reference. …Ten of the delivery batches were sampled at random and two analytical tests carried out on each of the 30 samples.The structure and summary of the data object arejulia> describe(dat[:Pastes])\n4×8 DataFrames.DataFrame. Omitted printing of 1 columns\n│ Row │ variable │ mean    │ min  │ median │ max  │ nunique │ nmissing │\n│     │ Symbol   │ Union…  │ Any  │ Union… │ Any  │ Union…  │ Nothing  │\n├─────┼──────────┼─────────┼──────┼────────┼──────┼─────────┼──────────┤\n│ 1   │ Y        │ 60.0533 │ 54.2 │ 59.3   │ 66.0 │         │          │\n│ 2   │ H        │         │ A    │        │ J    │ 10      │          │\n│ 3   │ c        │         │ a    │        │ c    │ 3       │          │\n│ 4   │ G        │         │ A:a  │        │ J:c  │ 30      │          │\nAs stated in the description in Davies (1972), there are 30 samples, three from each of the 10 delivery batches. We have labelled the levels of the sample factor with the label of the batch factor followed by a, b or c to distinguish the three samples taken from that batch.julia> freqtable(dat[:Pastes][:H], dat[:Pastes][:G])\n10×30 Named Array{Int64,2}\nDim1 ╲ Dim2 │ A:a  A:b  A:c  B:a  B:b  B:c  …  I:a  I:b  I:c  J:a  J:b  J:c\n────────────┼──────────────────────────────────────────────────────────────\nA           │   2    2    2    0    0    0  …    0    0    0    0    0    0\nB           │   0    0    0    2    2    2       0    0    0    0    0    0\nC           │   0    0    0    0    0    0       0    0    0    0    0    0\nD           │   0    0    0    0    0    0       0    0    0    0    0    0\nE           │   0    0    0    0    0    0       0    0    0    0    0    0\nF           │   0    0    0    0    0    0       0    0    0    0    0    0\nG           │   0    0    0    0    0    0       0    0    0    0    0    0\nH           │   0    0    0    0    0    0       0    0    0    0    0    0\nI           │   0    0    0    0    0    0       2    2    2    0    0    0\nJ           │   0    0    0    0    0    0  …    0    0    0    2    2    2\nWhen plotting the strength versus sample and in the data we should remember that we have two strength measurements on each of the 30 samples. It is tempting to use the cask designation (‘a’, ‘b’ and ‘c’) to determine, say, the plotting symbol within a sample. It would be fine to do this within a batch but the plot would be misleading if we used the same symbol for cask ‘a’ in different batches. There is no relationship between cask ‘a’ in batch ‘A’ and cask ‘a’ in batch ‘B’. The labels ‘a’, ‘b’ and ‘c’ are used only to distinguish the three samples within a batch; they do not have a meaning across batches.(Image: Strength of paste preparations according to the and the within the batch. There were two strength measurements on each of the 30 samples; three samples each from 10 batches.)[fig:Pastesplot]In Fig. [fig:Pastesplot] we plot the two strength measurements on each of the samples within each of the batches and join up the average strength for each sample. The perceptive reader will have noticed that the levels of the factors on the vertical axis in this figure, and in Fig. [fig:Dyestuffdot] and [fig:Penicillindot], have been reordered according to increasing average response. In all these cases there is no inherent ordering of the levels of the covariate such as or . Rather than confuse our interpretation of the plot by determining the vertical displacement of points according to a random ordering, we impose an ordering according to increasing mean response. This allows us to more easily check for structure in the data, including undesirable characteristics like increasing variability of the response with increasing mean level of the response.In Fig. [fig:Pastesplot] we order the samples within each batch separately then order the batches according to increasing mean strength.Figure [fig:Pastesplot] shows considerable variability in strength between samples relative to the variability within samples. There is some indication of variability between batches, in addition to the variability induced by the samples, but not a strong indication of a batch effect. For example, batches I and D, with low mean strength relative to the other batches, each contained one sample (I:b and D:c, respectively) that had high mean strength relative to the other samples. Also, batches H and C, with comparatively high mean batch strength, contain samples H:a and C:a with comparatively low mean sample strength. In Sect. [sec:TestingSig2is0] we will examine the need for incorporating batch-to-batch variability, in addition to sample-to-sample variability, in the statistical model."
},

{
    "location": "MultipleTerms/#Nested-Factors-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Nested Factors",
    "category": "section",
    "text": "Because each level of occurs with one and only one level of we say that is nested within . Some presentations of mixed-effects models, especially those related to multilevel modeling  or hierarchical linear models , leave the impression that one can only define random effects with respect to factors that are nested. This is the origin of the terms “multilevel”, referring to multiple, nested levels of variability, and “hierarchical”, also invoking the concept of a hierarchy of levels. To be fair, both those references do describe the use of models with random effects associated with non-nested factors, but such models tend to be treated as a special case.The blurring of mixed-effects models with the concept of multiple, hierarchical levels of variation results in an unwarranted emphasis on “levels” when defining a model and leads to considerable confusion. It is perfectly legitimate to define models having random effects associated with non-nested factors. The reasons for the emphasis on defining random effects with respect to nested factors only are that such cases do occur frequently in practice and that some of the computational methods for estimating the parameters in the models can only be easily applied to nested factors.This is not the case for the methods used in the package. Indeed there is nothing special done for models with random effects for nested factors. When random effects are associated with multiple factors exactly the same computational methods are used whether the factors form a nested sequence or are partially crossed or are completely crossed. There is, however, one aspect of nested grouping factors that we should emphasize, which is the possibility of a factor that is implicitly nested within another factor. Suppose, for example, that the factor was defined as having three levels instead of 30 with the implicit assumption that is nested within . It may seem silly to try to distinguish 30 different batches with only three levels of a factor but, unfortunately, data are frequently organized and presented like this, especially in text books. The factor in the data is exactly such an implicitly nested factor. If we cross-tabulate cask and batch        batch\n    cask A B C D E F G H I J\n       a 2 2 2 2 2 2 2 2 2 2\n       b 2 2 2 2 2 2 2 2 2 2\n       c 2 2 2 2 2 2 2 2 2 2we get the impression that the and factors are crossed, not nested. If we know that the cask should be considered as nested within the batch then we should create a new categorical variable giving the batch-cask combination, which is exactly what the factor is. A simple way to create such a factor is to use the interaction operator, ‘’, on the factors. It is advisable, but not necessary, to apply to the result thereby dropping unused levels of the interaction from the set of all possible levels of the factor. (An “unused level” is a combination that does not occur in the data.) A convenient code idiom isorIn a small data set like we can quickly detect a factor being implicitly nested within another factor and take appropriate action. In a large data set, perhaps hundreds of thousands of test scores for students in thousands of schools from hundreds of school districts, it is not always obvious if school identifiers are unique across the entire data set or just within a district. If you are not sure, the safest thing to do is to create the interaction factor, as shown above, so you can be confident that levels of the district:school interaction do indeed correspond to unique schools."
},

{
    "location": "MultipleTerms/#Fitting-a-Model-With-Nested-Random-Effects-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Fitting a Model With Nested Random Effects",
    "category": "section",
    "text": "Fitting a model with simple, scalar random effects for nested factors is done in exactly the same way as fitting a model with random effects for crossed grouping factors. We include random-effects terms for each factor, as injulia> pstsm = fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G) + (1|H)), dat[:Pastes]))\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G) + (1 | H)\n   logLik   -2 logLik     AIC        BIC    \n -123.99723  247.99447  255.99447  264.37184\n\nVariance components:\n              Column    Variance  Std.Dev.  \n G        (Intercept)  8.4336167 2.90406899\n H        (Intercept)  1.1991787 1.09507018\n Residual              0.6780021 0.82340886\n Number of obs: 60; levels of grouping factors: 30, 10\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   60.0533  0.642136 93.5212  <1e-99\n\nNot only is the model specification similar for nested and crossed factors, the internal calculations are performed according to the methods described in Sect. [sec:definitions] for each model type. Comparing the patterns in the matrices Lambda, mathbf Zmathbf Z and mathbf L for this model (Fig. [fig:fm04LambdaLimage]) to those in Fig. [fig:fm03LambdaLimage] shows that models with nested factors produce simple repeated structures along the diagonal of the sparse Cholesky factor, mathbf L. This type of structure has the desirable property that there is no “fill-in” during calculation of the Cholesky factor. In other words, the number of non-zeros in mathbf L is the same as the number of non-zeros in the lower triangle of the matrix being factored, Lambdamathbf Zmathbf ZLambda+mathbf I (which, because Lambda is diagonal, has the same structure as mathbf Zmathbf Z)."
},

{
    "location": "MultipleTerms/#Assessing-Parameter-Estimates-in-Model-pstsm-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Assessing Parameter Estimates in Model pstsm",
    "category": "section",
    "text": "The parameter estimates are: widehatsigma_1=2.904, the standard deviation of the random effects for sample; widehatsigma_2=1.095, the standard deviation of the random effects for batch; widehatsigma=0.823, the standard deviation of the residual noise term; and widehatbeta_0=60.053, the overall mean response, which is labeled (Intercept) in these models.The estimated standard deviation for sample is nearly three times as large as that for batch, which confirms what we saw in Fig. [fig:Pastesplot]. Indeed our conclusion from Fig. [fig:Pastesplot] was that there may not be a significant batch-to-batch variability in addition to the sample-to-sample variability.Plots of the prediction intervals of the random effects (Fig. [fig:fm04ranef])(Image: 95% prediction intervals on the random effects for model fit to the data.)[fig:fm04ranef]confirm this impression in that all the prediction intervals for the random effects for contain zero. Furthermore, a bootstrap samplejulia> Random.seed!(4321234);\n\njulia> @time pstsbstp = bootstrap(10000, pstsm);\n 21.205995 seconds (66.25 M allocations: 1.788 GiB, 3.44% gc time)\n(Image: )julia> plot(pstsbstp, x = :σ, density, xlabel(\"σ\"))\nPlot(...)\njulia> plot(x = pstsbstp[:σ₁], Geom.density(), Guide.xlabel(\"σ₁\"))\nPlot(...)\njulia> plot(x = pstsbstp[:σ₂], Geom.density(), Guide.xlabel(\"σ₂\"))\nPlot(...)\nand a normal probability plot ofjulia> plot(x = zquantiles, y = quantile(pstsbstp[:σ₂], ppt250), Geom.line,\n    Guide.xlabel(\"Standard Normal Quantiles\"), Guide.ylabel(\"σ₂\"))\nPlot(...)\njulia> count(x -> x < 1.0e-5, pstsbstp[:σ₂])\n3669\nOver 1/3 of the bootstrap samples of sigma_2 are zero.  Even a 50% confidence interval on this parameter will extend to zero.  One way to calculate confidence intervals based on a bootstrap sample is sort the sample and consider all the contiguous intervals that contain, say, 95% of the samples then choose the smallest of these.  For example,julia> hpdinterval(pstsbstp[:σ₂])\n2-element Array{Float64,1}:\n 0.0              \n 2.073479680297123\nprovides the confidence interval(Image: Profile zeta plots for the parameters in model .)[fig:fm04prplot]shows that the even the 50% profile-based confidence interval on sigma_2 extends to zero.Because there are several indications that sigma_2 could reasonably be zero, resulting in a simpler model incorporating random effects for only, we perform a statistical test of this hypothesis."
},

{
    "location": "MultipleTerms/#Testing-H_0:\\sigma_20-Versus-H_a:\\sigma_20-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Testing H_0sigma_2=0 Versus H_asigma_20",
    "category": "section",
    "text": "One of the many famous statements attributed to Albert Einstein is “Everything should be made as simple as possible, but not simpler.” In statistical modeling this principal of parsimony is embodied in hypothesis tests comparing two models, one of which contains the other as a special case. Typically, one or more of the parameters in the more general model, which we call the alternative hypothesis, is constrained in some way, resulting in the restricted model, which we call the null hypothesis. Although we phrase the hypothesis test in terms of the parameter restriction, it is important to realize that we are comparing the quality of fits obtained with two nested models. That is, we are not assessing parameter values per se; we are comparing the model fit obtainable with some constraints on parameter values to that without the constraints.Because the more general model, H_a, must provide a fit that is at least as good as the restricted model, H_0, our purpose is to determine whether the change in the quality of the fit is sufficient to justify the greater complexity of model H_a. This comparison is often reduced to a p-value, which is the probability of seeing a difference in the model fits as large as we did, or even larger, when, in fact, H_0 is adequate. Like all probabilities, a p-value must be between 0 and 1. When the p-value for a test is small (close to zero) we prefer the more complex model, saying that we “reject H_0 in favor of H_a”. On the other hand, when the p-value is not small we “fail to reject H_0”, arguing that there is a non-negligible probability that the observed difference in the model fits could reasonably be the result of random chance, not the inherent superiority of the model H_a. Under these circumstances we prefer the simpler model, H_0, according to the principal of parsimony.These are the general principles of statistical hypothesis tests. To perform a test in practice we must specify the criterion for comparing the model fits, the method for calculating the p-value from an observed value of the criterion, and the standard by which we will determine if the p-value is “small” or not. The criterion is called the test statistic, the p-value is calculated from a reference distribution for the test statistic, and the standard for small p-values is called the level of the test.In Sect. [sec:variability] we referred to likelihood ratio tests (LRTs) for which the test statistic is the difference in the deviance. That is, the LRT statistic is d_0-d_a where d_a is the deviance in the more general (H_a) model fit and d_0 is the deviance in the constrained (H_0) model. An approximate reference distribution for an LRT statistic is the chi^2_nu distribution where nu, the degrees of freedom, is determined by the number of constraints imposed on the parameters of H_a to produce H_0.The restricted model fitjulia> pstsm1 = fit!(LinearMixedModel(@formula(Y ~ 1 + (1|G)), dat[:Pastes]))\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + (1 | G)\n   logLik   -2 logLik     AIC        BIC    \n -124.20085  248.40170  254.40170  260.68473\n\nVariance components:\n              Column    Variance   Std.Dev. \n G        (Intercept)  9.63282202 3.1036788\n Residual              0.67800001 0.8234076\n Number of obs: 60; levels of grouping factors: 30\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   60.0533  0.576536 104.162  <1e-99\n\nis compared to model pstsm withjulia> MixedModels.lrt(pstsm1, pstsm)\n2×4 DataFrames.DataFrame\n│ Row │ Df    │ Deviance │ Chisq    │ pval     │\n│     │ Int64 │ Float64  │ Float64  │ Float64  │\n├─────┼───────┼──────────┼──────────┼──────────┤\n│ 1   │ 3     │ 248.402  │ NaN      │ NaN      │\n│ 2   │ 4     │ 247.994  │ 0.407234 │ 0.523377 │\nwhich provides a p-value of 05234. Because typical standards for “small” p-values are 5% or 1%, a p-value over 50% would not be considered significant at any reasonable level.We do need to be cautious in quoting this p-value, however, because the parameter value being tested, sigma_2=0, is on the boundary of set of possible values, sigma_2ge 0, for this parameter. The argument for using a chi^2_1 distribution to calculate a p-value for the change in the deviance does not apply when the parameter value being tested is on the boundary. As shown in Pinheiro and Bates (2000), the p-value from the chi^2_1 distribution will be “conservative” in the sense that it is larger than a simulation-based p-value would be. In the worst-case scenario the chi^2-based p-value will be twice as large as it should be but, even if that were true, an effective p-value of 26% would not cause us to reject H_0 in favor of H_a."
},

{
    "location": "MultipleTerms/#Assessing-the-Reduced-Model,-pstsm1-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Assessing the Reduced Model, pstsm1",
    "category": "section",
    "text": "A bootstrap samplejulia> @time psts1bstp = bootstrap(10000, pstsm1);\n  9.347764 seconds (24.71 M allocations: 704.028 MiB, 3.37% gc time)\nprovides empirical density plots(Image: )and(Image: )The profile zeta plots for the remaining parameters in model (Fig. [fig:fm04aprplot])(Image: Profile zeta plots for the parameters in model .)[fig:fm04aprplot]are similar to the corresponding panels in Fig. [fig:fm04prplot], as confirmed by the numerical values of the confidence intervals.                     2.5 %    97.5 %\n    .sig01       2.1579337  4.053589\n    .sig02       0.0000000  2.946591\n    .sigma       0.6520234  1.085448\n    (Intercept) 58.6636504 61.443016                     2.5 %    97.5 %\n    .sig01       2.4306377  4.122011\n    .sigma       0.6520207  1.085448\n    (Intercept) 58.8861831 61.220484The confidence intervals on log(sigma) and beta_0 are similar for the two models. The confidence interval on sigma_1 is slightly wider in model pstsm1 than in pstsm, because the variability that is attributed to sigma_2 in pstsm is incorporated into the variability due to sigma_1 in pstsm1.(Image: Profile pairs plot for the parameters in model fit to the data.)[fig:fm04aprpairs]The patterns in the profile pairs plot (Fig. [fig:fm04aprpairs]) for the reduced model are similar to those in Fig. [fig:fm1profpair], the profile pairs plot for model ."
},

{
    "location": "MultipleTerms/#A-Model-With-Partially-Crossed-Random-Effects-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "A Model With Partially Crossed Random Effects",
    "category": "section",
    "text": "Especially in observational studies with multiple grouping factors, the configuration of the factors frequently ends up neither nested nor completely crossed. We describe such situations as having partially crossed grouping factors for the random effects.Studies in education, in which test scores for students over time are also associated with teachers and schools, usually result in partially crossed grouping factors. If students with scores in multiple years have different teachers for the different years, the student factor cannot be nested within the teacher factor. Conversely, student and teacher factors are not expected to be completely crossed. To have complete crossing of the student and teacher factors it would be necessary for each student to be observed with each teacher, which would be unusual. A longitudinal study of thousands of students with hundreds of different teachers inevitably ends up partially crossed.In this section we consider an example with thousands of students and instructors where the response is the student’s evaluation of the instructor’s effectiveness. These data, like those from most large observational studies, are quite unbalanced."
},

{
    "location": "MultipleTerms/#The-InstEval-Data-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "The InstEval Data",
    "category": "section",
    "text": "The data are from a special evaluation of lecturers by students at the Swiss Federal Institute for Technology–Zürich (ETH–Zürich), to determine who should receive the “best-liked professor” award. These data have been slightly simplified and identifying labels have been removed, so as to preserve anonymity.The variablesjulia> names(dat[:InstEval])\n7-element Array{Symbol,1}:\n :G      \n :H      \n :studage\n :lectage\n :A      \n :I      \n :Y      \nhave somewhat cryptic names. Factor s designates the student and d the instructor. The factor dept is the department for the course and service indicates whether the course was a service course taught to students from other departments.Although the response, Y, is on a scale of 1 to 5,julia> freqtable(dat[:InstEval][:Y])\'\n1×5 Named LinearAlgebra.Adjoint{Int64,Array{Int64,1}}\n\' ╲ Dim1 │     1      2      3      4      5\n─────────┼──────────────────────────────────\n1        │ 10186  12951  17609  16921  15754\nit is sufficiently diffuse to warrant treating it as if it were a continuous response.At this point we will fit models that have random effects for student, instructor, and department (or the combination) to these data. In the next chapter we will fit models incorporating fixed-effects for instructor and department to these data.julia> @time instm = fit(LinearMixedModel, @formula(Y ~ 1 + A + (1|G) + (1|H) + (1|I)), dat[:InstEval])\n  6.545960 seconds (680.61 k allocations: 218.683 MiB, 1.01% gc time)\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + A + (1 | G) + (1 | H) + (1 | I)\n     logLik        -2 logLik          AIC             BIC       \n -1.18860884×10⁵  2.37721769×10⁵  2.37733769×10⁵  2.37788993×10⁵\n\nVariance components:\n              Column     Variance    Std.Dev. \n G        (Intercept)  0.1059727787 0.3255346\n H        (Intercept)  0.2652041783 0.5149798\n I        (Intercept)  0.0061673544 0.0785325\n Residual              1.3864885827 1.1774925\n Number of obs: 73421; levels of grouping factors: 2972, 1128, 14\n\n  Fixed-effects parameters:\n               Estimate Std.Error  z value P(>|z|)\n(Intercept)     3.28258  0.028411  115.539  <1e-99\nA: 1         -0.0925886 0.0133832 -6.91828  <1e-11\n\n(Fitting this complex model to a moderately large data set takes a few seconds on a modest desktop computer. Although this is more time than required for earlier model fits, it is a remarkably short time for fitting a model of this size and complexity. In some ways it is remarkable that such a model can be fit at all on such a computer.)All three estimated standard deviations of the random effects are less than widehatsigma, with widehatsigma_3, the estimated standard deviation of the random effects for dept, less than one-tenth of widehatsigma.It is not surprising that zero is within all of the prediction intervals on the random effects for this factor (Fig. [fig:fm05ranef]). In fact, zero is close to the middle of all these prediction intervals. However, the p-value for the LRT of H_0sigma_3=0 versus H_asigma_30Data: InstEval\nModels:\nfm05a: y ~ 1 + (1 | s) + (1 | d)\nfm05: y ~ 1 + (1 | s) + (1 | d) + (1 | dept:service)\n      Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)\nfm05a  4 237786 237823 -118889   237778                         \nfm05   5 237663 237709 -118827   237653 124.43      1  < 2.2e-16is highly significant. That is, we have very strong evidence that we should reject H_0 in favor of H_a.The seeming inconsistency of these conclusions is due to the large sample size (n=73421). When a model is fit to a very large sample even the most subtle of differences can be highly “statistically significant”. The researcher or data analyst must then decide if these terms have practical significance, beyond the apparent statistical significance.The large sample size also helps to assure that the parameters have good normal approximations. We could profile this model fit but doing so would take a very long time and, in this particular case, the analysts are more interested in a model that uses fixed-effects parameters for the instructors, which we will describe in the next chapter.We could pursue other mixed-effects models here, such as using the factor and not the interaction to define random effects, but we will revisit these data in the next chapter and follow up on some of these variations there.(Image: Image of the sparse Cholesky factor, $\\mathbf L$, from model )[fig:fm05Limage]"
},

{
    "location": "MultipleTerms/#Chapter-Summary-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Chapter Summary",
    "category": "section",
    "text": "A simple, scalar random effects term in an model formula is of the form , where is an expression whose value is the grouping factor of the set of random effects generated by this term. Typically, is simply the name of a factor, such as in the terms or in the examples in this chapter. However, the grouping factor can be the value of an expression, such as in the last example.Because simple, scalar random-effects terms can differ only in the description of the grouping factor we refer to configurations such as crossed or nested as applying to the terms or to the random effects, although it is more accurate to refer to the configuration as applying to the grouping factors.A model formula can include several such random effects terms. Because configurations such as nested or crossed or partially crossed grouping factors are a property of the data, the specification in the model formula does not depend on the configuration. We simply include multiple random effects terms in the formula specifying the model.One apparent exception to this rule occurs with implicitly nested factors, in which the levels of one factor are only meaningful within a particular level of the other factor. In the data, levels of the factor are only meaningful within a particular level of the factor. A model formula ofstrength ~ 1 + (1 | cask) + (1 | batch)would result in a fitted model that did not appropriately reflect the sources of variability in the data. Following the simple rule that the factor should be defined so that distinct experimental or observational units correspond to distinct levels of the factor will avoid such ambiguity.For convenience, a model with multiple, nested random-effects terms can be specified asstrength ~ 1 + (1 | batch/cask)which internally is re-expressed asstrength ~ 1 + (1 | batch) + (1 | batch:cask)We will avoid terms of the form , preferring instead an explicit specification with simple, scalar terms based on unambiguous grouping factors.The data, described in Sec. [sec:InstEval], illustrate some of the characteristics of the real data to which mixed-effects models are now fit. There is a large number of observations associated with several grouping factors; two of which, student and instructor, have a large number of levels and are partially crossed. Such data are common in sociological and educational studies but until now it has been very difficult to fit models that appropriately reflect such a structure. Much of the literature on mixed-effects models leaves the impression that multiple random effects terms can only be associated with nested grouping factors. The resulting emphasis on hierarchical or multilevel configurations is an artifact of the computational methods used to fit the models, not the models themselves.The parameters of the models fit to small data sets have properties similar to those for the models in the previous chapter. That is, profile-based confidence intervals on the fixed-effects parameter, beta_0, are symmetric about the estimate but overdispersed relative to those that would be calculated from a normal distribution and the logarithm of the residual standard deviation, log(sigma), has a good normal approximation. Profile-based confidence intervals for the standard deviations of random effects (sigma_1, sigma_2, etc.) are symmetric on a logarithmic scale except for those that could be zero.Another observation from the last example is that, for data sets with a very large numbers of observations, a term in a model may be “statistically significant” even when its practical significance is questionable."
},

{
    "location": "MultipleTerms/#Exercises-1",
    "page": "Models With Multiple Random-effects Terms",
    "title": "Exercises",
    "category": "section",
    "text": "These exercises use data sets from the package for . Recall that to access a particular data set, you must either attach the packageor load just the one data setWe begin with exercises using the ergostool data from the nlme package. The analysis and graphics in these exercises is performed in Chap. [chap:Covariates]. The purpose of these exercises is to see if you can use the material from this chapter to anticipate the results quoted in the next chapter.Check the documentation, the structure () and a summary of the data from the package. (If you are familiar with the Star Trek television series and movies, you may want to speculate about what, exactly, the “Borg scale” is.) Are these factors are nested, partially crossed or completely crossed. Is this a replicated or an unreplicated design?\nCreate a plot, similar to Fig. [fig:Penicillindot], showing the effort by subject with lines connecting points corresponding to the same stool types. Order the levels of the factor by increasing average .\nThe experimenters are interested in comparing these specific stool types. In the next chapter we will fit a model with fixed-effects for the factor and random effects for , allowing us to perform comparisons of these specific types. At this point fit a model with random effects for both and . What are the relative sizes of the estimates of the standard deviations, widehatsigma_1 (for ), widehatsigma_2 (for ) and widehatsigma (for the residual variability)?\nRefit the model using maximum likelihood. Check the parameter estimates and, in the case of the fixed-effects parameter, beta_0, its standard error. In what ways have the parameter estimates changed? Which parameter estimates have not changed?\nProfile the fitted model and construct 95% profile-based confidence intervals on the parameters. (Note that you will get the same profile object whether you start with the REML fit or the ML fit. There is a slight advantage in starting with the ML fit.) Is the confidence interval on sigma_1 close to being symmetric about its estimate? Is the confidence interval on sigma_2 close to being symmetric about its estimate? Is the corresponding interval on log(sigma_1) close to being symmetric about its estimate?\nCreate the profile zeta plot for this model. For which parameters are there good normal approximations?\nCreate a profile pairs plot for this model. Comment on the shapes of the profile traces in the transformed (zeta) scale and the shapes of the contours in the original scales of the parameters.\nCreate a plot of the 95% prediction intervals on the random effects for using (Substitute the name of your fitted model for in the call to .) Is there a clear winner among the stool types? (Assume that lower numbers on the Borg scale correspond to less effort).\nCreate a plot of the 95% prediction intervals on the random effects for .\nCheck the documentation, the structure () and a summary of the data from the package. Use a cross-tabulation to discover whether and are nested, partially crossed or completely crossed.\nFit a model of the in the data with random effects for , and .\nPlot the prediction intervals for each of the three sets of random effects.\nProfile the parameters in this model. Create a profile zeta plot. Does including the random effect for appear to be warranted. Does your conclusion from the profile zeta plot agree with your conclusion from examining the prediction intervals for the random effects for ?\nRefit the model without random effects for . Perform a likelihood ratio test of H_0sigma_3=0 versus H_asigma_30. Would you reject H_0 in favor of H_a or fail to reject H_0? Would you reach the same conclusion if you adjusted the p-value for the test by halving it, to take into account the fact that 0 is on the boundary of the parameter region?\nProfile the reduced model (i.e. the one without random effects for ) and create profile zeta and profile pairs plots. Can you explain the apparent interaction between log(sigma) and sigma_1? (This is a difficult question.)"
},

{
    "location": "SingularCovariance/#",
    "page": "Singular covariance estimates in random regression models",
    "title": "Singular covariance estimates in random regression models",
    "category": "page",
    "text": ""
},

{
    "location": "SingularCovariance/#Singular-covariance-estimates-in-random-regression-models-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "Singular covariance estimates in random regression models",
    "category": "section",
    "text": "This notebook explores the occurrence of singularity in the estimated covariance matrix of random regression models. These are mixed-effects models with vector-valued random effects.First, fit a model to the sleepstudy data from lme4."
},

{
    "location": "SingularCovariance/#Fitting-a-linear-mixed-model-to-the-sleepstudy-data-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "Fitting a linear mixed-model to the sleepstudy data",
    "category": "section",
    "text": "Load the required packagesjulia> using DataFrames, FreqTables, LinearAlgebra, MixedModels, Random, RData\n\njulia> using Gadfly\n\njulia> using Gadfly.Geom: density, histogram, point\n\njulia> using Gadfly.Guide: xlabel, ylabel\n\njulia> const dat = Dict(Symbol(k)=>v for (k,v) in \n    load(joinpath(dirname(pathof(MixedModels)), \"..\", \"test\", \"dat.rda\")));\nHowever, the LinearMixedModel constructor only creates a model structure but does not fit it. An explicit call to fit! is required to fit the model. As is customary (though not required) in Julia, a function whose name ends in ! is a mutating function that modifies one or more of its arguments.An optional second argument of true in the call to fit! produces verbose output from the optimization.julia> sleepm = fit!(LinearMixedModel(@formula(Y ~ 1 + U + (1+U|G)), dat[:sleepstudy]), verbose=true)\nf_1: 1784.6423 [1.0, 0.0, 1.0]\nf_2: 1790.12564 [1.75, 0.0, 1.0]\nf_3: 1798.99962 [1.0, 1.0, 1.0]\nf_4: 1803.8532 [1.0, 0.0, 1.75]\nf_5: 1800.61398 [0.25, 0.0, 1.0]\nf_6: 1798.60463 [1.0, -1.0, 1.0]\nf_7: 1752.26074 [1.0, 0.0, 0.25]\nf_8: 1797.58769 [1.18326, -0.00866189, 0.0]\nf_9: 1754.95411 [1.075, 0.0, 0.325]\nf_10: 1753.69568 [0.816632, 0.0111673, 0.288238]\nf_11: 1754.817 [1.0, -0.0707107, 0.196967]\nf_12: 1753.10673 [0.943683, 0.0638354, 0.262696]\nf_13: 1752.93938 [0.980142, -0.0266568, 0.274743]\nf_14: 1752.25688 [0.984343, -0.0132347, 0.247191]\nf_15: 1752.05745 [0.97314, 0.00253785, 0.23791]\nf_16: 1752.02239 [0.954526, 0.00386421, 0.235892]\nf_17: 1752.02273 [0.935929, 0.0013318, 0.234445]\nf_18: 1751.97169 [0.954965, 0.00790664, 0.229046]\nf_19: 1751.9526 [0.953313, 0.0166274, 0.225768]\nf_20: 1751.94852 [0.946929, 0.0130761, 0.222871]\nf_21: 1751.98718 [0.933418, 0.00613767, 0.218951]\nf_22: 1751.98321 [0.951544, 0.005789, 0.220618]\nf_23: 1751.95197 [0.952809, 0.0190332, 0.224178]\nf_24: 1751.94628 [0.946322, 0.0153739, 0.225088]\nf_25: 1751.9467 [0.947124, 0.0148894, 0.224892]\nf_26: 1751.94757 [0.946497, 0.0154643, 0.225814]\nf_27: 1751.94531 [0.946086, 0.0157934, 0.224449]\nf_28: 1751.94418 [0.945304, 0.0166902, 0.223361]\nf_29: 1751.94353 [0.944072, 0.0172106, 0.222716]\nf_30: 1751.94244 [0.941271, 0.0163099, 0.222523]\nf_31: 1751.94217 [0.939, 0.015899, 0.222132]\nf_32: 1751.94237 [0.938979, 0.016548, 0.221562]\nf_33: 1751.94228 [0.938863, 0.0152466, 0.222683]\nf_34: 1751.9422 [0.938269, 0.015733, 0.222024]\nf_35: 1751.94131 [0.938839, 0.0166373, 0.222611]\nf_36: 1751.94093 [0.938397, 0.0173965, 0.222817]\nf_37: 1751.94057 [0.937006, 0.0180445, 0.222534]\nf_38: 1751.94018 [0.934109, 0.0187354, 0.22195]\nf_39: 1751.94008 [0.932642, 0.0189242, 0.221726]\nf_40: 1751.94027 [0.931357, 0.0190082, 0.221309]\nf_41: 1751.9415 [0.932821, 0.0206454, 0.221367]\nf_42: 1751.93949 [0.931867, 0.0179574, 0.222564]\nf_43: 1751.93939 [0.929167, 0.0177824, 0.222534]\nf_44: 1751.9394 [0.929659, 0.0177721, 0.222508]\nf_45: 1751.93943 [0.929193, 0.0187806, 0.22257]\nf_46: 1751.93935 [0.928986, 0.0182366, 0.222484]\nf_47: 1751.93949 [0.928697, 0.0182937, 0.223175]\nf_48: 1751.93936 [0.928243, 0.0182695, 0.222584]\nf_49: 1751.93934 [0.929113, 0.0181791, 0.222624]\nf_50: 1751.93934 [0.929191, 0.0181658, 0.222643]\nf_51: 1751.93935 [0.929254, 0.0182093, 0.222621]\nf_52: 1751.93935 [0.929189, 0.0181298, 0.222573]\nf_53: 1751.93934 [0.929254, 0.0181676, 0.22265]\nf_54: 1751.93934 [0.929215, 0.0181717, 0.222647]\nf_55: 1751.93934 [0.929208, 0.0181715, 0.222646]\nf_56: 1751.93934 [0.929209, 0.018173, 0.222652]\nf_57: 1751.93934 [0.929221, 0.0181684, 0.222645]\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + U + ((1 + U) | G)\n   logLik   -2 logLik     AIC        BIC    \n -875.96967 1751.93934 1763.93934 1783.09709\n\nVariance components:\n              Column    Variance  Std.Dev.   Corr.\n G        (Intercept)  565.51067 23.780468\n          U             32.68212  5.716828  0.08\n Residual              654.94145 25.591824\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   251.405   6.63226 37.9064  <1e-99\nU             10.4673   1.50224 6.96781  <1e-11\n\nThe variables in the optimization are the elements of a lower triangular matrix, Lambda, which is the relative covariance factor of the random effects. The corresponding parameter vector is called theta.julia> Λ = sleepm.λ[1]\n2×2 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:\n 0.929221    ⋅      \n 0.0181684  0.222645\nThe matrix Lambda is the left (or lower) Cholesky factor of the covariance matrix of the unconditional distribution of the vector-valued random effects, relative to the variance, sigma^2, of the per-observation noise. That is \\begin{equation}     \\Sigma = \\sigma^2\\Lambda\\Lambda\' \\end{equation}In terms of the estimates,julia> s² = varest(sleepm)    # estimate of the residual variance\n654.941450830681\njulia> s² * Λ * Λ\'   # unconditional covariance matrix of the random effects\n2×2 Array{Float64,2}:\n 565.511  11.057 \n  11.057  32.6821\nThe estimated correlation of the random effects can, of course, be evaluated from the covariance matrix. Writing out the expressions for the elements of the covariance matrix in terms of the elements of Λ shows that many terms cancel in the evaluation of the correlation, resulting in the simpler formula.julia> Λ[2, 1] / sqrt(Λ[2, 1]^2 + Λ[2, 2]^2)\n0.08133214602351191\nFor a 2times 2 covariance matrix it is not terribly important to perform this calculation in an efficient and numerically stable way. However, it is a good idea to pay attention to stability and efficiency in a calculation that can be repeated tens of thousands of times in a simulation or a parametric bootstrap. The norm function evaluates the (geometric) length of a vector in a way that controls round-off better than the naive calculation. The view function provides access to a subarray, such as the second row of Lambda, without generating a copy. Thus the estimated correlation can be writtenjulia> Λ[2, 1] / norm(view(Λ, 2, :))\n0.08133214602351191\n"
},

{
    "location": "SingularCovariance/#Optimization-with-respect-to-θ-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "Optimization with respect to θ",
    "category": "section",
    "text": "As described in section 3 of the 2015 Journal of Statistical Software paper by Bates, Maechler, Bolker and Walker, using the relative covariance factor, Lambda, in the formulation of mixed-effects models in the lme4 and MixedModels packages and using the vector theta as the optimization variable was a conscious choice. Indeed, a great deal of effort went into creating this form so that the profiled log-likelihood can be easily evaluated and so that the constraints on the parameters, theta, are simple \"box\" constraints. In fact, the constraints are simple lower bounds.julia> show(sleepm.lowerbd)\n[0.0, -Inf, 0.0]In contrast, trying to optimize the log-likelihood with respect to standard deviations and correlations of the random effects would be quite difficult because the constraints on the correlations when the covariance matrix is larger than 2times 2 are quite complicated. Also, the correlation itself can be unstable. Consider what happens to the expression for the correlation if both Lambda_21 and Lambda_22 are small in magnitude. Small perturbations in Lambda_21 that result in sign changes can move the correlation from near -1 to near +1 or vice-versa.Some details on the optimization process are available in an OptSummary object stored as the optsum field of the model.julia> sleepm.optsum\nInitial parameter vector: [1.0, 0.0, 1.0]\nInitial objective value:  1784.6422961924507\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [0.0, -Inf, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 1.0, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     57\nFinal parameter vector:   [0.929221, 0.0181684, 0.222645]\nFinal objective value:    1751.9393444646757\nReturn code:              FTOL_REACHED\n\n"
},

{
    "location": "SingularCovariance/#Convergence-on-the-boundary-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "Convergence on the boundary",
    "category": "section",
    "text": "Determining if an estimated covariance matrix is singular is easy when using the theta  parameters because singularity corresponds to points on the boundary of the allowable parameter space. In other words, if the optimization converges to a vector in which either or both of theta_1 or theta_3 are zero, the covariance matrix is singular. Otherwise it is non-singular.The theta_1 parameter is the estimated relative standard deviation of the random intercepts. If this is zero then the correlation is undefined and reported as NaN. If theta_3 is zero and theta_2 is non-zero then the estimated correlation is pm 1 with the sign determined by the sign of theta_2. If both theta_2 and theta_3 are zero the correlation is NaN because the standard deviation of the random slopes will be zero.Singular covariance matrices larger than 2times 2 do not necessarily result in particular values, like ±1, for the correlations.Users of lmer or lmm are sometimes taken aback by convergence on the boundary if this produces correlations of NaN or pm 1. Some feel that this is a sign of model failure. Others consider such estimates as a sign that Bayesian methods with priors that pull singular covariance matrices away from the boundary should be used.This type of value judgement seems peculiar. An important property of maximum likelihood estimates is that these estimates are well-defined once the probability model for the data has been specified. It may be difficult to determine numerical values of the estimates but the definition itself is straightforward. If there is a direct method of evaluating the log-likelihood at a particular value of the parameters, then, by definition, the mle\'s are the parameter values that maximize this log-likelihood. Bates et al. (2015) provide such a method of evaluating the log-likelihood for a linear mixed-effects model. Indeed they go further and describe how the fixed-effects parameters and one of the variance components can be profiled out of the log-likelihood evaluation, thereby reducing the dimension of the nonlinear, constrained optimization problem to be solved.If the mle\'s correspond to a singular covariance matrix, this is a property of the model and the data. It is not a mistake in some way. It is just the way things are. It reflects the fact that often the distribution of the estimator of a covariance matrix is diffuse. It is difficult to estimate variances and covariances precisely. A search for papers or books on \"covariance estimation\" will produce many results, often describing ways of regularizing the estimating process because the data themselves do not provide precise estimates.For the example at hand a parametric bootstrap is one way of evaluating the precision of the estimates."
},

{
    "location": "SingularCovariance/#The-bootstrap-function-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "The bootstrap function",
    "category": "section",
    "text": "The MixedModels package provides a bootstrap method to create a parametric bootstrap sample from a fitted model.For reproducibility, set the random number seed to some arbitrary value.julia> Random.seed!(1234321);\nArguments to the bootstrap function are the number of samples to generate and the model from which to generate them. By default the converged parameter estimates are those used to generate the samples. Addition, named arguments can be used to override these parameter values, allowing bootstrap to be used for simulation.bootstrap returns a DataFrame with columns:obj: the objective (-2 loglikelihood)\nσ: the standard deviation of the per-observation noise\nβ₁ to βₚ: the fixed-effects coefficients\nθ₁ to θₖ: the covariance parameter elements\nσ₁ to σₛ: the estimates standard deviations of random effects.\nρ₁ to ρₜ: the estimated correlations of random effectsThe ρᵢ and σᵢ values are derived from the θᵢ and σ values.julia> sleepmbstrp = bootstrap(10000, sleepm);\n\njulia> show(names(sleepmbstrp))\nSymbol[:obj, :σ, :β₁, :β₂, :θ₁, :θ₂, :θ₃, :σ₁, :σ₂, :ρ₁]Recall that the constrained parameters are theta_1 and theta_3 which both must be non-negative. If either or both of these are zero (in practice the property to check is if they are \"very small\", which here is arbitrarily defined as less than 0.00001) then the covariance matrix is singular.julia> issmall(x) = x < 0.00001   # defines a one-liner function in Julia\nissmall (generic function with 1 method)\njulia> freqtable(issmall.(sleepmbstrp[:θ₁]), issmall.(sleepmbstrp[:θ₃]))\n2×2 Named Array{Int64,2}\nDim1 ╲ Dim2 │ false   true\n────────────┼─────────────\nfalse       │  9684    309\ntrue        │     7      0\nHere the covariance matrix estimate is non-singular in 9,686 of the 10,000 samples, has an zero estimated intercept variance in 6 samples and is otherwise singular (i.e. correlation estimate of pm 1) in 308 samples.Empirical densities of the θ components are:(Image: )(Image: )A density plot is typically a good way to visualize such a large sample. However, when there is a spike such as the spike at zero here, a histogram provides a more informative plot.(Image: )(Image: )"
},

{
    "location": "SingularCovariance/#Reciprocal-condition-number-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "Reciprocal condition number",
    "category": "section",
    "text": "The definitve way to assess singularity of the estimated covariance matrix is by its condition number or, alternatively, its reciprocal condition number. In general the condition number, kappa, of a matrix is the ratio of the largest singular value to the smallest. For singular matrices it is infty, which is why it is often more convenient to evaluate and plot kappa^-1. Because kappa is a ratio of singular values it is unaffected by nonzero scale factors. Thus \\begin{equation} \\kappa^{-1}(s^2\\Lambda\\Lambda\') = \\kappa^{-1}(\\Lambda\\Lambda\') = [\\kappa^{-1}(\\Lambda)]^2 \\end{equation}function recipcond(bstrp::DataFrame)\n    T = eltype(bstrp[:θ₁])\n    val = sizehint!(T[], size(bstrp, 1))\n    d = Matrix{T}(undef, 2, 2)\n    for (t1, t2, t3) in zip(bstrp[:θ₁], bstrp[:θ₂], bstrp[:θ₃])\n        d[1, 1] = t1\n        d[1, 2] = t2\n        d[2, 2] = t3\n        v = svdvals!(d)\n        push!(val, v[2] / v[1])\n    end\n    val\nend\nrc = recipcond(sleepmbstrp)(Image: )kappa^-1is small if either or both of theta_1 or theta_3 is small.julia> sum(issmall, rc)\n316\nThe density of the estimated correlation(Image: )julia> sum(isfinite, sleepmbstrp[:ρ₁])  # recall that ρ = NaN in 7 cases\n9993\njulia> sum(x -> x == -1, sleepmbstrp[:ρ₁])  # number of cases of rho == -1\n1\njulia> sum(x -> x == +1, sleepmbstrp[:ρ₁])  # number of cases of rho == +1\n308\nIn this case the bootstrap simulations that resulted in rho = -1 were not close to being indeterminant with respect to sign. That is, the values of theta_2 were definitely negative.julia> sleepmbstrp[:θ₂][findall(x -> x == -1, sleepmbstrp[:ρ₁])]\n1-element Array{Float64,1}:\n -0.2544952267491184\n"
},

{
    "location": "SingularCovariance/#The-Oxboys-data-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "The Oxboys data",
    "category": "section",
    "text": "In the nlme package for R there are several data sets to which random regression models are fit. The RCall package for Julia provides the ability to run an embedded R process and communicate with it. The simplest form of writing R code within Julia is to use character strings prepended with R. In Julia strings are delimited by \" or by \"\"\". With \"\"\" multi-line strings are allowed.julia> using RCall\n\njulia> R\"\"\"\nlibrary(nlme)\nplot(Oxboys)\n\"\"\"\nRCall.RObject{RCall.NilSxp}\nNULL\n\njulia> oxboys = rcopy(R\"Oxboys\");\n\njulia> show(names(oxboys))\nSymbol[:Subject, :age, :height, :Occasion]julia> oxboysm = fit(LinearMixedModel, @formula(height ~ 1 + age + (1+age | Subject)), oxboys)\nLinear mixed model fit by maximum likelihood\n Formula: height ~ 1 + age + ((1 + age) | Subject)\n   logLik   -2 logLik     AIC        BIC    \n -362.98384  725.96769  737.96769  758.69962\n\nVariance components:\n              Column    Variance   Std.Dev.    Corr.\n Subject  (Intercept)  62.7888388 7.92394087\n          age           2.7115491 1.64667821  0.64\n Residual               0.4354569 0.65989157\n Number of obs: 234; levels of grouping factors: 26\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   149.372   1.55461  96.083  <1e-99\nage           6.52547   0.32976 19.7885  <1e-86\n\njulia> show(getθ(oxboysm))\n[12.0079, 1.60155, 1.91362]As seen in the plot and by the estimate widehattheta_1 = 120, the estimated standard deviation of the random intercepts is much greater than the residual standard deviation. It is unlikely that bootstrap samples will include singular covariance estimates.julia> Random.seed!(4321234);\n\njulia> oxboysmbtstrp = bootstrap(10000, oxboysm);\nIn this bootstrap sample, there are no singular estimated covariance matrices for the random effects.julia> freqtable(issmall.(oxboysmbtstrp[:θ₁]), issmall.(oxboysmbtstrp[:θ₃]))\n1×1 Named Array{Int64,2}\nDim1 ╲ Dim2 │ false\n────────────┼──────\nfalse       │ 10000\nThe empirical density of the correlation estimates shows that even in this case the correlation is not precisely estimated.(Image: )julia> extrema(oxboysmbtstrp[:ρ₁])\n(0.033603332880225045, 0.9352626530655941)\nThe reciprocal condition numberjulia> rc = recipcond(oxboysmbtstrp);\n\njulia> extrema(rc)\n(0.06198656881812736, 0.36249490895418984)\ndoes not get very close to zero.(Image: )"
},

{
    "location": "SingularCovariance/#The-Orthodont-data-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "The Orthodont data",
    "category": "section",
    "text": "julia> R\"plot(Orthodont)\"\nRCall.RObject{RCall.VecSxp}\n\nThe subject labels distinguish between the male and the female subjects.  Consider first the female subjects only.julia> orthfemale = rcopy(R\"subset(Orthodont, Sex == \'Female\', -Sex)\");\n\njulia> orthfm = fit!(LinearMixedModel(@formula(distance ~ 1 + age + (1 + age | Subject)), orthfemale))\nLinear mixed model fit by maximum likelihood\n Formula: distance ~ 1 + age + ((1 + age) | Subject)\n   logLik   -2 logLik     AIC        BIC    \n  -67.25463  134.50927  146.50927  157.21441\n\nVariance components:\n              Column    Variance   Std.Dev.    Corr.\n Subject  (Intercept)  2.97138401 1.72377029\n          age          0.02151328 0.14667406 -0.30\n Residual              0.44659786 0.66827978\n Number of obs: 44; levels of grouping factors: 27\n\n  Fixed-effects parameters:\n             Estimate Std.Error z value P(>|z|)\n(Intercept)   17.3727  0.725193  23.956  <1e-99\nage          0.479545 0.0631327 7.59583  <1e-13\n\njulia> Random.seed!(1234123)\nMersenneTwister(UInt32[0x0012d4cb], Random.DSFMT.DSFMT_state(Int32[1849428804, 1072710534, 1722234079, 1073299110, 2058053067, 1072801015, 18044541, 1072957251, 668716466, 1073001711  …  -1153221639, 1073553062, 1653158638, 1073411494, 780501209, -2117144994, -394908522, -1446490633, 382, 0]), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], UInt128[0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000  …  0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000, 0x00000000000000000000000000000000], 1002, 0)\n\njulia> orthfmbtstrp = bootstrap(10000, orthfm);\njulia> freqtable(issmall.(orthfmbtstrp[:θ₁]), issmall.(orthfmbtstrp[:θ₃]))\n2×2 Named Array{Int64,2}\nDim1 ╲ Dim2 │ false   true\n────────────┼─────────────\nfalse       │  6784   3184\ntrue        │    32      0\nFor this model almost 1/3 of the bootstrap samples converge to singular covariance estimates for the vector-valued random effects. A histogram of the estimated correlations of the random effects is dominated by the boundary values.(Image: )Even though the estimated correlation in the model is -0.30, more of the boundary values are at +1 than at -1. This may be an artifact of the optimization routine used. In some cases there may be multiple optima on the boundary. It is difficult to determine the global optimum in these cases.A histogram of the reciprocal condition number is also dominated by the boundary values."
},

{
    "location": "SingularCovariance/#Early-childhood-cognitive-study-1",
    "page": "Singular covariance estimates in random regression models",
    "title": "Early childhood cognitive study",
    "category": "section",
    "text": "This example from Singer and Willett (2003), Applied Longitudinal Data Analysis was the motivation for reformulating the estimation methods to allow for singular covariance matrices. Cognitive scores (cog) were recorded at age 1, 1.5 and 2 years on 103 infants, of whom 58 were in the treatment group and 45 in the control group.  The treatment began at age 6 months (0.5 years).  The data are available as the Early data set in the mlmRev package for R.  In the model, time on study (tos) is used instead of age because the zero point on the time scale should be when the treatment begins.julia> R\"\"\"\nsuppressMessages(library(mlmRev))\nlibrary(lattice)\nEarly$tos <- Early$age - 0.5\nEarly$trttos <- Early$tos * (Early$trt == \"Y\")\nxyplot(cog ~ tos | reorder(id, cog, min), Early, \n    type = c(\"p\",\"l\",\"g\"), aspect=\"xy\")\n\"\"\"\nRCall.RObject{RCall.VecSxp}\n\nNotice that most of these plots within subjects have a negative slope and that the scores at 1 year of age (tos = 0.5) are frequently greater than would be expected on an age-adjusted scale.julia> R\"print(xtabs(cog ~ age + trt, Early) / xtabs(~ age + trt, Early))\";\n     trt\nage           N         Y\n  1   108.53333 112.93103\n  1.5  95.88889 110.29310\n  2    87.40000  97.06897\nWhen the time origin is the beginning of the treatment there is not generally a \"main effect\" for the treatment but only an interaction of trt and tos.julia> early = rcopy(R\"subset(Early, select = c(cog, tos, id, trt, trttos))\");\n\njulia> earlym = fit(LinearMixedModel, @formula(cog ~ 1 + tos + trttos + (1 + tos | id)), early)\nLinear mixed model fit by maximum likelihood\n Formula: cog ~ 1 + tos + trttos + ((1 + tos) | id)\n   logLik   -2 logLik     AIC        BIC    \n -1185.6369  2371.2738  2385.2738  2411.4072\n\nVariance components:\n              Column    Variance   Std.Dev.    Corr.\n id       (Intercept)  165.476453 12.8637651\n          tos           10.744791  3.2779247 -1.00\n Residual               74.946837  8.6571841\n Number of obs: 309; levels of grouping factors: 103\n\n  Fixed-effects parameters:\n             Estimate Std.Error  z value P(>|z|)\n(Intercept)   120.783    1.8178  66.4447  <1e-99\ntos           -22.474    1.4878 -15.1055  <1e-50\ntrttos        7.65205   1.43609  5.32841   <1e-7\n\nThe model converges to a singular covariance matrix for the random effects.julia> getθ(earlym)\n3-element Array{Float64,1}:\n  1.4859063765534406\n -0.3786363648039024\n  0.0               \nThe conditional (on the observed responses) means of the random effects fall along a line.(Image: )"
},

{
    "location": "SubjectItem/#",
    "page": "-",
    "title": "-",
    "category": "page",
    "text": "julia> using DataFrames, MixedModels, RData\n\njulia> const dat = Dict(Symbol(k)=>v for (k,v) in \n    load(joinpath(dirname(pathof(MixedModels)), \"..\", \"test\", \"dat.rda\")));\njulia> mm1 = fit(LinearMixedModel, @formula(Y ~ 1+S+T+U+V+W+X+Z+(1+S+T+U+V+W+X+Z|G)+(1+S+T+U+V+W+X+Z|H)), dat[:kb07])\nLinear mixed model fit by maximum likelihood\n Formula: Y ~ 1 + S + T + U + V + W + X + Z + ((1 + S + T + U + V + W + X + Z) | G) + ((1 + S + T + U + V + W + X + Z) | H)\n     logLik        -2 logLik          AIC             BIC       \n -1.42931596×10⁴  2.85863192×10⁴  2.87483192×10⁴  2.91930068×10⁴\n\nVariance components:\n              Column     Variance   Std.Dev.    Corr.\n G        (Intercept)   90782.9823 301.302145\n          S              5183.6887  71.997838 -0.43\n          T              5545.2694  74.466566 -0.47  0.07\n          U              7592.2351  87.133433  0.21 -0.20  0.41\n          V              8828.0524  93.957716  0.20 -0.76 -0.54 -0.20\n          W              1822.0044  42.684943  0.47 -0.53 -0.11 -0.44  0.28\n          X              7424.6857  86.166616 -0.10  0.13 -0.05 -0.86 -0.06  0.70\n          Z              3801.7944  61.658693 -0.48  0.41 -0.39 -0.09  0.18 -0.78 -0.39\n H        (Intercept)  130028.5528 360.594721\n          S              1855.5643  43.076261 -0.34\n          T             62432.1537 249.864271 -0.68 -0.45\n          U              2955.9678  54.368813  0.20 -0.02 -0.18\n          V              1040.6601  32.259264  0.57 -0.76  0.02  0.01\n          W              1616.8109  40.209587  0.28 -0.04 -0.27  0.44 -0.21\n          X              4699.1718  68.550505  0.08 -0.24  0.21 -0.13 -0.26  0.02\n          Z              4821.2935  69.435535  0.04 -0.47  0.32 -0.68  0.65 -0.69 -0.10\n Residual              399589.9278 632.131258\n Number of obs: 1790; levels of grouping factors: 56, 32\n\n  Fixed-effects parameters:\n             Estimate Std.Error  z value P(>|z|)\n(Intercept)   2180.63   76.8621  28.3706  <1e-99\nS            -66.9897   19.3344 -3.46479  0.0005\nT            -333.881   47.6791 -7.00268  <1e-11\nU             78.9869   21.2419  3.71846  0.0002\nV             22.1517    20.333  1.08945  0.2760\nW            -18.9246   17.5022 -1.08127  0.2796\nX             5.26219    22.421 0.234699  0.8144\nZ            -23.9509   21.0195 -1.13946  0.2545\n\njulia> mm1.optsum\nInitial parameter vector: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0  …  1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]\nInitial objective value:  30014.369768606295\n\nOptimizer (from NLopt):   LN_BOBYQA\nLower bounds:             [0.0, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf  …  0.0, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, 0.0, -Inf, 0.0]\nftol_rel:                 1.0e-12\nftol_abs:                 1.0e-8\nxtol_rel:                 0.0\nxtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10  …  1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10]\ninitial_step:             [0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0  …  0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75]\nmaxfeval:                 -1\n\nFunction evaluations:     2093\nFinal parameter vector:   [0.476645, -0.0494739, -0.0557462, 0.0295519, 0.0292019, 0.031941, -0.013964, -0.0464088, 0.102591, -0.0171748  …  0.0243508, -0.0518579, -0.0435887, 0.0582836, 0.00661947, -0.00588158, -0.00248396, 0.0, -1.69798e-5, 0.0]\nFinal objective value:    28586.319170623105\nReturn code:              FTOL_REACHED\n\njulia> mm1.trms[1].Λ\n8×8 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:\n  0.476645     ⋅           ⋅          …    ⋅            ⋅           ⋅ \n -0.0494739   0.102591     ⋅               ⋅            ⋅           ⋅ \n -0.0557462  -0.0171748   0.102346         ⋅            ⋅           ⋅ \n  0.0295519  -0.0165941   0.078039         ⋅            ⋅           ⋅ \n  0.0292019  -0.110815   -0.0944958        ⋅            ⋅           ⋅ \n  0.031941   -0.0243539   0.00504511  …   0.0           ⋅           ⋅ \n -0.013964    0.0125809  -0.0133861       0.000105014  0.0          ⋅ \n -0.0464088   0.021845   -0.0650391      -4.02353e-5   6.50416e-5  0.0\njulia> mm1.trms[2].Λ\n8×8 LinearAlgebra.LowerTriangular{Float64,Array{Float64,2}}:\n  0.570443      ⋅            ⋅          …    ⋅            ⋅           ⋅ \n -0.023398     0.0640016     ⋅               ⋅            ⋅           ⋅ \n -0.267225    -0.287873     0.0442821        ⋅            ⋅           ⋅ \n  0.017337     0.00426167  -0.006672         ⋅            ⋅           ⋅ \n  0.0290524   -0.0304972   -0.0146136        ⋅            ⋅           ⋅ \n  0.0178871    0.00395068  -0.0207794   …   0.00661947    ⋅           ⋅ \n  0.00853432  -0.0241396    0.0954565      -0.00588158   0.0          ⋅ \n  0.00489295  -0.0537545   -0.00619495     -0.00248396  -1.69798e-5  0.0\n"
},

{
    "location": "benchmarks/#",
    "page": "Benchmark Report for /home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl",
    "title": "Benchmark Report for /home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl",
    "category": "page",
    "text": ""
},

{
    "location": "benchmarks/#Benchmark-Report-for-*/home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl*-1",
    "page": "Benchmark Report for /home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl",
    "title": "Benchmark Report for /home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl",
    "category": "section",
    "text": ""
},

{
    "location": "benchmarks/#Job-Properties-1",
    "page": "Benchmark Report for /home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl",
    "title": "Job Properties",
    "category": "section",
    "text": "Time of benchmark: 2 Oct 2018 - 13:42\nPackage commit: non gi\nJulia commit: 5d4eac\nJulia command flags: None\nEnvironment variables: None"
},

{
    "location": "benchmarks/#Results-1",
    "page": "Benchmark Report for /home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl",
    "title": "Results",
    "category": "section",
    "text": "Below is a table of this job\'s results, obtained by running the benchmarks. The values listed in the ID column have the structure [parent_group, child_group, ..., key], and can be used to index into the BaseBenchmarks suite to retrieve the corresponding benchmarks. The percentages accompanying time and memory values in the below table are noise tolerances. The \"true\" time/memory value for a given benchmark is expected to fall within this percentage of the reported value. An empty cell means that the value was zero.ID time GC time memory allocations\n`[\"crossed\", \"Assay:1+A+B*C+(1 G)+(1 H)\"]` 2.943 ms (5%) \n`[\"crossed\", \"Demand:1+U+V+W+X+(1 G)+(1 H)\"]` 2.775 ms (5%) \n`[\"crossed\", \"InstEval:1+A*I+(1 G)+(1 H)\"]` 1.247 s (5%) 114.131 ms\n`[\"crossed\", \"InstEval:1+A+(1 G)+(1 H)+(1 I)\"]` 1.999 s (5%)\n`[\"crossed\", \"Penicillin:1+(1 G)+(1 H)\"]` 2.697 ms (5%) \n`[\"crossed\", \"ScotsSec:1+A+U+V+(1 G)+(1 H)\"]` 4.833 ms (5%) \n`[\"crossed\", \"dialectNL:1+A+T+U+V+W+X+(1 G)+(1 H)+(1 I)\"]` 416.892 ms (5%)\n`[\"crossed\", \"egsingle:1+A+U+V+(1 G)+(1 H)\"]` 31.421 ms (5%) 3.427 ms\n`[\"crossed\", \"ml1m:1+(1 G)+(1 H)\"]` 36.714 s (5%) 225.872 ms\n`[\"crossed\", \"paulsim:1+S+T+U+(1 H)+(1 G)\"]` 14.097 ms (5%) \n`[\"crossedvector\", \"bs10:1+U+V+W+((1+U+V+W) G)+((1+U+V+W) H)\"]` 165.171 ms (5%) 3.149 ms\n`[\"crossedvector\", \"d3:1+U+((1+U) G)+((1+U) H)+((1+U) I)\"]` 49.023 s (5%)\n`[\"crossedvector\", \"d3:1+U+(1 G)+(1 H)+(1 I)\"]` 299.348 ms (5%)\n`[\"crossedvector\", \"gb12:1+S+T+U+V+W+X+Z+((1+S+U+W) G)+((1+S+T+V) H)\"]` 134.101 ms (5%) \n`[\"crossedvector\", \"kb07:1+S+T+U+V+W+X+Z+((1+S+T+U+V+W+X+Z) G)+((1+S+T+U+V+W+X+Z) H)\"]` 3.488 s (5%) 16.508 ms\n`[\"crossedvector\", \"kb07:1+S+T+U+V+W+X+Z+(1 G)+((0+S) G)+((0+T) G)+((0+U) G)+((0+V)\n`[\"nested\", \"Animal:1+(1 G)+(1 H)\"]` 1.261 ms (5%) \n`[\"nested\", \"Chem97:1+(1 G)+(1 H)\"]` 58.460 ms (5%) 6.975 ms\n`[\"nested\", \"Chem97:1+U+(1 G)+(1 H)\"]` 59.353 ms (5%) 7.019 ms\n`[\"nested\", \"Genetics:1+A+(1 G)+(1 H)\"]` 2.062 ms (5%) \n`[\"nested\", \"Pastes:1+(1 G)+(1 H)\"]` 2.298 ms (5%) \n`[\"nested\", \"Semi2:1+A+(1 G)+(1 H)\"]` 2.309 ms (5%) \n`[\"simplescalar\", \"Alfalfa:1+A*B+(1 G)\"]` 1.210 ms (5%)  208.80 KiB (1%)\n`[\"simplescalar\", \"Alfalfa:1+A+B+(1 G)\"]` 1.021 ms (5%)  168.47 KiB (1%)\n`[\"simplescalar\", \"AvgDailyGain:1+A*U+(1 G)\"]` 1.287 ms (5%)  193.33 KiB (1%)\n`[\"simplescalar\", \"AvgDailyGain:1+A+U+(1 G)\"]` 1.144 ms (5%)  169.59 KiB (1%)\n`[\"simplescalar\", \"BIB:1+A*U+(1 G)\"]` 1.574 ms (5%)  222.20 KiB (1%)\n`[\"simplescalar\", \"BIB:1+A+U+(1 G)\"]` 1.171 ms (5%)  171.31 KiB (1%)\n`[\"simplescalar\", \"Bond:1+A+(1 G)\"]` 958.770 μs (5%)  141.25 KiB (1%)\n`[\"simplescalar\", \"Cultivation:1+A*B+(1 G)\"]` 1.089 ms (5%)  173.38 KiB (1%)\n`[\"simplescalar\", \"Cultivation:1+A+(1 G)\"]` 1.138 ms (5%)  162.14 KiB (1%)\n`[\"simplescalar\", \"Cultivation:1+A+B+(1 G)\"]` 1.147 ms (5%)  173.47 KiB (1%)\n`[\"simplescalar\", \"Dyestuff2:1+(1 G)\"]` 830.840 μs (5%)  105.20 KiB (1%)\n`[\"simplescalar\", \"Dyestuff:1+(1 G)\"]` 974.091 μs (5%)  120.86 KiB (1%)\n`[\"simplescalar\", \"Exam:1+A*U+B+(1 G)\"]` 2.250 ms (5%)  1.17 MiB (1%)\n`[\"simplescalar\", \"Exam:1+A+B+U+(1 G)\"]` 2.133 ms (5%)  1.03 MiB (1%)\n`[\"simplescalar\", \"Gasoline:1+U+(1 G)\"]` 1.164 ms (5%)  162.03 KiB (1%)\n`[\"simplescalar\", \"Hsb82:1+A+B+C+U+(1 G)\"]` 3.048 ms (5%)  2.12 MiB (1%)\n`[\"simplescalar\", \"IncBlk:1+A+U+V+W+Z+(1 G)\"]` 1.226 ms (5%)  208.83 KiB (1%)\n`[\"simplescalar\", \"Mississippi:1+A+(1 G)\"]` 980.968 μs (5%)  145.75 KiB (1%)\n`[\"simplescalar\", \"PBIB:1+A+(1 G)\"]` 1.509 ms (5%)  234.47 KiB (1%)\n`[\"simplescalar\", \"Rail:1+(1 G)\"]` 1.251 ms (5%)  151.34 KiB (1%)\n`[\"simplescalar\", \"Semiconductor:1+A*B+(1 G)\"]` 1.313 ms (5%)  222.95 KiB (1%)\n`[\"simplescalar\", \"TeachingII:1+A+T+U+V+W+X+Z+(1 G)\"]` 1.483 ms (5%)  284.53 KiB (1%)\n`[\"simplescalar\", \"cake:1+A*B+(1 G)\"]` 1.606 ms (5%)  412.83 KiB (1%)\n`[\"simplescalar\", \"ergoStool:1+A+(1 G)\"]` 1.057 ms (5%)  155.59 KiB (1%)\n`[\"singlevector\", \"Early:1+U+U&A+((1+U) G)\"]` 20.373 ms (5%)  3.47 MiB (1%)\n`[\"singlevector\", \"HR:1+A*U+V+((1+U) G)\"]` 5.183 ms (5%)  915.00 KiB (1%)\n`[\"singlevector\", \"Oxboys:1+U+((1+U) G)\"]` 13.207 ms (5%)  1.93 MiB (1%)\n`[\"singlevector\", \"SIMS:1+U+((1+U) G)\"]` 61.675 ms (5%)  12.86 MiB (1%)\n`[\"singlevector\", \"WWheat:1+U+((1+U) G)\"]` 7.311 ms (5%)  902.31 KiB (1%)\n`[\"singlevector\", \"Weights:1+A*U+((1+U) G)\"]` 18.303 ms (5%)  3.20 MiB (1%)\n`[\"singlevector\", \"sleepstudy:1+U+((1+U) G)\"]` 4.829 ms (5%)  797.48 KiB (1%)\n`[\"singlevector\", \"sleepstudy:1+U+(1 G)+((0+U) G)\"]` 3.219 ms (5%) "
},

{
    "location": "benchmarks/#Benchmark-Group-List-1",
    "page": "Benchmark Report for /home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl",
    "title": "Benchmark Group List",
    "category": "section",
    "text": "Here\'s a list of all the benchmark groups executed by this job:[\"crossed\"]\n[\"crossedvector\"]\n[\"nested\"]\n[\"simplescalar\"]\n[\"singlevector\"]"
},

{
    "location": "benchmarks/#Julia-versioninfo-1",
    "page": "Benchmark Report for /home/bates/.julia/packages/MixedModels/dn0WY/src/MixedModels.jl",
    "title": "Julia versioninfo",
    "category": "section",
    "text": "Julia Version 1.0.0\nCommit 5d4eaca0c9 (2018-08-08 20:58 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n      Ubuntu 18.04.1 LTS\n  uname: Linux 4.15.0-36-generic #39-Ubuntu SMP Mon Sep 24 16:19:09 UTC 2018 x86_64 x86_64\n  CPU: Intel(R) Core(TM) i5-3570 CPU @ 3.40GHz: \n              speed         user         nice          sys         idle          irq\n       #1  1690 MHz     140498 s        134 s      18382 s    1495130 s          0 s\n       #2  2513 MHz     131505 s         16 s      18277 s    1504212 s          0 s\n       #3  1900 MHz     145131 s        581 s      18892 s    1485409 s          0 s\n       #4  1682 MHz     190751 s         38 s      17941 s    1445446 s          0 s\n       \n  Memory: 15.554645538330078 GB (10502.1171875 MB free)\n  Uptime: 16578.0 sec\n  Load Avg:  1.4091796875  2.07080078125  1.63037109375\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-6.0.0 (ORCJIT, ivybridge)"
},

]}
