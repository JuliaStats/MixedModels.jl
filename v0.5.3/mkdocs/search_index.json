{
    "docs": [
        {
            "location": "/", 
            "text": "MixedModels.jl\n\n\nFitting and examining mixed-effects models\n\n\n\n\nManual Outline\n\n\n\n\nFitting linear mixed-effects models\n\n\nA simple example\n\n\nMore substantial examples\n\n\n\n\n\n\n\n\n\n\nLibrary Outline\n\n\n\n\nPublic Documentation\n\n\nContents\n\n\nIndex\n\n\nMixedModels\n\n\n\n\n\n\nInternal Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nFunctions and methods\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nBase.LinAlg.cond\n\n\nMixedModels.LaplaceDeviance\n\n\nMixedModels.LinearMixedModel\n\n\nMixedModels.ScalarReMat\n\n\nMixedModels.VarCorr\n\n\nMixedModels.VectorReMat\n\n\nMixedModels.bootstrap\n\n\nMixedModels.fixef\n\n\nMixedModels.lmm\n\n\nMixedModels.lowerbd\n\n\nMixedModels.objective\n\n\nMixedModels.pirls!\n\n\nMixedModels.pwrss\n\n\nMixedModels.ranef\n\n\nMixedModels.refit!\n\n\nMixedModels.remat\n\n\nMixedModels.sdest\n\n\nMixedModels.simulate!\n\n\nMixedModels.varest\n\n\nStatsBase.deviance\n\n\nStatsBase.df\n\n\nStatsBase.fit!\n\n\nStatsBase.model_response\n\n\nStatsBase.nobs\n\n\nStatsBase.vcov\n\n\nMixedModels.OptSummary\n\n\nMixedModels.ranef!", 
            "title": "Home"
        }, 
        {
            "location": "/#mixedmodelsjl", 
            "text": "Fitting and examining mixed-effects models", 
            "title": "MixedModels.jl"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "Fitting linear mixed-effects models  A simple example  More substantial examples", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/#library-outline", 
            "text": "Public Documentation  Contents  Index  MixedModels    Internal Documentation  Contents  Index  Types  Functions and methods", 
            "title": "Library Outline"
        }, 
        {
            "location": "/#index", 
            "text": "Base.LinAlg.cond  MixedModels.LaplaceDeviance  MixedModels.LinearMixedModel  MixedModels.ScalarReMat  MixedModels.VarCorr  MixedModels.VectorReMat  MixedModels.bootstrap  MixedModels.fixef  MixedModels.lmm  MixedModels.lowerbd  MixedModels.objective  MixedModels.pirls!  MixedModels.pwrss  MixedModels.ranef  MixedModels.refit!  MixedModels.remat  MixedModels.sdest  MixedModels.simulate!  MixedModels.varest  StatsBase.deviance  StatsBase.df  StatsBase.fit!  StatsBase.model_response  StatsBase.nobs  StatsBase.vcov  MixedModels.OptSummary  MixedModels.ranef!", 
            "title": "Index"
        }, 
        {
            "location": "/man/fitting/", 
            "text": "Fitting linear mixed-effects models\n\n\nThe \nlmm\n function is similar to the \nlmer\n function in the \nlme4\n package for \nR\n.  The first two arguments for in the \nR\n version are \nformula\n and \ndata\n.  The principle method for the \nJulia\n version takes these arguments.\n\n\n\n\nA simple example\n\n\nThe simplest example of a mixed-effects model that we use in the \nlme4 package for R\n is a model fit to the \nDyestuff\n data.\n\n\n str\n(\nDyestuff\n)\n\n\ndata.frame\n:\n   \n30\n obs. of  \n2\n variables\n:\n\n \n$\n Batch\n:\n Factor w\n/\n \n6\n levels \nA\n,\nB\n,\nC\n,\nD\n,\n..\n:\n \n1\n \n1\n \n1\n \n1\n \n1\n \n2\n \n2\n \n2\n \n2\n \n2\n \n...\n\n \n$\n Yield\n:\n num  \n1545\n \n1440\n \n1440\n \n1520\n \n1580\n \n...\n\n\n \n(\nfm1 \n-\n lmer\n(\nYield \n~\n \n1\n|\nBatch\n,\n Dyestuff\n,\n REML\n=\nFALSE\n))\n\nLinear mixed model fit by maximum likelihood \n[\nlmerMod\n]\n\nFormula\n:\n Yield \n~\n \n1\n \n|\n Batch\n   Data\n:\n Dyestuff\n\n      AIC       BIC    logLik  deviance\n \n333.3271\n  \n337.5307\n \n-163.6635\n  \n327.3271\n\n\nRandom effects\n:\n\n Groups   Name        Variance Std.Dev.\n Batch    \n(\nIntercept\n)\n \n1388\n     \n37.26\n\n Residual             \n2451\n     \n49.51\n\n Number of obs\n:\n \n30\n,\n groups\n:\n Batch\n,\n \n6\n\n\nFixed effects\n:\n\n            Estimate Std. Error t value\n\n(\nIntercept\n)\n  \n1527.50\n      \n17.69\n   \n86.33\n\n\n\n\n\n\nThese \nDyestuff\n data are available through \nRCall\n but to run the doctests we use a stored copy of the dataframe.\n\n\njulia\n \nusing\n \nDataFrames\n,\n \nMixedModels\n\n\n\njulia\n \nds\n\n\n30\n\u00d72\n \nDataFrames\n.\nDataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \nYield\n  \n\u2502\n \nBatch\n \n\u2502\n\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n\n\u2502\n \n1\n   \n\u2502\n \n1545.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n1440.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n1440.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n1520.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n1580.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n1540.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n7\n   \n\u2502\n \n1555.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n8\n   \n\u2502\n \n1490.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u22ee\n\n\n\u2502\n \n22\n  \n\u2502\n \n1630.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n23\n  \n\u2502\n \n1515.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n24\n  \n\u2502\n \n1635.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n25\n  \n\u2502\n \n1625.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n26\n  \n\u2502\n \n1520.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n27\n  \n\u2502\n \n1455.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n28\n  \n\u2502\n \n1450.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n29\n  \n\u2502\n \n1480.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n30\n  \n\u2502\n \n1445.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\n\n\n\nlmm\n defaults to maximum likelihood estimation whereas \nlmer\n in \nR\n defaults to REML estimation.\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nFormula\n:\n \nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n)\n\n   \nlogLik\n    \n-\n2\n \nlogLik\n     \nAIC\n        \nBIC\n    \n  \n-\n163.66353\n  \n327.32706\n  \n333.32706\n  \n337.53065\n\n\n\nVariance\n \ncomponents\n:\n\n              \nColumn\n    \nVariance\n  \nStd\n.\nDev\n.\n\n \nBatch\n    \n(\nIntercept\n)\n  \n1388.3332\n \n37.260344\n\n \nResidual\n              \n2451.2500\n \n49.510100\n\n \nNumber\n \nof\n \nobs\n:\n \n30\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n6\n\n\n  \nFixed\n-\neffects\n \nparameters\n:\n\n             \nEstimate\n \nStd\n.\nError\n \nz\n \nvalue\n\n\n(\nIntercept\n)\n    \n1527.5\n   \n17.6946\n  \n86.326\n\n\n\n\n\n\nIn general the model should be fit through an explicit call to the \nfit!\n function, which may take a second argument indicating a verbose fit.\n\n\njulia\n \nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n),\n \ntrue\n);\n\n\nf_1\n:\n \n327.76702\n,\n \n[\n1.0\n]\n\n\nf_2\n:\n \n331.03619\n,\n \n[\n1.75\n]\n\n\nf_3\n:\n \n330.64583\n,\n \n[\n0.25\n]\n\n\nf_4\n:\n \n327.69511\n,\n \n[\n0.97619\n]\n\n\nf_5\n:\n \n327.56631\n,\n \n[\n0.928569\n]\n\n\nf_6\n:\n \n327.3826\n,\n \n[\n0.833327\n]\n\n\nf_7\n:\n \n327.35315\n,\n \n[\n0.807188\n]\n\n\nf_8\n:\n \n327.34663\n,\n \n[\n0.799688\n]\n\n\nf_9\n:\n \n327.341\n,\n \n[\n0.792188\n]\n\n\nf_10\n:\n \n327.33253\n,\n \n[\n0.777188\n]\n\n\nf_11\n:\n \n327.32733\n,\n \n[\n0.747188\n]\n\n\nf_12\n:\n \n327.32862\n,\n \n[\n0.739688\n]\n\n\nf_13\n:\n \n327.32706\n,\n \n[\n0.752777\n]\n\n\nf_14\n:\n \n327.32707\n,\n \n[\n0.753527\n]\n\n\nf_15\n:\n \n327.32706\n,\n \n[\n0.752584\n]\n\n\nf_16\n:\n \n327.32706\n,\n \n[\n0.752509\n]\n\n\nf_17\n:\n \n327.32706\n,\n \n[\n0.752591\n]\n\n\nf_18\n:\n \n327.32706\n,\n \n[\n0.752581\n]\n\n\nFTOL_REACHED\n\n\n\n\n\n\nThe numeric representation of the model has type\n\n\njulia\n \ntypeof\n(\nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n)))\n\n\nMixedModels\n.\nLinearMixedModel\n{\nFloat64\n}\n\n\n\n\n\n\nThose familiar with the \nlme4\n package for \nR\n will see the usual suspects.\n\n\njulia\n \nm\n \n=\n \nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n));\n\n\n\njulia\n \nfixef\n(\nm\n)\n  \n# estimates of the fixed-effects parameters\n\n\n1\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n1527.5\n\n\n\njulia\n \ncoef\n(\nm\n)\n  \n# another name for fixef\n\n\n1\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n1527.5\n\n\n\njulia\n \nranef\n(\nm\n)\n\n\n1\n-\nelement\n \nArray\n{\nArray\n{\nFloat64\n,\n2\n},\n1\n}:\n\n \n1\n\u00d76\n \nArray\n{\nFloat64\n,\n2\n}:\n\n \n-\n16.6282\n  \n0.369516\n  \n26.9747\n  \n-\n21.8014\n  \n53.5798\n  \n-\n42.4943\n\n\n\njulia\n \nranef\n(\nm\n,\n \ntrue\n)\n  \n# on the u scale\n\n\n1\n-\nelement\n \nArray\n{\nArray\n{\nFloat64\n,\n2\n},\n1\n}:\n\n \n1\n\u00d76\n \nArray\n{\nFloat64\n,\n2\n}:\n\n \n-\n22.0949\n  \n0.490999\n  \n35.8429\n  \n-\n28.9689\n  \n71.1948\n  \n-\n56.4648\n\n\n\njulia\n \ndeviance\n(\nm\n)\n\n\n327.3270598811394\n\n\n\njulia\n \nobjective\n(\nm\n)\n\n\n327.3270598811394\n\n\n\n\n\n\nWe prefer \nobjective\n to \ndeviance\n because the value returned is \n-2loglikelihood(m)\n, without the correction for the null deviance. It is not clear how the null deviance should be defined for these models.\n\n\n\n\nMore substantial examples\n\n\nFitting a model to the \nDyestuff\n data is trivial.  The \nInstEval\n data in the \nlme4\n package is more of a challenge in that there are nearly 75,000 evaluations by 2972 students on a total of 1128 instructors.\n\n\njulia\n \nhead\n(\ninst\n)\n\n\n6x7\n \nDataFrames\n.DataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \ns\n   \n\u2502\n \nd\n      \n\u2502\n \nstudage\n \n\u2502\n \nlectage\n \n\u2502\n \nservice\n \n\u2502\n \ndept\n \n\u2502\n \ny\n \n\u2502\n\n\n\u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2525\n\n\n\u2502\n \n1\n   \n\u2502\n \n1\n \n\u2502\n \n1002\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n0\n     \n\u2502\n \n2\n  \n\u2502\n \n5\n \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n1\n \n\u2502\n \n1050\n \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n1\n     \n\u2502\n \n6\n  \n\u2502\n \n2\n \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n1\n \n\u2502\n \n1582\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n0\n     \n\u2502\n \n2\n  \n\u2502\n \n5\n \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n1\n \n\u2502\n \n2050\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n3\n  \n\u2502\n \n3\n \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n2\n \n\u2502\n \n115\n  \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n0\n     \n\u2502\n \n5\n  \n\u2502\n \n2\n \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n2\n \n\u2502\n \n756\n  \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n0\n     \n\u2502\n \n5\n  \n\u2502\n \n4\n \n\u2502\n\n\n\njulia\n \nm2\n \n=\n \nfit\n!(\nlmm\n(\ny\n \n~\n \n1\n \n+\n \ndept\n*\nservice\n \n+\n \n(\n1\n|\ns\n)\n \n+\n \n(\n1\n|\nd\n),\n \ninst\n))\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nlogLik\n:\n \n-118792\n.776708\n,\n \ndeviance\n:\n \n237585\n.553415\n,\n \nAIC\n:\n \n237647\n.553415\n,\n \nBIC\n:\n \n237932\n.876339\n\n\n\nVariance\n \ncomponents\n:\n\n            \nVariance\n   \nStd\n.Dev\n.\n\n \ns\n        \n0\n.105417971\n \n0\n.32468134\n\n \nd\n        \n0\n.258416394\n \n0\n.50834673\n\n \nResidual\n \n1\n.384727771\n \n1\n.17674456\n\n \nNumber\n \nof\n \nobs\n:\n \n73421\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n2972\n,\n \n1128\n\n\n  \nFixed-effects\n \nparameters\n:\n\n                           \nEstimate\n \nStd\n.Error\n   \nz\n \nvalue\n\n\n(\nIntercept\n)\n                 \n3\n.22961\n  \n0\n.064053\n   \n50\n.4209\n\n\ndept\n \n-\n \n5\n                   \n0\n.129536\n  \n0\n.101294\n   \n1\n.27882\n\n\ndept\n \n-\n \n10\n                 \n-0\n.176751\n \n0\n.0881352\n  \n-2\n.00545\n\n\ndept\n \n-\n \n12\n                 \n0\n.0517102\n \n0\n.0817524\n  \n0\n.632522\n\n\ndept\n \n-\n \n6\n                  \n0\n.0347319\n  \n0\n.085621\n  \n0\n.405647\n\n\ndept\n \n-\n \n7\n                    \n0\n.14594\n \n0\n.0997984\n   \n1\n.46235\n\n\ndept\n \n-\n \n4\n                   \n0\n.151689\n \n0\n.0816897\n   \n1\n.85689\n\n\ndept\n \n-\n \n8\n                   \n0\n.104206\n  \n0\n.118751\n  \n0\n.877517\n\n\ndept\n \n-\n \n9\n                  \n0\n.0440401\n \n0\n.0962985\n  \n0\n.457329\n\n\ndept\n \n-\n \n14\n                 \n0\n.0517546\n \n0\n.0986029\n  \n0\n.524879\n\n\ndept\n \n-\n \n1\n                  \n0\n.0466719\n  \n0\n.101942\n  \n0\n.457828\n\n\ndept\n \n-\n \n3\n                  \n0\n.0563461\n \n0\n.0977925\n   \n0\n.57618\n\n\ndept\n \n-\n \n11\n                 \n0\n.0596536\n  \n0\n.100233\n   \n0\n.59515\n\n\ndept\n \n-\n \n2\n                 \n0\n.00556281\n  \n0\n.110867\n \n0\n.0501757\n\n\nservice\n \n-\n \n1\n                \n0\n.252025\n \n0\n.0686507\n   \n3\n.67112\n\n\ndept\n \n-\n \n5\n \n \nservice\n \n-\n \n1\n    \n-0\n.180757\n  \n0\n.123179\n  \n-1\n.46744\n\n\ndept\n \n-\n \n10\n \n \nservice\n \n-\n \n1\n   \n0\n.0186492\n  \n0\n.110017\n  \n0\n.169512\n\n\ndept\n \n-\n \n12\n \n \nservice\n \n-\n \n1\n   \n-0\n.282269\n \n0\n.0792937\n  \n-3\n.55979\n\n\ndept\n \n-\n \n6\n \n \nservice\n \n-\n \n1\n    \n-0\n.494464\n \n0\n.0790278\n  \n-6\n.25683\n\n\ndept\n \n-\n \n7\n \n \nservice\n \n-\n \n1\n    \n-0\n.392054\n  \n0\n.110313\n  \n-3\n.55403\n\n\ndept\n \n-\n \n4\n \n \nservice\n \n-\n \n1\n    \n-0\n.278547\n \n0\n.0823727\n  \n-3\n.38154\n\n\ndept\n \n-\n \n8\n \n \nservice\n \n-\n \n1\n    \n-0\n.189526\n  \n0\n.111449\n  \n-1\n.70056\n\n\ndept\n \n-\n \n9\n \n \nservice\n \n-\n \n1\n    \n-0\n.499868\n \n0\n.0885423\n  \n-5\n.64553\n\n\ndept\n \n-\n \n14\n \n \nservice\n \n-\n \n1\n   \n-0\n.497162\n \n0\n.0917162\n  \n-5\n.42065\n\n\ndept\n \n-\n \n1\n \n \nservice\n \n-\n \n1\n     \n-0\n.24042\n \n0\n.0982071\n   \n-2\n.4481\n\n\ndept\n \n-\n \n3\n \n \nservice\n \n-\n \n1\n    \n-0\n.223013\n \n0\n.0890548\n  \n-2\n.50422\n\n\ndept\n \n-\n \n11\n \n \nservice\n \n-\n \n1\n   \n-0\n.516997\n \n0\n.0809077\n  \n-6\n.38997\n\n\ndept\n \n-\n \n2\n \n \nservice\n \n-\n \n1\n    \n-0\n.384773\n  \n0\n.091843\n  \n-4\n.18946\n\n\n\n\n\n\nModels with vector-valued random effects can be fit\n\n\njulia\n \nslp\n\n\n180\n\u00d73\n \nDataFrames\n.\nDataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \nReaction\n \n\u2502\n \nDays\n \n\u2502\n \nSubject\n \n\u2502\n\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n\n\u2502\n \n1\n   \n\u2502\n \n249.56\n   \n\u2502\n \n0\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n258.705\n  \n\u2502\n \n1\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n250.801\n  \n\u2502\n \n2\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n321.44\n   \n\u2502\n \n3\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n356.852\n  \n\u2502\n \n4\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n414.69\n   \n\u2502\n \n5\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n7\n   \n\u2502\n \n382.204\n  \n\u2502\n \n6\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n8\n   \n\u2502\n \n290.149\n  \n\u2502\n \n7\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u22ee\n\n\n\u2502\n \n172\n \n\u2502\n \n273.474\n  \n\u2502\n \n1\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n173\n \n\u2502\n \n297.597\n  \n\u2502\n \n2\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n174\n \n\u2502\n \n310.632\n  \n\u2502\n \n3\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n175\n \n\u2502\n \n287.173\n  \n\u2502\n \n4\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n176\n \n\u2502\n \n329.608\n  \n\u2502\n \n5\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n177\n \n\u2502\n \n334.482\n  \n\u2502\n \n6\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n178\n \n\u2502\n \n343.22\n   \n\u2502\n \n7\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n179\n \n\u2502\n \n369.142\n  \n\u2502\n \n8\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n180\n \n\u2502\n \n364.124\n  \n\u2502\n \n9\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\njulia\n \nfm3\n \n=\n \nfit!\n(\nlmm\n(\nReaction\n \n~\n \n1\n \n+\n \nDays\n \n+\n \n(\n1\n+\nDays\n|\nSubject\n),\n \nslp\n))\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nFormula\n:\n \nReaction\n \n~\n \n1\n \n+\n \nDays\n \n+\n \n((\n1\n \n+\n \nDays\n)\n \n|\n \nSubject\n)\n\n  \nlogLik\n    \n-\n2\n \nlogLik\n     \nAIC\n        \nBIC\n\n \n-\n875.96967\n \n1751.93934\n \n1763.93934\n \n1783.09709\n\n\n\nVariance\n \ncomponents\n:\n\n              \nColumn\n    \nVariance\n  \nStd\n.\nDev\n.\n   \nCorr\n.\n\n \nSubject\n  \n(\nIntercept\n)\n  \n565.51066\n \n23.780468\n\n          \nDays\n          \n32.68212\n  \n5.716828\n  \n0.08\n\n \nResidual\n \nDays\n         \n654.94145\n \n25.591824\n\n \nNumber\n \nof\n \nobs\n:\n \n180\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n18\n\n\n  \nFixed\n-\neffects\n \nparameters\n:\n\n             \nEstimate\n \nStd\n.\nError\n \nz\n \nvalue\n \nP\n(\n|\nz\n|\n)\n\n\n(\nIntercept\n)\n   \n251.405\n   \n6.63226\n \n37.9064\n  \n1e-99\n\n\nDays\n          \n10.4673\n   \n1.50224\n \n6.96781\n  \n1e-11", 
            "title": "Fitting"
        }, 
        {
            "location": "/man/fitting/#fitting-linear-mixed-effects-models", 
            "text": "The  lmm  function is similar to the  lmer  function in the  lme4  package for  R .  The first two arguments for in the  R  version are  formula  and  data .  The principle method for the  Julia  version takes these arguments.", 
            "title": "Fitting linear mixed-effects models"
        }, 
        {
            "location": "/man/fitting/#a-simple-example", 
            "text": "The simplest example of a mixed-effects model that we use in the  lme4 package for R  is a model fit to the  Dyestuff  data.   str ( Dyestuff )  data.frame :     30  obs. of   2  variables : \n  $  Batch :  Factor w /   6  levels  A , B , C , D , .. :   1   1   1   1   1   2   2   2   2   2   ... \n  $  Yield :  num   1545   1440   1440   1520   1580   ...    ( fm1  -  lmer ( Yield  ~   1 | Batch ,  Dyestuff ,  REML = FALSE )) \nLinear mixed model fit by maximum likelihood  [ lmerMod ] \nFormula :  Yield  ~   1   |  Batch\n   Data :  Dyestuff\n\n      AIC       BIC    logLik  deviance\n  333.3271    337.5307   -163.6635    327.3271 \n\nRandom effects : \n Groups   Name        Variance Std.Dev.\n Batch     ( Intercept )   1388       37.26 \n Residual              2451       49.51 \n Number of obs :   30 ,  groups :  Batch ,   6 \n\nFixed effects : \n            Estimate Std. Error t value ( Intercept )    1527.50        17.69     86.33   These  Dyestuff  data are available through  RCall  but to run the doctests we use a stored copy of the dataframe.  julia   using   DataFrames ,   MixedModels  julia   ds  30 \u00d72   DataFrames . DataFrame  \u2502   Row   \u2502   Yield    \u2502   Batch   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   1     \u2502   1545.0   \u2502   A     \u2502  \u2502   2     \u2502   1440.0   \u2502   A     \u2502  \u2502   3     \u2502   1440.0   \u2502   A     \u2502  \u2502   4     \u2502   1520.0   \u2502   A     \u2502  \u2502   5     \u2502   1580.0   \u2502   A     \u2502  \u2502   6     \u2502   1540.0   \u2502   B     \u2502  \u2502   7     \u2502   1555.0   \u2502   B     \u2502  \u2502   8     \u2502   1490.0   \u2502   B     \u2502  \u22ee  \u2502   22    \u2502   1630.0   \u2502   E     \u2502  \u2502   23    \u2502   1515.0   \u2502   E     \u2502  \u2502   24    \u2502   1635.0   \u2502   E     \u2502  \u2502   25    \u2502   1625.0   \u2502   E     \u2502  \u2502   26    \u2502   1520.0   \u2502   F     \u2502  \u2502   27    \u2502   1455.0   \u2502   F     \u2502  \u2502   28    \u2502   1450.0   \u2502   F     \u2502  \u2502   29    \u2502   1480.0   \u2502   F     \u2502  \u2502   30    \u2502   1445.0   \u2502   F     \u2502   lmm  defaults to maximum likelihood estimation whereas  lmer  in  R  defaults to REML estimation.  Linear   mixed   model   fit   by   maximum   likelihood \n  Formula :   Yield   ~   1   +   ( 1   |   Batch ) \n    logLik      - 2   logLik       AIC          BIC     \n   - 163.66353    327.32706    333.32706    337.53065  Variance   components : \n               Column      Variance    Std . Dev . \n  Batch      ( Intercept )    1388.3332   37.260344 \n  Residual                2451.2500   49.510100 \n  Number   of   obs :   30 ;   levels   of   grouping   factors :   6 \n\n   Fixed - effects   parameters : \n              Estimate   Std . Error   z   value  ( Intercept )      1527.5     17.6946    86.326   In general the model should be fit through an explicit call to the  fit!  function, which may take a second argument indicating a verbose fit.  julia   fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds ),   true );  f_1 :   327.76702 ,   [ 1.0 ]  f_2 :   331.03619 ,   [ 1.75 ]  f_3 :   330.64583 ,   [ 0.25 ]  f_4 :   327.69511 ,   [ 0.97619 ]  f_5 :   327.56631 ,   [ 0.928569 ]  f_6 :   327.3826 ,   [ 0.833327 ]  f_7 :   327.35315 ,   [ 0.807188 ]  f_8 :   327.34663 ,   [ 0.799688 ]  f_9 :   327.341 ,   [ 0.792188 ]  f_10 :   327.33253 ,   [ 0.777188 ]  f_11 :   327.32733 ,   [ 0.747188 ]  f_12 :   327.32862 ,   [ 0.739688 ]  f_13 :   327.32706 ,   [ 0.752777 ]  f_14 :   327.32707 ,   [ 0.753527 ]  f_15 :   327.32706 ,   [ 0.752584 ]  f_16 :   327.32706 ,   [ 0.752509 ]  f_17 :   327.32706 ,   [ 0.752591 ]  f_18 :   327.32706 ,   [ 0.752581 ]  FTOL_REACHED   The numeric representation of the model has type  julia   typeof ( fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds )))  MixedModels . LinearMixedModel { Float64 }   Those familiar with the  lme4  package for  R  will see the usual suspects.  julia   m   =   fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds ));  julia   fixef ( m )    # estimates of the fixed-effects parameters  1 - element   Array { Float64 , 1 }: \n  1527.5  julia   coef ( m )    # another name for fixef  1 - element   Array { Float64 , 1 }: \n  1527.5  julia   ranef ( m )  1 - element   Array { Array { Float64 , 2 }, 1 }: \n  1 \u00d76   Array { Float64 , 2 }: \n  - 16.6282    0.369516    26.9747    - 21.8014    53.5798    - 42.4943  julia   ranef ( m ,   true )    # on the u scale  1 - element   Array { Array { Float64 , 2 }, 1 }: \n  1 \u00d76   Array { Float64 , 2 }: \n  - 22.0949    0.490999    35.8429    - 28.9689    71.1948    - 56.4648  julia   deviance ( m )  327.3270598811394  julia   objective ( m )  327.3270598811394   We prefer  objective  to  deviance  because the value returned is  -2loglikelihood(m) , without the correction for the null deviance. It is not clear how the null deviance should be defined for these models.", 
            "title": "A simple example"
        }, 
        {
            "location": "/man/fitting/#more-substantial-examples", 
            "text": "Fitting a model to the  Dyestuff  data is trivial.  The  InstEval  data in the  lme4  package is more of a challenge in that there are nearly 75,000 evaluations by 2972 students on a total of 1128 instructors.  julia   head ( inst )  6x7   DataFrames .DataFrame  \u2502   Row   \u2502   s     \u2502   d        \u2502   studage   \u2502   lectage   \u2502   service   \u2502   dept   \u2502   y   \u2502  \u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2525  \u2502   1     \u2502   1   \u2502   1002   \u2502   2       \u2502   2       \u2502   0       \u2502   2    \u2502   5   \u2502  \u2502   2     \u2502   1   \u2502   1050   \u2502   2       \u2502   1       \u2502   1       \u2502   6    \u2502   2   \u2502  \u2502   3     \u2502   1   \u2502   1582   \u2502   2       \u2502   2       \u2502   0       \u2502   2    \u2502   5   \u2502  \u2502   4     \u2502   1   \u2502   2050   \u2502   2       \u2502   2       \u2502   1       \u2502   3    \u2502   3   \u2502  \u2502   5     \u2502   2   \u2502   115    \u2502   2       \u2502   1       \u2502   0       \u2502   5    \u2502   2   \u2502  \u2502   6     \u2502   2   \u2502   756    \u2502   2       \u2502   1       \u2502   0       \u2502   5    \u2502   4   \u2502  julia   m2   =   fit !( lmm ( y   ~   1   +   dept * service   +   ( 1 | s )   +   ( 1 | d ),   inst ))  Linear   mixed   model   fit   by   maximum   likelihood \n  logLik :   -118792 .776708 ,   deviance :   237585 .553415 ,   AIC :   237647 .553415 ,   BIC :   237932 .876339  Variance   components : \n             Variance     Std .Dev . \n  s          0 .105417971   0 .32468134 \n  d          0 .258416394   0 .50834673 \n  Residual   1 .384727771   1 .17674456 \n  Number   of   obs :   73421 ;   levels   of   grouping   factors :   2972 ,   1128 \n\n   Fixed-effects   parameters : \n                            Estimate   Std .Error     z   value  ( Intercept )                   3 .22961    0 .064053     50 .4209  dept   -   5                     0 .129536    0 .101294     1 .27882  dept   -   10                   -0 .176751   0 .0881352    -2 .00545  dept   -   12                   0 .0517102   0 .0817524    0 .632522  dept   -   6                    0 .0347319    0 .085621    0 .405647  dept   -   7                      0 .14594   0 .0997984     1 .46235  dept   -   4                     0 .151689   0 .0816897     1 .85689  dept   -   8                     0 .104206    0 .118751    0 .877517  dept   -   9                    0 .0440401   0 .0962985    0 .457329  dept   -   14                   0 .0517546   0 .0986029    0 .524879  dept   -   1                    0 .0466719    0 .101942    0 .457828  dept   -   3                    0 .0563461   0 .0977925     0 .57618  dept   -   11                   0 .0596536    0 .100233     0 .59515  dept   -   2                   0 .00556281    0 .110867   0 .0501757  service   -   1                  0 .252025   0 .0686507     3 .67112  dept   -   5     service   -   1      -0 .180757    0 .123179    -1 .46744  dept   -   10     service   -   1     0 .0186492    0 .110017    0 .169512  dept   -   12     service   -   1     -0 .282269   0 .0792937    -3 .55979  dept   -   6     service   -   1      -0 .494464   0 .0790278    -6 .25683  dept   -   7     service   -   1      -0 .392054    0 .110313    -3 .55403  dept   -   4     service   -   1      -0 .278547   0 .0823727    -3 .38154  dept   -   8     service   -   1      -0 .189526    0 .111449    -1 .70056  dept   -   9     service   -   1      -0 .499868   0 .0885423    -5 .64553  dept   -   14     service   -   1     -0 .497162   0 .0917162    -5 .42065  dept   -   1     service   -   1       -0 .24042   0 .0982071     -2 .4481  dept   -   3     service   -   1      -0 .223013   0 .0890548    -2 .50422  dept   -   11     service   -   1     -0 .516997   0 .0809077    -6 .38997  dept   -   2     service   -   1      -0 .384773    0 .091843    -4 .18946   Models with vector-valued random effects can be fit  julia   slp  180 \u00d73   DataFrames . DataFrame  \u2502   Row   \u2502   Reaction   \u2502   Days   \u2502   Subject   \u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u2502   1     \u2502   249.56     \u2502   0      \u2502   1         \u2502  \u2502   2     \u2502   258.705    \u2502   1      \u2502   1         \u2502  \u2502   3     \u2502   250.801    \u2502   2      \u2502   1         \u2502  \u2502   4     \u2502   321.44     \u2502   3      \u2502   1         \u2502  \u2502   5     \u2502   356.852    \u2502   4      \u2502   1         \u2502  \u2502   6     \u2502   414.69     \u2502   5      \u2502   1         \u2502  \u2502   7     \u2502   382.204    \u2502   6      \u2502   1         \u2502  \u2502   8     \u2502   290.149    \u2502   7      \u2502   1         \u2502  \u22ee  \u2502   172   \u2502   273.474    \u2502   1      \u2502   18        \u2502  \u2502   173   \u2502   297.597    \u2502   2      \u2502   18        \u2502  \u2502   174   \u2502   310.632    \u2502   3      \u2502   18        \u2502  \u2502   175   \u2502   287.173    \u2502   4      \u2502   18        \u2502  \u2502   176   \u2502   329.608    \u2502   5      \u2502   18        \u2502  \u2502   177   \u2502   334.482    \u2502   6      \u2502   18        \u2502  \u2502   178   \u2502   343.22     \u2502   7      \u2502   18        \u2502  \u2502   179   \u2502   369.142    \u2502   8      \u2502   18        \u2502  \u2502   180   \u2502   364.124    \u2502   9      \u2502   18        \u2502  julia   fm3   =   fit! ( lmm ( Reaction   ~   1   +   Days   +   ( 1 + Days | Subject ),   slp ))  Linear   mixed   model   fit   by   maximum   likelihood \n  Formula :   Reaction   ~   1   +   Days   +   (( 1   +   Days )   |   Subject ) \n   logLik      - 2   logLik       AIC          BIC \n  - 875.96967   1751.93934   1763.93934   1783.09709  Variance   components : \n               Column      Variance    Std . Dev .     Corr . \n  Subject    ( Intercept )    565.51066   23.780468 \n           Days            32.68212    5.716828    0.08 \n  Residual   Days           654.94145   25.591824 \n  Number   of   obs :   180 ;   levels   of   grouping   factors :   18 \n\n   Fixed - effects   parameters : \n              Estimate   Std . Error   z   value   P ( | z | )  ( Intercept )     251.405     6.63226   37.9064    1e-99  Days            10.4673     1.50224   6.96781    1e-11", 
            "title": "More substantial examples"
        }, 
        {
            "location": "/lib/public/", 
            "text": "Public Documentation\n\n\nDocumentation for \nMixedModels.jl\n's public interface.\n\n\nSee \nInternal Documentation\n for internal package docs covering all submodules.\n\n\n\n\nContents\n\n\n\n\nPublic Documentation\n\n\nContents\n\n\nIndex\n\n\nMixedModels\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nBase.LinAlg.cond\n\n\nMixedModels.LaplaceDeviance\n\n\nMixedModels.LinearMixedModel\n\n\nMixedModels.ScalarReMat\n\n\nMixedModels.VarCorr\n\n\nMixedModels.VectorReMat\n\n\nMixedModels.bootstrap\n\n\nMixedModels.fixef\n\n\nMixedModels.lmm\n\n\nMixedModels.lowerbd\n\n\nMixedModels.objective\n\n\nMixedModels.pirls!\n\n\nMixedModels.pwrss\n\n\nMixedModels.ranef\n\n\nMixedModels.refit!\n\n\nMixedModels.remat\n\n\nMixedModels.sdest\n\n\nMixedModels.simulate!\n\n\nMixedModels.varest\n\n\nStatsBase.deviance\n\n\nStatsBase.df\n\n\nStatsBase.fit!\n\n\nStatsBase.model_response\n\n\nStatsBase.nobs\n\n\nStatsBase.vcov\n\n\n\n\n\n\nMixedModels\n\n\n#\n\n\nMixedModels.LinearMixedModel\n \n \nType\n.\n\n\nLinearMixedModel\n\n\n\n\n\nLinear mixed-effects model representation\n\n\nMembers:\n\n\n\n\nformula\n: the formula for the model\n\n\nmf\n: the model frame, mostly used to get the \nterms\n component for labelling fixed effects\n\n\nwttrms\n: a length \nnt\n vector of weighted model matrices. The last two elements are \nX\n and \ny\n.\n\n\ntrms\n: a vector of unweighted model matrices.  If \nisempty(sqrtwts)\n the same object as \nwttrms\n\n\n\u039b\n: a length \nnt - 2\n vector of lower triangular matrices\n\n\nsqrtwts\n: the \nDiagonal\n matrix of the square roots of the case weights.  Allowed to be size 0\n\n\nA\n: an \nnt \u00d7 nt\n symmetric matrix of matrices representing \nhcat(Z,X,y)'hcat(Z,X,y)\n\n\nR\n: a \nnt \u00d7 nt\n matrix of matrices - the upper Cholesky factor of \n\u039b'A\u039b+I\n\n\nopt\n: an \nOptSummary\n object\n\n\n\n\n#\n\n\nMixedModels.ScalarReMat\n \n \nType\n.\n\n\nScalarReMat\n\n\n\n\n\nThe representation of the model matrix for a scalar random-effects term\n\n\nMembers:\n\n\n\n\nf\n: the grouping factor as a \nPooledDataVector\n\n\nz\n: the raw random-effects model matrix as a \nVector\n\n\nfnm\n: the name of the grouping factor as a \nSymbol\n\n\ncnms\n: a \nVector\n of column names\n\n\n\n\n#\n\n\nMixedModels.VarCorr\n \n \nType\n.\n\n\nVarCorr\n\n\n\n\n\nAn encapsulation of information on the fitted random-effects variance-covariance matrices.\n\n\nMembers:\n\n\n\n\n\u039b\n: the vector of lower triangular matrices from the \nMixedModel\n\n\nfnms\n: a \nVector{ASCIIString}\n of grouping factor names\n\n\ncnms\n: a \nVector{Vector{ASCIIString}}\n of column names\n\n\ns\n: the estimate of \u03c3, the standard deviation of the per-observation noise\n\n\n\n\nThe main purpose of defining this type is to isolate the logic in the show method.\n\n\n#\n\n\nMixedModels.VectorReMat\n \n \nType\n.\n\n\nVectorReMat\n\n\n\n\n\nThe representation of the model matrix for a vector-valued random-effects term\n\n\nMembers:\n\n\n\n\nf\n: the grouping factor as a \nPooledDataVector\n\n\nz\n: the transposed raw random-effects model matrix\n\n\nfnm\n: the name of the grouping factor as a \nSymbol\n\n\ncnms\n: a \nVector\n of column names (row names after transposition) of \nz\n\n\n\n\n#\n\n\nMixedModels.bootstrap\n \n \nFunction\n.\n\n\nbootstrap(m::LinearMixedModels, N::Integer, saveresults::Function)\n\n\n\n\n\nSimulate \nN\n response vectors from \nm\n, refitting the model.  \nsaveresults\n is called after each refit with arguments \ni::Int\n, the index, and \nm\n.\n\n\nTo save space \nm.trms[end]\n, which is the response vector, is overwritten by each simulation.  The original response is restored and the model refit before returning.\n\n\n#\n\n\nBase.LinAlg.cond\n \n \nMethod\n.\n\n\ncond(m::MixedModel)\n\n\n\n\n\nReturns a vector of the condition numbers of the blocks of \nm.\u039b\n\n\n#\n\n\nStatsBase.df\n \n \nMethod\n.\n\n\ndf(obj::StatisticalModel)\n\n\nReturns the number of degrees of freedom consumed in the model, including when applicable the intercept and the distribution's dispersion parameter.\n\n\n#\n\n\nStatsBase.deviance\n \n \nMethod\n.\n\n\ndeviance(obj::StatisticalModel)\n\n\nReturns the deviance of the model relative to a reference, which is usually when applicable the saturated model. It is equal, \nup to a constant\n, to \n-2 log L\n, with \nL\n the likelihood of the model.\n\n\n#\n\n\nStatsBase.fit!\n \n \nMethod\n.\n\n\nfit\n!(\nm\n:\n:LinearMixedModel\n,\n \nverbose\n=\nfalse\n;\n \noptimizer\n=\n:LN_BOBYQA\n)\n\n\n\n\n\n\nOptimize the objective of a \nLinearMixedModel\n using an \nNLopt\n optimizer.\n\n\nNamed Arguments:\n\n\n\n\noptimizer::Symbol\n the name of a derivative-free optimizer from \nNLopt\n that allows for   box constraints.\n\n\n\n\n#\n\n\nMixedModels.fixef\n \n \nFunction\n.\n\n\nfixef(m::MixedModel)\n\n\n\n\n\nReturns the estimate of the fixed-effects parameter vector.\n\n\n#\n\n\nMixedModels.LaplaceDeviance\n \n \nFunction\n.\n\n\nLaplaceDeviance(m::GeneralizedLinearMixedModel)\n\n\n\n\n\nLaplace approximation to the deviance of a GLMM.  For a distribution that does not have a scale factor this is defined as the squared length of the conditional modes, \nu\n, plus the determinant of \n\u039b'Z'Z\u039b + 1\n, plus the sum of the squared deviance residuals.\n\n\n#\n\n\nMixedModels.lmm\n \n \nFunction\n.\n\n\nlmm(m::MixedModel)\n\n\n\n\n\nExtract the \nLinearMixedModel\n from a \nMixedModel\n.  If \nm\n is a \nLinearMixedModel\n return \nm\n. If \nm\n is a \nGeneralizedLinearMixedModel\n return \nm.LMM\n.\n\n\nlmm\n(\nf\n:\n:DataFrames\n.Formula\n,\n \nfr\n:\n:DataFrames\n.DataFrame\n)\n\n\nlmm\n(\nf\n:\n:DataFrames\n.Formula\n,\n \nfr\n:\n:DataFrames\n.DataFrame\n;\n \nweights\n \n=\n \n[]\n)\n\n\n\n\n\n\nCreate a \nLinearMixedModel\n from \nf\n, which contains both fixed-effects terms and random effects, and \nfr\n. The return value is ready to be \nfit!\n but has not yet been fit.\n\n\n#\n\n\nMixedModels.lowerbd\n \n \nFunction\n.\n\n\nlowerbd(m)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nMixedModel\n\n\n\n\nReturns:   A \nVector\n of lower bounds on the covariance parameter vector \nm[:\u03b8]\n\n\nlowerbd{T}(A::LowerTriangular{T,Matrix{T}})\n\n\n\n\n\nlower bounds on the parameters (elements in the lower triangle)\n\n\n#\n\n\nStatsBase.model_response\n \n \nMethod\n.\n\n\nnothing\n\n#\n\n\nStatsBase.nobs\n \n \nMethod\n.\n\n\nnobs(obj::StatisticalModel)\n\n\nReturns the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.\n\n\n#\n\n\nMixedModels.objective\n \n \nFunction\n.\n\n\nobjective(m::LinearMixedModel)\n\n\n\n\n\nNegative twice the log-likelihood of model \nm\n\n\n#\n\n\nMixedModels.pwrss\n \n \nFunction\n.\n\n\npwrss(m::LinearMixedModel)\n\n\n\n\n\nThe penalized residual sum-of-squares.\n\n\n#\n\n\nMixedModels.pirls!\n \n \nFunction\n.\n\n\npirls!(m::GeneralizedLinearMixedModel)\n\n\n\n\n\nUse Penalized Iteratively Reweighted Least Squares (PIRLS) to determine the conditional modes of the random effects.\n\n\n#\n\n\nMixedModels.ranef\n \n \nFunction\n.\n\n\nranef(m)\nranef(m, uscale)\n\n\n\n\n\nConditional modes of the random effects in model \nm\n\n\nArgs:\n\n\n\n\nm\n: a fitted \nMixedModel\n object\n\n\nuscale\n: a \nBool\n indicating conditional modes are on the \nu\n scale or the \nb\n scale.  Defaults to \nfalse\n\n\n\n\nReturns:   A \nVector\n of matrices of the conditional modes of the random effects on the indicated scale.   For a scalar random-effects term the matrix is \n1 \u00d7 k\n where \nk\n is the number of levels of the grouping factor.   For a vector-valued random-effects term the matrix is \nl \u00d7 k\n where \nl\n is the dimension of each random effect.\n\n\n#\n\n\nMixedModels.refit!\n \n \nFunction\n.\n\n\nrefit!{T}(m::LinearMixedModel, y::Vector{T})\n\n\n\n\n\nRefit the model \nm\n with response \ny\n.\n\n\n#\n\n\nMixedModels.remat\n \n \nFunction\n.\n\n\n remat(e::Expr, df::DataFrames.DataFrame)\n\n\n\n\n\nA factory for \nReMat\n objects.  \ne\n should be of the form \n:(e1 | e2)\n where \ne1\n is a valid rhs of a \nFormula\n and \npool(e2)\n can be evaluated within \ndf\n.  The result is a \nScalarReMat\n or a \nVectorReMat\n, as appropriate.\n\n\n#\n\n\nMixedModels.sdest\n \n \nFunction\n.\n\n\nsdest(m::LinearMixedModel)\n\n\n\n\n\nThe estimate of \u03c3, the standard deviation of the per-observation noise.\n\n\n#\n\n\nMixedModels.simulate!\n \n \nFunction\n.\n\n\nsimulate\n!(\nm\n:\n:LinearMixedModel\n;\n \n\u03b2\n=\nfixef\n(\nm\n),\n \n\u03c3\n=\nsdest\n(\nm\n),\n \n\u03b8\n=\nm\n[\n:\n\u03b8\n]\n)\n\n\n\n\n\n\nInstall a simulated response vector in model \nm\n and refit it.\n\n\n#\n\n\nMixedModels.varest\n \n \nFunction\n.\n\n\nvarest(m::LinearMixedModel)\n\n\n\n\n\nThe estimate of \u03c3\u00b2, the variance of the conditional distribution of Y given B.\n\n\n#\n\n\nStatsBase.vcov\n \n \nFunction\n.\n\n\n vcov(m)\n\n\n\n\n\nEstimated covariance matrix of the fixed-effects estimator\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n\n\n\n\nReturns   a \np \u00d7 p\n \nMatrix", 
            "title": "Public"
        }, 
        {
            "location": "/lib/public/#public-documentation", 
            "text": "Documentation for  MixedModels.jl 's public interface.  See  Internal Documentation  for internal package docs covering all submodules.", 
            "title": "Public Documentation"
        }, 
        {
            "location": "/lib/public/#contents", 
            "text": "Public Documentation  Contents  Index  MixedModels", 
            "title": "Contents"
        }, 
        {
            "location": "/lib/public/#index", 
            "text": "Base.LinAlg.cond  MixedModels.LaplaceDeviance  MixedModels.LinearMixedModel  MixedModels.ScalarReMat  MixedModels.VarCorr  MixedModels.VectorReMat  MixedModels.bootstrap  MixedModels.fixef  MixedModels.lmm  MixedModels.lowerbd  MixedModels.objective  MixedModels.pirls!  MixedModels.pwrss  MixedModels.ranef  MixedModels.refit!  MixedModels.remat  MixedModels.sdest  MixedModels.simulate!  MixedModels.varest  StatsBase.deviance  StatsBase.df  StatsBase.fit!  StatsBase.model_response  StatsBase.nobs  StatsBase.vcov", 
            "title": "Index"
        }, 
        {
            "location": "/lib/public/#mixedmodels", 
            "text": "#  MixedModels.LinearMixedModel     Type .  LinearMixedModel  Linear mixed-effects model representation  Members:   formula : the formula for the model  mf : the model frame, mostly used to get the  terms  component for labelling fixed effects  wttrms : a length  nt  vector of weighted model matrices. The last two elements are  X  and  y .  trms : a vector of unweighted model matrices.  If  isempty(sqrtwts)  the same object as  wttrms  \u039b : a length  nt - 2  vector of lower triangular matrices  sqrtwts : the  Diagonal  matrix of the square roots of the case weights.  Allowed to be size 0  A : an  nt \u00d7 nt  symmetric matrix of matrices representing  hcat(Z,X,y)'hcat(Z,X,y)  R : a  nt \u00d7 nt  matrix of matrices - the upper Cholesky factor of  \u039b'A\u039b+I  opt : an  OptSummary  object   #  MixedModels.ScalarReMat     Type .  ScalarReMat  The representation of the model matrix for a scalar random-effects term  Members:   f : the grouping factor as a  PooledDataVector  z : the raw random-effects model matrix as a  Vector  fnm : the name of the grouping factor as a  Symbol  cnms : a  Vector  of column names   #  MixedModels.VarCorr     Type .  VarCorr  An encapsulation of information on the fitted random-effects variance-covariance matrices.  Members:   \u039b : the vector of lower triangular matrices from the  MixedModel  fnms : a  Vector{ASCIIString}  of grouping factor names  cnms : a  Vector{Vector{ASCIIString}}  of column names  s : the estimate of \u03c3, the standard deviation of the per-observation noise   The main purpose of defining this type is to isolate the logic in the show method.  #  MixedModels.VectorReMat     Type .  VectorReMat  The representation of the model matrix for a vector-valued random-effects term  Members:   f : the grouping factor as a  PooledDataVector  z : the transposed raw random-effects model matrix  fnm : the name of the grouping factor as a  Symbol  cnms : a  Vector  of column names (row names after transposition) of  z   #  MixedModels.bootstrap     Function .  bootstrap(m::LinearMixedModels, N::Integer, saveresults::Function)  Simulate  N  response vectors from  m , refitting the model.   saveresults  is called after each refit with arguments  i::Int , the index, and  m .  To save space  m.trms[end] , which is the response vector, is overwritten by each simulation.  The original response is restored and the model refit before returning.  #  Base.LinAlg.cond     Method .  cond(m::MixedModel)  Returns a vector of the condition numbers of the blocks of  m.\u039b  #  StatsBase.df     Method .  df(obj::StatisticalModel)  Returns the number of degrees of freedom consumed in the model, including when applicable the intercept and the distribution's dispersion parameter.  #  StatsBase.deviance     Method .  deviance(obj::StatisticalModel)  Returns the deviance of the model relative to a reference, which is usually when applicable the saturated model. It is equal,  up to a constant , to  -2 log L , with  L  the likelihood of the model.  #  StatsBase.fit!     Method .  fit !( m : :LinearMixedModel ,   verbose = false ;   optimizer = :LN_BOBYQA )   Optimize the objective of a  LinearMixedModel  using an  NLopt  optimizer.  Named Arguments:   optimizer::Symbol  the name of a derivative-free optimizer from  NLopt  that allows for   box constraints.   #  MixedModels.fixef     Function .  fixef(m::MixedModel)  Returns the estimate of the fixed-effects parameter vector.  #  MixedModels.LaplaceDeviance     Function .  LaplaceDeviance(m::GeneralizedLinearMixedModel)  Laplace approximation to the deviance of a GLMM.  For a distribution that does not have a scale factor this is defined as the squared length of the conditional modes,  u , plus the determinant of  \u039b'Z'Z\u039b + 1 , plus the sum of the squared deviance residuals.  #  MixedModels.lmm     Function .  lmm(m::MixedModel)  Extract the  LinearMixedModel  from a  MixedModel .  If  m  is a  LinearMixedModel  return  m . If  m  is a  GeneralizedLinearMixedModel  return  m.LMM .  lmm ( f : :DataFrames .Formula ,   fr : :DataFrames .DataFrame )  lmm ( f : :DataFrames .Formula ,   fr : :DataFrames .DataFrame ;   weights   =   [] )   Create a  LinearMixedModel  from  f , which contains both fixed-effects terms and random effects, and  fr . The return value is ready to be  fit!  but has not yet been fit.  #  MixedModels.lowerbd     Function .  lowerbd(m)  Args:   m : a  MixedModel   Returns:   A  Vector  of lower bounds on the covariance parameter vector  m[:\u03b8]  lowerbd{T}(A::LowerTriangular{T,Matrix{T}})  lower bounds on the parameters (elements in the lower triangle)  #  StatsBase.model_response     Method .  nothing #  StatsBase.nobs     Method .  nobs(obj::StatisticalModel)  Returns the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.  #  MixedModels.objective     Function .  objective(m::LinearMixedModel)  Negative twice the log-likelihood of model  m  #  MixedModels.pwrss     Function .  pwrss(m::LinearMixedModel)  The penalized residual sum-of-squares.  #  MixedModels.pirls!     Function .  pirls!(m::GeneralizedLinearMixedModel)  Use Penalized Iteratively Reweighted Least Squares (PIRLS) to determine the conditional modes of the random effects.  #  MixedModels.ranef     Function .  ranef(m)\nranef(m, uscale)  Conditional modes of the random effects in model  m  Args:   m : a fitted  MixedModel  object  uscale : a  Bool  indicating conditional modes are on the  u  scale or the  b  scale.  Defaults to  false   Returns:   A  Vector  of matrices of the conditional modes of the random effects on the indicated scale.   For a scalar random-effects term the matrix is  1 \u00d7 k  where  k  is the number of levels of the grouping factor.   For a vector-valued random-effects term the matrix is  l \u00d7 k  where  l  is the dimension of each random effect.  #  MixedModels.refit!     Function .  refit!{T}(m::LinearMixedModel, y::Vector{T})  Refit the model  m  with response  y .  #  MixedModels.remat     Function .   remat(e::Expr, df::DataFrames.DataFrame)  A factory for  ReMat  objects.   e  should be of the form  :(e1 | e2)  where  e1  is a valid rhs of a  Formula  and  pool(e2)  can be evaluated within  df .  The result is a  ScalarReMat  or a  VectorReMat , as appropriate.  #  MixedModels.sdest     Function .  sdest(m::LinearMixedModel)  The estimate of \u03c3, the standard deviation of the per-observation noise.  #  MixedModels.simulate!     Function .  simulate !( m : :LinearMixedModel ;   \u03b2 = fixef ( m ),   \u03c3 = sdest ( m ),   \u03b8 = m [ : \u03b8 ] )   Install a simulated response vector in model  m  and refit it.  #  MixedModels.varest     Function .  varest(m::LinearMixedModel)  The estimate of \u03c3\u00b2, the variance of the conditional distribution of Y given B.  #  StatsBase.vcov     Function .   vcov(m)  Estimated covariance matrix of the fixed-effects estimator  Args:   m : a  LinearMixedModel   Returns   a  p \u00d7 p   Matrix", 
            "title": "MixedModels"
        }, 
        {
            "location": "/lib/internals/", 
            "text": "Internal Documentation\n\n\n\n\nContents\n\n\n\n\nInternal Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nFunctions and methods\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nMixedModels.OptSummary\n\n\nMixedModels.ranef!\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nMixedModels.OptSummary\n \n \nType\n.\n\n\nOptSummary\n\n\n\n\n\nSummary of an \nNLopt\n optimization\n\n\nMembers:\n\n\n\n\ninitial\n: a copy of the initial parameter values in the optimization\n\n\nfinal\n: a copy of the final parameter values from the optimization\n\n\nfmin\n: the final value of the objective\n\n\nfeval\n: the number of function evaluations\n\n\noptimizer\n: the name of the optimizer used, as a \nSymbol\n\n\n\n\n\n\nFunctions and methods\n\n\n#\n\n\nMixedModels.ranef!\n \n \nFunction\n.\n\n\nranef!(v, m, uscale)\n\n\n\n\n\nOverwrite v with the conditional modes of the random effects for \nm\n\n\nArgs:\n\n\n\n\nv\n: a \nVector\n of matrices\n\n\nm\n: a \nMixedModel\n\n\nuscale\n: \nBool\n, return the random-effects on the spherical (i.e. \nu\n) scale?\n\n\n\n\nReturns:   \nv\n, overwritten with the conditional modes", 
            "title": "Internals"
        }, 
        {
            "location": "/lib/internals/#internal-documentation", 
            "text": "", 
            "title": "Internal Documentation"
        }, 
        {
            "location": "/lib/internals/#contents", 
            "text": "Internal Documentation  Contents  Index  Types  Functions and methods", 
            "title": "Contents"
        }, 
        {
            "location": "/lib/internals/#index", 
            "text": "MixedModels.OptSummary  MixedModels.ranef!", 
            "title": "Index"
        }, 
        {
            "location": "/lib/internals/#types", 
            "text": "#  MixedModels.OptSummary     Type .  OptSummary  Summary of an  NLopt  optimization  Members:   initial : a copy of the initial parameter values in the optimization  final : a copy of the final parameter values from the optimization  fmin : the final value of the objective  feval : the number of function evaluations  optimizer : the name of the optimizer used, as a  Symbol", 
            "title": "Types"
        }, 
        {
            "location": "/lib/internals/#functions-and-methods", 
            "text": "#  MixedModels.ranef!     Function .  ranef!(v, m, uscale)  Overwrite v with the conditional modes of the random effects for  m  Args:   v : a  Vector  of matrices  m : a  MixedModel  uscale :  Bool , return the random-effects on the spherical (i.e.  u ) scale?   Returns:    v , overwritten with the conditional modes", 
            "title": "Functions and methods"
        }
    ]
}