---
title: "Gradient-based Optimization of the Profiled log-likelihood"
author:
  - name: Douglas Bates
    email: dmbates@gmail.com
    orcid: 0000-0001-8316-9503
    affiliation:
      - name: University of Wisconsin - Madison
        city: Madison
        state: WI
        url: https://www.wisc.edu
        department: Statistics
  - name: Phillip Alday
    email: me@phillipalday.com
    orcid: 0000-0002-9984-5745
    affiliation:
      - name: Beacon Biosignals
        url: https://beacon.bio
date: last-modified
date-format: iso
toc: true
bibliography: bibliography.bib
number-sections: true
engine: julia
julia:
  exeflags:
    - -tauto
    - --project=@.
format:
  html:
    toc: true
    toc-location: right
    embed-resources: true
---

## Introduction {#sec-intro}

Before devoting too much effort to efficient evaluation of the gradient of the profiled log-likelihood, we should check if using gradient-based optimization requires sufficiently fewer evaluations of the objective, and the gradient, than does derivative-free optimization.

Here we fit a few models using automatic differentiation from [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) and the `ForwardDiff` extension to [MixedModels.jl](https://github.com/JuliaStats/MixedModels.jl) to optimize the profiled log-likelihood with the `LD_LBFGS` optimizer from [NLopt.jl](https://github.com/jump-dev/NLopt.jl), instead of the default `LN_NEWUOA` which does not use gradients.

The results are more-or-less a toss-up when using `ForwardDiff` to evaluate the gradient.
A more efficient evaluation of the gradient, taking advantage of the sparse-blocked structure of the Cholesky factorization to evaluate the profiled log-likelihood, may tip the balance in favor of gradient-based methods.

## Preliminaries {#sec-prelim}

Load the packages to be used

```{julia}
#| label: load_packages
using ForwardDiff
using MixedModels
using MixedModels: fd_deviance
using MixedModelsDatasets: dataset
using NLopt
using Tables: table
using TypedTables: Table

const progress = false
```

## Examples {#sec-examples}

We create a function to take a `LinearMixedModel` that has been fit and refit it using the `:LD_LBFGS` optimizer applied to an objective function that evaluates the gradient using `ForwardDiff`.

```{julia}
addinds(ch::Char, n::Integer) = Symbol.(lpad.(string.(ch, Base.OneTo(n)), ndigits(n), '0'))
function gr_refit!(m::LinearMixedModel{T}) where {T}
  θ = copy(m.optsum.initial)
  k = length(θ)
  fitlog = sizehint!(T[], 50 * k)
  grad_config = ForwardDiff.GradientConfig(fd_deviance(m), θ)
  function obj(θ::Vector{Float64}, grad::Vector{Float64})
    val = objective(updateL!(setθ!(m, θ)))
    push!(fitlog, val)
    append!(fitlog, θ)
    if !isempty(grad)
      ForwardDiff.gradient!(grad, m, θ, grad_config)
      append!(fitlog, grad)
    else
      append!(fitlog, fill(NaN, k))  # never called with empty grad but just in case
    end
    return val
  end
  opt = NLopt.Opt(:LD_LBFGS, k)
  NLopt.ftol_rel!(opt, 1.e-12)
  NLopt.ftol_abs!(opt, 1.e-8)
  NLopt.min_objective!(opt, obj)
  min_f, min_x, ret = NLopt.optimize(opt, θ)
  header = vcat([:obj], addinds('θ', k), addinds('g', k))
  return Table(table(transpose(reshape(fitlog, 2k + 1, :)); header))
end
```

### Penicillin data {#sec-penicillin}

Define a model for the `penicillin` data

```{julia}
#| label: m1
m1 = fit(
  MixedModel,
  @formula(diameter ~ 1 + (1|plate) + (1|sample)),
  dataset(:penicillin);
  progress,
)
print(m1)
```

for which the optimization summary is

```{julia}
m1.optsum
```

and refit the model using ForwardDiff gradient evaluations.

```{julia}
fitlog = gr_refit!(m1)
```

The objective at convergence is

```{julia}
last(fitlog.obj)
```

and the last few evaluations are

```{julia}
last(fitlog, 5)
```

### Sleepstudy {#sec-sleepstudy}

```{julia}
m2 = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days|subj)), dataset(:sleepstudy); progress)
print(m2)
```

```{julia}
m2.optsum
```

```{julia}
fitlog = gr_refit!(m2)
```

```{julia}
last(fitlog.obj)
```

```{julia}
last(fitlog, 5)
```

### Kronmueller-Barr 2007 {#sec-kb07}

```{julia}
# this model is very overparameterized, but it's a test example
m3 = fit(
  MixedModel,
  @formula(rt_trunc ~ 1 + spkr * prec * load + (1 + spkr * prec * load | subj) + (1 + spkr * prec * load | item)),
  dataset(:kb07);
  progress,
)
print(m3)
```

```{julia}
m3.optsum
```

Several of the parameters on the diagonal of $\boldsymbol{\Lambda}$ are close to zero at convergence and are replaced by zero in the returned parameter vector

```{julia}
findall(iszero, m3.θ)
```

```{julia}
fitlog = gr_refit!(m3)
```

```{julia}
last(fitlog.obj)
```

```{julia}
last(fitlog, 5)
```