<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Details of the parameter estimation · MixedModels</title><meta name="title" content="Details of the parameter estimation · MixedModels"/><meta property="og:title" content="Details of the parameter estimation · MixedModels"/><meta property="twitter:title" content="Details of the parameter estimation · MixedModels"/><meta name="description" content="Documentation for MixedModels."/><meta property="og:description" content="Documentation for MixedModels."/><meta property="twitter:description" content="Documentation for MixedModels."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="MixedModels logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MixedModels</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">MixedModels.jl Documentation</a></li><li><a class="tocitem" href="../constructors/">Model constructors</a></li><li class="is-active"><a class="tocitem" href>Details of the parameter estimation</a><ul class="internal"><li><a class="tocitem" href="#The-probability-model"><span>The probability model</span></a></li><li><a class="tocitem" href="#Linear-Mixed-Effects-Models"><span>Linear Mixed-Effects Models</span></a></li><li><a class="tocitem" href="#Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z"><span>Internal structure of <span>$\Lambda_\theta$</span> and <span>$\bf Z$</span></span></a></li><li><a class="tocitem" href="#A-blocked-Cholesky-factor"><span>A blocked Cholesky factor</span></a></li><li><a class="tocitem" href="#Modifying-the-optimization-process"><span>Modifying the optimization process</span></a></li><li><a class="tocitem" href="#Generalized-Linear-Mixed-Effects-Models"><span>Generalized Linear Mixed-Effects Models</span></a></li></ul></li><li><a class="tocitem" href="../GaussHermite/">Normalized Gauss-Hermite Quadrature</a></li><li><a class="tocitem" href="../prediction/">Prediction and simulation in Mixed-Effects Models</a></li><li><a class="tocitem" href="../bootstrap/">Parametric bootstrap for mixed-effects models</a></li><li><a class="tocitem" href="../rankdeficiency/">Rank deficiency in mixed-effects models</a></li><li><a class="tocitem" href="../mime/">Alternative display and output formats</a></li><li><a class="tocitem" href="../derivatives/">Gradient and Hessian computation</a></li><li><a class="tocitem" href="../formula_syntax/">Formula syntax</a></li><li><a class="tocitem" href="../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Details of the parameter estimation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Details of the parameter estimation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaStats/MixedModels.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaStats/MixedModels.jl/blob/main/docs/src/optimization.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Details-of-the-parameter-estimation"><a class="docs-heading-anchor" href="#Details-of-the-parameter-estimation">Details of the parameter estimation</a><a id="Details-of-the-parameter-estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Details-of-the-parameter-estimation" title="Permalink"></a></h1><h2 id="The-probability-model"><a class="docs-heading-anchor" href="#The-probability-model">The probability model</a><a id="The-probability-model-1"></a><a class="docs-heading-anchor-permalink" href="#The-probability-model" title="Permalink"></a></h2><p>Maximum likelihood estimates are based on the probability model for the observed responses. In the probability model the distribution of the responses is expressed as a function of one or more <em>parameters</em>.</p><p>For a continuous distribution the probability density is a function of the responses, given the parameters. The <em>likelihood</em> function is the same expression as the probability density but regarding the observed values as fixed and the parameters as varying.</p><p>In general a mixed-effects model incorporates two random variables: <span>$\mathcal{B}$</span>, the <span>$q$</span>-dimensional vector of random effects, and <span>$\mathcal{Y}$</span>, the <span>$n$</span>-dimensional response vector. The value, <span>$\bf y$</span>, of <span>$\mathcal{Y}$</span> is observed; the value, <span>$\bf b$</span>, of <span>$\mathcal{B}$</span> is not.</p><h2 id="Linear-Mixed-Effects-Models"><a class="docs-heading-anchor" href="#Linear-Mixed-Effects-Models">Linear Mixed-Effects Models</a><a id="Linear-Mixed-Effects-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Mixed-Effects-Models" title="Permalink"></a></h2><p>In a linear mixed model the unconditional distribution of <span>$\mathcal{B}$</span> and the conditional distribution, <span>$(\mathcal{Y} | \mathcal{B}=\bf{b})$</span>, are both multivariate Gaussian distributions,</p><p class="math-container">\[\begin{aligned}
  (\mathcal{Y} | \mathcal{B}=\bf{b}) &amp;\sim\mathcal{N}(\bf{ X\beta + Z b},\sigma^2\bf{I})\\\\
  \mathcal{B}&amp;\sim\mathcal{N}(\bf{0},\Sigma_\theta) .
\end{aligned}\]</p><p>The <em>conditional mean</em> of <span>$\mathcal Y$</span>, given <span>$\mathcal B=\bf b$</span>, is the <em>linear predictor</em>, <span>$\bf X\bf\beta+\bf Z\bf b$</span>, which depends on the <span>$p$</span>-dimensional <em>fixed-effects parameter</em>, <span>$\bf \beta$</span>, and on <span>$\bf b$</span>. The <em>model matrices</em>, <span>$\bf X$</span> and <span>$\bf Z$</span>, of dimension <span>$n\times p$</span> and <span>$n\times q$</span>, respectively, are determined from the formula for the model and the values of covariates. Although the matrix <span>$\bf Z$</span> can be large (i.e. both <span>$n$</span> and <span>$q$</span> can be large), it is sparse (i.e. most of the elements in the matrix are zero).</p><p>The <em>relative covariance factor</em>, <span>$\Lambda_\theta$</span>, is a <span>$q\times q$</span> lower-triangular matrix, depending on the <em>variance-component parameter</em>, <span>$\bf\theta$</span>, and generating the symmetric <span>$q\times q$</span> variance-covariance matrix, <span>$\Sigma_\theta$</span>, as</p><p class="math-container">\[\Sigma_\theta=\sigma^2\Lambda_\theta\Lambda_\theta&#39;\]</p><p>The <em>spherical random effects</em>, <span>$\mathcal{U}\sim\mathcal{N}(\bf{0},\sigma^2\bf{I}_q)$</span>, determine <span>$\mathcal B$</span> according to</p><p class="math-container">\[\mathcal{B}=\Lambda_\theta\mathcal{U}.\]</p><p>The <em>penalized residual sum of squares</em> (PRSS),</p><p class="math-container">\[r^2(\theta,\beta,\bf{u})=\|\bf{y} - \bf{X}\beta -\bf{Z}\Lambda_\theta\bf{u}\|^2+\|\bf{u}\|^2,\]</p><p>is the sum of the residual sum of squares, measuring fidelity of the model to the data, and a penalty on the size of <span>$\bf u$</span>, measuring the complexity of the model. Minimizing <span>$r^2$</span> with respect to <span>$\bf u$</span>,</p><p class="math-container">\[r^2_{\beta,\theta} =\min_{\bf{u}}\left(\|\bf{y} -\bf{X}{\beta} -\bf{Z}\Lambda_\theta\bf{u}\|^2+\|\bf{u}\|^2\right)\]</p><p>is a direct (i.e. non-iterative) computation. The particular method used to solve this generates a <em>blocked Choleksy factor</em>, <span>$\bf{L}_\theta$</span>, which is an lower triangular <span>$q\times q$</span> matrix satisfying</p><p class="math-container">\[\bf{L}_\theta\bf{L}_\theta&#39;=\Lambda_\theta&#39;\bf{Z}&#39;\bf{Z}\Lambda_\theta+\bf{I}_q .\]</p><p>where <span>${\bf I}_q$</span> is the <span>$q\times q$</span> <em>identity matrix</em>.</p><p>Negative twice the log-likelihood of the parameters, given the data, <span>$\bf y$</span>, is</p><p class="math-container">\[d({\bf\theta},{\bf\beta},\sigma|{\bf y})
=n\log(2\pi\sigma^2)+\log(|{\bf L}_\theta|^2)+\frac{r^2_{\beta,\theta}}{\sigma^2}.\]</p><p>where <span>$|{\bf L}_\theta|$</span> denotes the <em>determinant</em> of <span>${\bf L}_\theta$</span>. Because <span>${\bf L}_\theta$</span> is triangular, its determinant is the product of its diagonal elements.</p><p>Because the conditional mean, <span>$\bf\mu_{\mathcal Y|\mathcal B=\bf b}=\bf X\bf\beta+\bf Z\Lambda_\theta\bf u$</span>, is a linear function of both <span>$\bf\beta$</span> and <span>$\bf u$</span>, minimization of the PRSS with respect to both <span>$\bf\beta$</span> and <span>$\bf u$</span> to produce</p><p class="math-container">\[r^2_\theta =\min_{{\bf\beta},{\bf u}}\left(\|{\bf y} -{\bf X}{\bf\beta} -{\bf Z}\Lambda_\theta{\bf u}\|^2+\|{\bf u}\|^2\right)\]</p><p>is also a direct calculation. The values of <span>$\bf u$</span> and <span>$\bf\beta$</span> that provide this minimum are called, respectively, the <em>conditional mode</em>, <span>$\tilde{\bf u}_\theta$</span>, of the spherical random effects and the conditional estimate, <span>$\widehat{\bf\beta}_\theta$</span>, of the fixed effects. At the conditional estimate of the fixed effects the objective is</p><p class="math-container">\[d({\bf\theta},\widehat{\beta}_\theta,\sigma|{\bf y})
=n\log(2\pi\sigma^2)+\log(|{\bf L}_\theta|^2)+\frac{r^2_\theta}{\sigma^2}.\]</p><p>Minimizing this expression with respect to <span>$\sigma^2$</span> produces the conditional estimate</p><p class="math-container">\[\widehat{\sigma^2}_\theta=\frac{r^2_\theta}{n}\]</p><p>which provides the <em>profiled log-likelihood</em> on the deviance scale as</p><p class="math-container">\[\tilde{d}(\theta|{\bf y})=d(\theta,\widehat{\beta}_\theta,\widehat{\sigma}_\theta|{\bf y})
=\log(|{\bf L}_\theta|^2)+n\left[1+\log\left(\frac{2\pi r^2_\theta}{n}\right)\right],\]</p><p>a function of <span>$\bf\theta$</span> alone.</p><p>The MLE of <span>$\bf\theta$</span>, written <span>$\widehat{\bf\theta}$</span>, is the value that minimizes this profiled objective. We determine this value by numerical optimization. In the process of evaluating <span>$\tilde{d}(\widehat{\theta}|{\bf y})$</span> we determine <span>$\widehat{\beta}=\widehat{\beta}_{\widehat\theta}$</span>, <span>$\tilde{\bf u}_{\widehat{\theta}}$</span> and <span>$r^2_{\widehat{\theta}}$</span>, from which we can evaluate <span>$\widehat{\sigma}=\sqrt{r^2_{\widehat{\theta}}/n}$</span>.</p><p>The elements of the conditional mode of <span>$\mathcal B$</span>, evaluated at the parameter estimates,</p><p class="math-container">\[\tilde{\bf b}_{\widehat{\theta}}=\Lambda_{\widehat{\theta}}\tilde{\bf u}_{\widehat{\theta}}\]</p><p>are sometimes called the <em>best linear unbiased predictors</em> or BLUPs of the random effects. Although BLUPs an appealing acronym, I don’t find the term particularly instructive (what is a “linear unbiased predictor” and in what sense are these the “best”?) and prefer the term “conditional modes”, because these are the values of <span>$\bf b$</span> that maximize the density of the conditional distribution <span>$\mathcal{B} | \mathcal{Y} = {\bf y}$</span>. For a linear mixed model, where all the conditional and unconditional distributions are Gaussian, these values are also the <em>conditional means</em>.</p><h2 id="Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z"><a class="docs-heading-anchor" href="#Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z">Internal structure of <span>$\Lambda_\theta$</span> and <span>$\bf Z$</span></a><a id="Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z-1"></a><a class="docs-heading-anchor-permalink" href="#Internal-structure-of-\\Lambda_\\theta-and-\\bf-Z" title="Permalink"></a></h2><p>In the types of <code>LinearMixedModel</code> available through the <code>MixedModels</code> package, groups of random effects and the corresponding columns of the model matrix, <span>$\bf Z$</span>, are associated with <em>random-effects terms</em> in the model formula.</p><p>For the simple example</p><pre><code class="language-julia hljs">using BenchmarkTools, DataFrames, MixedModels, MixedModelsDatasets</code></pre><pre><code class="language-julia hljs">dyestuff = MixedModelsDatasets.dataset(:dyestuff)
fm1 = fit(MixedModel, @formula(yield ~ 1 + (1|batch)), dyestuff)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Linear mixed model fit by maximum likelihood
 yield ~ 1 + (1 | batch)
   logLik   -2 logLik     AIC       AICc        BIC    
  -163.6635   327.3271   333.3271   334.2501   337.5307

Variance components:
            Column    Variance Std.Dev.
batch    (Intercept)  1388.3332 37.2603
Residual              2451.2500 49.5101
 Number of obs: 30; levels of grouping factors: 6

  Fixed-effects parameters:
────────────────────────────────────────────────
              Coef.  Std. Error      z  Pr(&gt;|z|)
────────────────────────────────────────────────
(Intercept)  1527.5     17.6946  86.33    &lt;1e-99
────────────────────────────────────────────────</code></pre><p>the only random effects term in the formula is <code>(1|batch)</code>, a simple, scalar random-effects term.</p><pre><code class="language-julia hljs">t1 = only(fm1.reterms);
Int.(t1)  # convert to integers for more compact display</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">30×6 Matrix{Int64}:
 1  0  0  0  0  0
 1  0  0  0  0  0
 1  0  0  0  0  0
 1  0  0  0  0  0
 1  0  0  0  0  0
 0  1  0  0  0  0
 0  1  0  0  0  0
 0  1  0  0  0  0
 0  1  0  0  0  0
 0  1  0  0  0  0
 ⋮              ⋮
 0  0  0  0  1  0
 0  0  0  0  1  0
 0  0  0  0  1  0
 0  0  0  0  1  0
 0  0  0  0  0  1
 0  0  0  0  0  1
 0  0  0  0  0  1
 0  0  0  0  0  1
 0  0  0  0  0  1</code></pre><p>The matrix <code>t1</code> is a sparse matrix, meaning that most of the elements are zero, and its transpose is stored in a sparse form.</p><pre><code class="language-julia hljs">sparse(t1)&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6×30 SparseArrays.SparseMatrixCSC{Float64, Int32} with 30 stored entries:
⎡⠉⠉⠑⠒⠒⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⎤
⎣⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠤⠤⢄⣀⣀⎦</code></pre><p>provides a compact representation of the positions of the non-zeros in this matrix.</p><p>This <code>RandomEffectsTerm</code> contributes a block of columns to the model matrix <span>$\bf Z$</span> and a diagonal block to <span>$\Lambda_\theta$</span>. In this case the diagonal block of <span>$\Lambda_\theta$</span> (which is also the only block) is a multiple of the <span>$6\times6$</span> identity matrix where the multiple is</p><pre><code class="language-julia hljs">t1.λ</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×1 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:
 0.7525806540074477</code></pre><p>Because there is only one random-effects term in the model, the matrix <span>$\bf Z$</span> is the indicators matrix shown as the result of <code>Int.(t1)</code>, but stored in a special sparse format. Furthermore, there is only one block in <span>$\Lambda_\theta$</span>.</p><p>For a vector-valued random-effects term, as in</p><pre><code class="language-julia hljs">sleepstudy = MixedModelsDatasets.dataset(:sleepstudy)
fm2 = fit(MixedModel, @formula(reaction ~ 1+days+(1+days|subj)), sleepstudy)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Linear mixed model fit by maximum likelihood
 reaction ~ 1 + days + (1 + days | subj)
   logLik   -2 logLik     AIC       AICc        BIC    
  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971

Variance components:
            Column    Variance Std.Dev.   Corr.
subj     (Intercept)  565.52071 23.78068
         days          32.68242  5.71685 +0.08
Residual              654.94015 25.59180
 Number of obs: 180; levels of grouping factors: 18

  Fixed-effects parameters:
──────────────────────────────────────────────────
                Coef.  Std. Error      z  Pr(&gt;|z|)
──────────────────────────────────────────────────
(Intercept)  251.405      6.6323   37.91    &lt;1e-99
days          10.4673     1.50224   6.97    &lt;1e-11
──────────────────────────────────────────────────</code></pre><p>the model matrix <span>$\bf Z$</span> is of the form</p><pre><code class="language-julia hljs">t21 = only(fm2.reterms);
sparse(t21)&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">36×180 SparseArrays.SparseMatrixCSC{Float64, Int32} with 360 stored entries:
⎡⠉⠉⠓⠒⠢⠤⢤⣀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠛⠛⠷⠶⠦⠤⢤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠓⠒⠲⠶⢶⣤⣤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠉⠓⠒⠢⠤⢤⣀⣀⎦</code></pre><p>and <span>$\Lambda_\theta$</span> is a <span>$36\times36$</span> block diagonal matrix with <span>$18$</span> diagonal blocks, all of the form</p><pre><code class="language-julia hljs">t21.λ</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:
 0.92923     ⋅ 
 0.0181644  0.222646</code></pre><p>The <span>$\theta$</span> vector is</p><pre><code class="language-julia hljs">MixedModels.getθ(t21)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
 0.9292304864356652
 0.018164371001158423
 0.22264644003494607</code></pre><p>Random-effects terms in the model formula that have the same grouping factor are amalgamated into a single <code>ReMat</code> object.</p><pre><code class="language-julia hljs">fm3 = fit(MixedModel, @formula(reaction ~ 1+days+(1|subj) + (0+days|subj)), sleepstudy)
t31 = only(fm3.reterms);
sparse(t31)&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">36×180 SparseArrays.SparseMatrixCSC{Float64, Int32} with 360 stored entries:
⎡⠉⠉⠓⠒⠢⠤⢤⣀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠛⠛⠷⠶⠦⠤⢤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠓⠒⠲⠶⢶⣤⣤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠉⠓⠒⠢⠤⢤⣀⣀⎦</code></pre><p>For this model the matrix <span>$\bf Z$</span> is the same as that of model <code>fm2</code> but the diagonal blocks of <span>$\Lambda_\theta$</span> are themselves diagonal.</p><pre><code class="language-julia hljs">t31.λ</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 LinearAlgebra.Diagonal{Float64, Vector{Float64}}:
 0.945815   ⋅ 
  ⋅        0.226928</code></pre><pre><code class="language-julia hljs">MixedModels.getθ(t31)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.9458146476504666
 0.2269279731994651</code></pre><p>Random-effects terms with distinct grouping factors generate distinct elements of the <code>reterms</code> field of the <code>LinearMixedModel</code> object. Multiple <code>ReMat</code> objects are sorted by decreasing numbers of random effects.</p><pre><code class="language-julia hljs">penicillin = MixedModelsDatasets.dataset(:penicillin)
fm4 = fit(MixedModel,
    @formula(diameter ~ 1 + (1|sample) + (1|plate)),
    penicillin)
sparse(first(fm4.reterms))&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">24×144 SparseArrays.SparseMatrixCSC{Float64, Int32} with 144 stored entries:
⎡⠉⠙⠒⠒⠒⠤⠤⠤⢄⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠒⠒⠒⠦⠤⠤⢤⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠉⠓⠒⠒⠢⠤⠤⠤⣀⣀⣀⡀⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⎦</code></pre><pre><code class="language-julia hljs">sparse(last(fm4.reterms))&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6×144 SparseArrays.SparseMatrixCSC{Float64, Int32} with 144 stored entries:
⎡⢑⠈⡂⠘⡀⢃⠈⡂⠘⡀⢃⠈⡂⠘⡀⢃⠈⡂⠑⡀⢃⠈⡂⠑⡀⢃⠈⡂⢑⠀⢃⠈⡂⢑⠀⢃⠘⡀⢑⠀⎤
⎣⠀⢅⠈⡄⢡⠀⢅⠨⡀⢡⠀⢅⠨⡀⢡⠈⢄⠨⡀⢡⠈⢄⠨⡀⢡⠈⡄⠨⡀⢡⠈⡄⠨⡀⢡⠈⡄⠨⡀⢅⎦</code></pre><p>Note that the first <code>ReMat</code> in <code>fm4.reterms</code> corresponds to grouping factor <code>plate</code> even though the term <code>(1|plate)</code> occurs in the formula after <code>(1|sample)</code>.</p><h3 id="Progress-of-the-optimization"><a class="docs-heading-anchor" href="#Progress-of-the-optimization">Progress of the optimization</a><a id="Progress-of-the-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Progress-of-the-optimization" title="Permalink"></a></h3><p>By default a progress display is shown when fitting a model that takes a second or more to fit. (The optional named argument, <code>progress=false</code>, can be used to suppress this display.) The number of iterations performed, the average time per iteration and the current value of the objective are shown in this display.</p><p>After the model has been fit, a summary of the optimization process is available as the <code>optsum</code> property of the <code>LinearMixedModel</code>.</p><pre><code class="language-julia hljs">fm2.optsum</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Initial parameter vector: [1.0, 0.0, 1.0]
Initial objective value:  1784.642296192471

Backend:                  nlopt
Optimizer:                LN_NEWUOA
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]
initial_step:             [1.0, 1.0, 1.0]
maxfeval:                 -1
maxtime:                  -1.0

Function evaluations:     72
xtol_zero_abs:            0.001
ftol_zero_abs:            1.0e-5
Final parameter vector:   [0.9292304864356652, 0.018164371001158423, 0.22264644003494607]
Final objective value:    1751.9393444640505
Return code:              FTOL_REACHED
</code></pre><p>More detailed information about the intermediate steps of the nonlinear optimizer can be obtained the <code>fitlog</code> field.</p><pre><code class="language-julia hljs">first(fm2.optsum.fitlog, 5)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Table with 2 columns and 5 rows:
     θ                objective
   ┌───────────────────────────
 1 │ [1.0, 0.0, 1.0]  1784.64
 2 │ [2.0, 0.0, 1.0]  1792.86
 3 │ [1.0, 1.0, 1.0]  1799.0
 4 │ [1.0, 0.0, 2.0]  1808.55
 5 │ [0.0, 0.0, 1.0]  1806.24</code></pre><h2 id="A-blocked-Cholesky-factor"><a class="docs-heading-anchor" href="#A-blocked-Cholesky-factor">A blocked Cholesky factor</a><a id="A-blocked-Cholesky-factor-1"></a><a class="docs-heading-anchor-permalink" href="#A-blocked-Cholesky-factor" title="Permalink"></a></h2><p>A <code>LinearMixedModel</code> object contains two blocked matrices; a symmetric matrix <code>A</code> (only the lower triangle is stored) and a lower-triangular <code>L</code> which is the lower Cholesky factor of the updated and inflated <code>A</code>. In versions 4.0.0 and later of <code>MixedModels</code> only the blocks in the lower triangle are stored in <code>A</code> and <code>L</code>, as a <code>Vector{AbstractMatrix{T}}</code>.</p><p><code>BlockDescription</code> shows the structure of the blocks</p><pre><code class="language-julia hljs">BlockDescription(fm2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">rows:     subj         fixed     
  36:   BlkDiag    
   3:    Dense         Dense     
</code></pre><p>Another change in v4.0.0 and later is that the last row of blocks is constructed from <code>m.Xymat</code> which contains the full-rank model matrix <code>X</code> with the response <code>y</code> concatenated on the right.</p><p>The operation of installing a new value of the variance parameters, <code>θ</code>, and updating <code>L</code></p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MixedModels.setθ!" href="#MixedModels.setθ!"><code>MixedModels.setθ!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">setθ!(m::LinearMixedModel, v)</code></pre><p>Install <code>v</code> as the θ parameters in <code>m</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MixedModels.jl/blob/ec7b98695ce959ef21b980bf4e3042df646a4c9b/src/linearmixedmodel.jl#L1043-L1047">source</a></section><section><div><pre><code class="language-julia hljs">setθ!(bsamp::MixedModelFitCollection, θ::AbstractVector)
setθ!(bsamp::MixedModelFitCollection, i::Integer)</code></pre><p>Install the values of the i&#39;th θ value of <code>bsamp.fits</code> in <code>bsamp.λ</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MixedModels.jl/blob/ec7b98695ce959ef21b980bf4e3042df646a4c9b/src/bootstrap.jl#L438-L443">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MixedModels.updateL!" href="#MixedModels.updateL!"><code>MixedModels.updateL!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">updateL!(m::LinearMixedModel)</code></pre><p>Update the blocked lower Cholesky factor, <code>m.L</code>, from <code>m.A</code> and <code>m.reterms</code> (used for λ only)</p><p>This is the crucial step in evaluating the objective, given a new parameter value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MixedModels.jl/blob/ec7b98695ce959ef21b980bf4e3042df646a4c9b/src/linearmixedmodel.jl#L1299-L1305">source</a></section></article><p>is the central step in evaluating the objective (negative twice the log-likelihood).</p><p>Typically, the (1,1) block is the largest block in <code>A</code> and <code>L</code> and it has a special form, either <code>Diagonal</code> or <code>UniformBlockDiagonal</code> providing a compact representation and fast matrix multiplication or solutions of linear systems of equations.</p><h2 id="Modifying-the-optimization-process"><a class="docs-heading-anchor" href="#Modifying-the-optimization-process">Modifying the optimization process</a><a id="Modifying-the-optimization-process-1"></a><a class="docs-heading-anchor-permalink" href="#Modifying-the-optimization-process" title="Permalink"></a></h2><p>The <a href="../api/#MixedModels.OptSummary"><code>OptSummary</code></a> object contains both input and output fields for the optimizer. To modify the optimization process the input fields can be changed after constructing the model but before fitting it. In addition to various tolerances, which we will not discuss further here, users can specify the choice of <code>backend</code> (i.e., the non-linear optimization library used) and <code>optimizer</code> (i.e., the implementation of an algorithm provided by the backend).</p><p>The current default backend is <a href="https://github.com/JuliaOpt/NLopt.jl">NLopt</a>, which is a direct dependency of MixedModels.jl. A <a href="https://github.com/libprima/PRIMA.jl/">PRIMA</a> backend is also provided as a package extension and thus only available when the PRIMA package is loaded. The list of currently loaded backends is available as <a href="../api/#MixedModels.OPTIMIZATION_BACKENDS"><code>MixedModels.OPTIMIZATION_BACKENDS</code></a>. For each individual backend, the list of available optimizers can be inspected with the function <a href="../api/#MixedModels.optimizers"><code>MixedModels.optimizers</code></a>.</p><pre><code class="language-julia hljs">MixedModels.optimizers(:nlopt)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Symbol}:
 :LN_NEWUOA
 :LN_BOBYQA
 :LN_COBYLA
 :LN_NELDERMEAD
 :LN_PRAXIS</code></pre><p>Similarly, the list of applicable optimization parameters can be inspected with the function <a href="../api/#MixedModels.opt_params"><code>MixedModels.opt_params</code></a>.</p><pre><code class="language-julia hljs">MixedModels.opt_params(:nlopt)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">7-element Vector{Symbol}:
 :ftol_rel
 :ftol_abs
 :xtol_rel
 :xtol_abs
 :initial_step
 :maxfeval
 :maxtime</code></pre><div class="admonition is-info" id="Optimizer-defaults-subject-to-change-f63ac1caf4f281d7"><header class="admonition-header">Optimizer defaults subject to change<a class="admonition-anchor" href="#Optimizer-defaults-subject-to-change-f63ac1caf4f281d7" title="Permalink"></a></header><div class="admonition-body"><p>The choice of default backend and optimizer is subject to change without being considered a breaking change. If you want to guarantee a particular backend and optimizer, then you should explicitly load the associated backend&#39;s package (e.g. NLopt or PRIMA) and manually set the <code>optimizer</code> and <code>backend</code> fields.</p></div></div><p>Suppose, for example, that the user wishes to try a <a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method">Nelder-Mead</a> optimization method instead of the default <a href="https://en.wikipedia.org/wiki/BOBYQA"><code>BOBYQA</code></a> (Bounded Optimization BY Quadratic Approximation) method.</p><pre><code class="language-julia hljs">fm2nm = LinearMixedModel(@formula(reaction ~ 1+days+(1+days|subj)), sleepstudy);
fm2nm.optsum.optimizer = :LN_NELDERMEAD;
fit!(fm2nm)
fm2nm.optsum</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Initial parameter vector: [1.0, 0.0, 1.0]
Initial objective value:  1784.642296192471

Backend:                  nlopt
Optimizer:                LN_NELDERMEAD
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10, 1.0e-10]
initial_step:             [1.0, 1.0, 1.0]
maxfeval:                 -1
maxtime:                  -1.0

Function evaluations:     141
xtol_zero_abs:            0.001
ftol_zero_abs:            1.0e-5
Final parameter vector:   [0.9292380588736397, 0.018166001665650102, 0.2226463527556629]
Final objective value:    1751.9393444668437
Return code:              FTOL_REACHED
</code></pre><p>The parameter estimates are quite similar to those using <code>:LN_BOBYQA</code> but at the expense of 140 functions evaluations for <code>:LN_NELDERMEAD</code> versus 57 for <code>:LN_BOBYQA</code>. When plotting the progress of the individual fits, it becomes obvious that <code>:LN_BOBYQA</code> has fully converged by the time <code>:LN_NELDERMEAD</code> begins to approach the optimum.</p><pre><code class="language-julia hljs">using Gadfly
nm = fm2nm.optsum.fitlog
bob = fm2.optsum.fitlog
convdf = DataFrame(algorithm=[repeat([&quot;NelderMead&quot;], length(nm));
                           repeat([&quot;BOBYQA&quot;], length(bob))],
                   objective=[last.(nm); last.(bob)],
                   step=[1:length(nm); 1:length(bob)])
plot(convdf, x=:step, y=:objective, color=:algorithm, Geom.line)</code></pre><img src="eedfc387.svg" alt="Example block output"/><p>Run time can be constrained with  <code>maxfeval</code> and <code>maxtime</code>.</p><h3 id="Convergence-to-singular-covariance-matrices"><a class="docs-heading-anchor" href="#Convergence-to-singular-covariance-matrices">Convergence to singular covariance matrices</a><a id="Convergence-to-singular-covariance-matrices-1"></a><a class="docs-heading-anchor-permalink" href="#Convergence-to-singular-covariance-matrices" title="Permalink"></a></h3><p>To ensure identifiability of <span>$\Sigma_\theta=\sigma^2\Lambda_\theta \Lambda_\theta$</span>, the elements of <span>$\theta$</span> corresponding to diagonal elements of <span>$\Lambda_\theta$</span> are constrained to be non-negative. For example, in a trivial case of a single, simple, scalar, random-effects term as in <code>fm1</code>, the one-dimensional <span>$\theta$</span> vector is the ratio of the standard deviation of the random effects to the standard deviation of the response. It happens that <span>$-\theta$</span> produces the same log-likelihood but, by convention, we define the standard deviation to be the positive square root of the variance. Requiring the diagonal elements of <span>$\Lambda_\theta$</span> to be non-negative is a generalization of using this positive square root.</p><p>If the optimization converges on the boundary of the feasible region, that is if one or more of the diagonal elements of <span>$\Lambda_\theta$</span> is zero at convergence, the covariance matrix <span>$\Sigma_\theta$</span> will be <em>singular</em>. This means that there will be linear combinations of random effects that are constant. Usually convergence to a singular covariance matrix is a sign of an over-specified model.</p><p>Singularity can be checked with the <code>issingular</code> predicate function.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MixedModels.issingular" href="#MixedModels.issingular"><code>MixedModels.issingular</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">issingular(m::MixedModel, θ=m.θ; atol::Real=0, rtol::Real=atol&gt;0 ? 0 : √eps)</code></pre><p>Test whether the model <code>m</code> is singular if the parameter vector is <code>θ</code>.</p><p>Equality comparisons are used b/c small non-negative θ values are replaced by 0 in <code>fit!</code>.</p><div class="admonition is-info" id="Note-37706c852132b75e"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-37706c852132b75e" title="Permalink"></a></header><div class="admonition-body"><p>For <code>GeneralizedLinearMixedModel</code>, the entire parameter vector (including β in the case <code>fast=false</code>) must be specified if the default is not used.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MixedModels.jl/blob/ec7b98695ce959ef21b980bf4e3042df646a4c9b/src/mixedmodel.jl#L60-L70">source</a></section><section><div><pre><code class="language-julia hljs">issingular(bsamp::MixedModelFitCollection;
           atol::Real=0, rtol::Real=atol&gt;0 ? 0 : √eps))</code></pre><p>Test each bootstrap sample for singularity of the corresponding fit.</p><p>Equality comparisons are used b/c small non-negative θ values are replaced by 0 in <code>fit!</code>.</p><p>See also <a href="../api/#MixedModels.issingular-Tuple{MixedModels.MixedModelFitCollection}"><code>issingular(::MixedModel)</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/MixedModels.jl/blob/ec7b98695ce959ef21b980bf4e3042df646a4c9b/src/bootstrap.jl#L399-L408">source</a></section></article><pre><code class="language-julia hljs">issingular(fm2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">false</code></pre><h2 id="Generalized-Linear-Mixed-Effects-Models"><a class="docs-heading-anchor" href="#Generalized-Linear-Mixed-Effects-Models">Generalized Linear Mixed-Effects Models</a><a id="Generalized-Linear-Mixed-Effects-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-Linear-Mixed-Effects-Models" title="Permalink"></a></h2><p>In a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model"><em>generalized linear model</em></a> the responses are modelled as coming from a particular distribution, such as <code>Bernoulli</code> for binary responses or <code>Poisson</code> for responses that represent counts. The scalar distributions of individual responses differ only in their means, which are determined by a <em>linear predictor</em> expression <span>$\eta=\bf X\beta$</span>, where, as before, <span>$\bf X$</span> is a model matrix derived from the values of covariates and <span>$\beta$</span> is a vector of coefficients.</p><p>The unconstrained components of <span>$\eta$</span> are mapped to the, possibly constrained, components of the mean response, <span>$\mu$</span>, via a scalar function, <span>$g^{-1}$</span>, applied to each component of <span>$\eta$</span>. For historical reasons, the inverse of this function, taking components of <span>$\mu$</span> to the corresponding component of <span>$\eta$</span> is called the <em>link function</em> and the more frequently used map from <span>$\eta$</span> to <span>$\mu$</span> is the <em>inverse link</em>.</p><p>A <em>generalized linear mixed-effects model</em> (GLMM) is defined, for the purposes of this package, by</p><p class="math-container">\[\begin{aligned}
  (\mathcal{Y} | \mathcal{B}=\bf{b}) &amp;\sim\mathcal{D}(\bf{g^{-1}(X\beta + Z b)},\phi)\\\\
  \mathcal{B}&amp;\sim\mathcal{N}(\bf{0},\Sigma_\theta) .
\end{aligned}\]</p><p>where <span>$\mathcal{D}$</span> indicates the distribution family parameterized by the mean and, when needed, a common scale parameter, <span>$\phi$</span>. (There is no scale parameter for <code>Bernoulli</code> or for <code>Poisson</code>. Specifying the mean completely determines the distribution.)</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Distributions.Bernoulli" href="#Distributions.Bernoulli"><code>Distributions.Bernoulli</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Bernoulli(p)</code></pre><p>A <em>Bernoulli distribution</em> is parameterized by a success rate <code>p</code>, which takes value 1 with probability <code>p</code> and 0 with probability <code>1-p</code>.</p><p class="math-container">\[P(X = k) = \begin{cases}
1 - p &amp; \quad \text{for } k = 0, \\
p &amp; \quad \text{for } k = 1.
\end{cases}\]</p><pre><code class="language-julia hljs">Bernoulli()    # Bernoulli distribution with p = 0.5
Bernoulli(p)   # Bernoulli distribution with success rate p

params(d)      # Get the parameters, i.e. (p,)
succprob(d)    # Get the success rate, i.e. p
failprob(d)    # Get the failure rate, i.e. 1 - p</code></pre><p>External links:</p><ul><li><a href="http://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution on Wikipedia</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/Distributions.jl/blob/v0.25.120/src/univariate/discrete/bernoulli.jl#L1-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Distributions.Poisson" href="#Distributions.Poisson"><code>Distributions.Poisson</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Poisson(λ)</code></pre><p>A <em>Poisson distribution</em> describes the number of independent events occurring within a unit time interval, given the average rate of occurrence <code>λ</code>.</p><p class="math-container">\[P(X = k) = \frac{\lambda^k}{k!} e^{-\lambda}, \quad \text{ for } k = 0,1,2,\ldots.\]</p><pre><code class="language-julia hljs">Poisson()        # Poisson distribution with rate parameter 1
Poisson(lambda)       # Poisson distribution with rate parameter lambda

params(d)        # Get the parameters, i.e. (λ,)
mean(d)          # Get the mean arrival rate, i.e. λ</code></pre><p>External links:</p><ul><li><a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution on Wikipedia</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaStats/Distributions.jl/blob/v0.25.120/src/univariate/discrete/poisson.jl#L1-L22">source</a></section></article><p>A <code>GeneralizedLinearMixedModel</code> object is generated from a formula, data frame and distribution family.</p><pre><code class="language-julia hljs">verbagg = MixedModelsDatasets.dataset(:verbagg)
const vaform = @formula(r2 ~ 1 + anger + gender + btype + situ + (1|subj) + (1|item));
mdl = GeneralizedLinearMixedModel(vaform, verbagg, Bernoulli());
typeof(mdl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GeneralizedLinearMixedModel{Float64, Bernoulli{Float64}}</code></pre><p>A separate call to <code>fit!</code> can be used to fit the model. This involves optimizing an objective function, the Laplace approximation to the deviance, with respect to the parameters, which are <span>$\beta$</span>, the fixed-effects coefficients, and <span>$\theta$</span>, the covariance parameters. The starting estimate for <span>$\beta$</span> is determined by fitting a GLM to the fixed-effects part of the formula</p><pre><code class="language-julia hljs">mdl.β</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6-element Vector{Float64}:
  0.20605302210322698
  0.0399403760511499
  0.23131667674984466
 -0.7941857249205363
 -1.539188208545692
 -0.7766556048305917</code></pre><p>and the starting estimate for <span>$\theta$</span>, which is a vector of the two standard deviations of the random effects, is chosen to be</p><pre><code class="language-julia hljs">mdl.θ</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 1.0
 1.0</code></pre><p>The Laplace approximation to the deviance requires determining the conditional modes of the random effects. These are the values that maximize the conditional density of the random effects, given the model parameters and the data. This is done using Penalized Iteratively Reweighted Least Squares (PIRLS). In most cases PIRLS is fast and stable. It is simply a penalized version of the IRLS algorithm used in fitting GLMs.</p><p>The distinction between the &quot;fast&quot; and &quot;slow&quot; algorithms in the <code>MixedModels</code> package (<code>nAGQ=0</code> or <code>nAGQ=1</code> in <code>lme4</code>) is whether the fixed-effects parameters, <span>$\beta$</span>, are optimized in PIRLS or in the nonlinear optimizer. In a call to the <code>pirls!</code> function the first argument is a <code>GeneralizedLinearMixedModel</code>, which is modified during the function call. (By convention, the names of such <em>mutating functions</em> end in <code>!</code> as a warning to the user that they can modify an argument, usually the first argument.) The second and third arguments are optional logical values indicating if <span>$\beta$</span> is to be varied and if verbose output is to be printed.</p><pre><code class="language-julia hljs">pirls!(mdl, true, false)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"></code></pre><pre><code class="language-julia hljs">deviance(mdl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8201.848559060621</code></pre><pre><code class="language-julia hljs">mdl.β</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6-element Vector{Float64}:
  0.21853493716521089
  0.051438542580812194
  0.29022454166301426
 -0.9791237061900246
 -1.9540167628140386
 -0.9794925718037328</code></pre><pre><code class="language-julia hljs">mdl.θ # current values of the standard deviations of the random effects</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 1.0
 1.0</code></pre><p>If the optimization with respect to <span>$\beta$</span> is performed within PIRLS then the nonlinear optimization of the Laplace approximation to the deviance requires optimization with respect to <span>$\theta$</span> only. This is the &quot;fast&quot; algorithm. Given a value of <span>$\theta$</span>, PIRLS is used to determine the conditional estimate of <span>$\beta$</span> and the conditional mode of the random effects, <strong>b</strong>.</p><pre><code class="language-julia hljs">mdl.b # conditional modes of b</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Matrix{Float64}}:
 [-0.6007716038488835 -1.9322680866219604 … -0.14455373975335975 -0.5752238433556965]
 [-0.18636418747906855 0.021422773585938852 … 0.641038340209828 0.6496779078972675]</code></pre><pre><code class="language-julia hljs">fit!(mdl, fast=true);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
  r2 ~ 1 + anger + gender + btype + situ + (1 | subj) + (1 | item)
  Distribution: Bernoulli{Float64}
  Link: LogitLink()

   logLik    deviance     AIC       AICc        BIC    
 -4075.7917  8151.5833  8167.5833  8167.6024  8223.0537

Variance components:
        Column   Variance Std.Dev. 
subj (Intercept)  1.794432 1.339564
item (Intercept)  0.246843 0.496833

 Number of obs: 7584; levels of grouping factors: 316, 24

Fixed-effects parameters:
─────────────────────────────────────────────────────
                   Coef.  Std. Error      z  Pr(&gt;|z|)
─────────────────────────────────────────────────────
(Intercept)    0.208273    0.405426    0.51    0.6075
anger          0.0543791   0.0167533   3.25    0.0012
gender: M      0.304089    0.191223    1.59    0.1118
btype: scold  -1.0165      0.257531   -3.95    &lt;1e-04
btype: shout  -2.0218      0.259235   -7.80    &lt;1e-14
situ: self    -1.01344     0.210888   -4.81    &lt;1e-05
─────────────────────────────────────────────────────</code></pre><p>The optimization process is summarized by</p><pre><code class="language-julia hljs">mdl.LMM.optsum</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Initial parameter vector: [1.0, 1.0]
Initial objective value:  8201.848559060621

Backend:                  nlopt
Optimizer:                LN_NEWUOA
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10]
initial_step:             [1.0, 1.0]
maxfeval:                 -1
maxtime:                  -1.0

Function evaluations:     36
xtol_zero_abs:            0.001
ftol_zero_abs:            1.0e-5
Final parameter vector:   [1.339564107131669, 0.49683280588920387]
Final objective value:    8151.58334013186
Return code:              FTOL_REACHED
</code></pre><p>As one would hope, given the name of the option, this fit is comparatively fast.</p><pre><code class="language-julia hljs">@btime fit(MixedModel, vaform, verbagg, Bernoulli(), fast=true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
  r2 ~ 1 + anger + gender + btype + situ + (1 | subj) + (1 | item)
  Distribution: Bernoulli{Float64}
  Link: LogitLink()

   logLik    deviance     AIC       AICc        BIC    
 -4075.7917  8151.5833  8167.5833  8167.6024  8223.0537

Variance components:
        Column   Variance Std.Dev. 
subj (Intercept)  1.794432 1.339564
item (Intercept)  0.246843 0.496833

 Number of obs: 7584; levels of grouping factors: 316, 24

Fixed-effects parameters:
─────────────────────────────────────────────────────
                   Coef.  Std. Error      z  Pr(&gt;|z|)
─────────────────────────────────────────────────────
(Intercept)    0.208273    0.405426    0.51    0.6075
anger          0.0543791   0.0167533   3.25    0.0012
gender: M      0.304089    0.191223    1.59    0.1118
btype: scold  -1.0165      0.257531   -3.95    &lt;1e-04
btype: shout  -2.0218      0.259235   -7.80    &lt;1e-14
situ: self    -1.01344     0.210888   -4.81    &lt;1e-05
─────────────────────────────────────────────────────</code></pre><p>The alternative algorithm is to use PIRLS to find the conditional mode of the random effects, given <span>$\beta$</span> and <span>$\theta$</span> and then use the general nonlinear optimizer to fit with respect to both <span>$\beta$</span> and <span>$\theta$</span>.</p><pre><code class="language-julia hljs">mdl1 = @btime fit(MixedModel, vaform, verbagg, Bernoulli())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)
  r2 ~ 1 + anger + gender + btype + situ + (1 | subj) + (1 | item)
  Distribution: Bernoulli{Float64}
  Link: LogitLink()

   logLik    deviance     AIC       AICc        BIC    
 -4075.6999  8151.3997  8167.3997  8167.4187  8222.8701

Variance components:
        Column   Variance Std.Dev. 
subj (Intercept)  1.794893 1.339736
item (Intercept)  0.245225 0.495202

 Number of obs: 7584; levels of grouping factors: 316, 24

Fixed-effects parameters:
─────────────────────────────────────────────────────
                   Coef.  Std. Error      z  Pr(&gt;|z|)
─────────────────────────────────────────────────────
(Intercept)    0.19934     0.405163    0.49    0.6227
anger          0.0574083   0.0167576   3.43    0.0006
gender: M      0.320615    0.191262    1.68    0.0937
btype: scold  -1.05833     0.256756   -4.12    &lt;1e-04
btype: shout  -2.10481     0.258479   -8.14    &lt;1e-15
situ: self    -1.0556      0.210262   -5.02    &lt;1e-06
─────────────────────────────────────────────────────</code></pre><p>This fit provided slightly better results (Laplace approximation to the deviance of 8151.400 versus 8151.583) but took 6 times as long. That is not terribly important when the times involved are a few seconds but can be important when the fit requires many hours or days of computing time.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../constructors/">« Model constructors</a><a class="docs-footer-nextpage" href="../GaussHermite/">Normalized Gauss-Hermite Quadrature »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Friday 29 August 2025 07:20">Friday 29 August 2025</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
