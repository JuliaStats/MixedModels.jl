{
    "docs": [
        {
            "location": "/", 
            "text": "MixedModels.jl\n\n\nFitting and examining mixed-effects models\n\n\n\n\nManual Outline\n\n\n\n\nFitting linear mixed-effects models\n\n\nA simple example\n\n\nMore substantial examples\n\n\n\n\n\n\n\n\n\n\nLibrary Outline\n\n\n\n\nPublic Documentation\n\n\nContents\n\n\nIndex\n\n\nMixedModels\n\n\n\n\n\n\nInternal Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nFunctions and methods\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nBase.LinAlg.cond\n\n\nMixedModels.LaplaceDeviance\n\n\nMixedModels.LinearMixedModel\n\n\nMixedModels.ScalarReMat\n\n\nMixedModels.VarCorr\n\n\nMixedModels.VectorReMat\n\n\nMixedModels.bootstrap\n\n\nMixedModels.fixef\n\n\nMixedModels.lmm\n\n\nMixedModels.lowerbd\n\n\nMixedModels.objective\n\n\nMixedModels.pirls!\n\n\nMixedModels.pwrss\n\n\nMixedModels.ranef\n\n\nMixedModels.refit!\n\n\nMixedModels.remat\n\n\nMixedModels.sdest\n\n\nMixedModels.simulate!\n\n\nMixedModels.varest\n\n\nStatsBase.coef\n\n\nStatsBase.coeftable\n\n\nStatsBase.deviance\n\n\nStatsBase.df\n\n\nStatsBase.fit!\n\n\nStatsBase.fitted\n\n\nStatsBase.model_response\n\n\nStatsBase.nobs\n\n\nStatsBase.vcov\n\n\nMixedModels.OptSummary\n\n\nMixedModels.ranef!", 
            "title": "Home"
        }, 
        {
            "location": "/#mixedmodelsjl", 
            "text": "Fitting and examining mixed-effects models", 
            "title": "MixedModels.jl"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "Fitting linear mixed-effects models  A simple example  More substantial examples", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/#library-outline", 
            "text": "Public Documentation  Contents  Index  MixedModels    Internal Documentation  Contents  Index  Types  Functions and methods", 
            "title": "Library Outline"
        }, 
        {
            "location": "/#index", 
            "text": "Base.LinAlg.cond  MixedModels.LaplaceDeviance  MixedModels.LinearMixedModel  MixedModels.ScalarReMat  MixedModels.VarCorr  MixedModels.VectorReMat  MixedModels.bootstrap  MixedModels.fixef  MixedModels.lmm  MixedModels.lowerbd  MixedModels.objective  MixedModels.pirls!  MixedModels.pwrss  MixedModels.ranef  MixedModels.refit!  MixedModels.remat  MixedModels.sdest  MixedModels.simulate!  MixedModels.varest  StatsBase.coef  StatsBase.coeftable  StatsBase.deviance  StatsBase.df  StatsBase.fit!  StatsBase.fitted  StatsBase.model_response  StatsBase.nobs  StatsBase.vcov  MixedModels.OptSummary  MixedModels.ranef!", 
            "title": "Index"
        }, 
        {
            "location": "/man/fitting/", 
            "text": "Fitting linear mixed-effects models\n\n\nThe \nlmm\n function is similar to the \nlmer\n function in the \nlme4\n package for \nR\n.  The first two arguments for in the \nR\n version are \nformula\n and \ndata\n.  The principle method for the \nJulia\n version takes these arguments.\n\n\n\n\nA simple example\n\n\nThe simplest example of a mixed-effects model that we use in the \nlme4 package for R\n is a model fit to the \nDyestuff\n data.\n\n\n str\n(\nDyestuff\n)\n\n\ndata.frame\n:\n   \n30\n obs. of  \n2\n variables\n:\n\n \n$\n Batch\n:\n Factor w\n/\n \n6\n levels \nA\n,\nB\n,\nC\n,\nD\n,\n..\n:\n \n1\n \n1\n \n1\n \n1\n \n1\n \n2\n \n2\n \n2\n \n2\n \n2\n \n...\n\n \n$\n Yield\n:\n num  \n1545\n \n1440\n \n1440\n \n1520\n \n1580\n \n...\n\n\n \n(\nfm1 \n-\n lmer\n(\nYield \n~\n \n1\n|\nBatch\n,\n Dyestuff\n,\n REML\n=\nFALSE\n))\n\nLinear mixed model fit by maximum likelihood \n[\nlmerMod\n]\n\nFormula\n:\n Yield \n~\n \n1\n \n|\n Batch\n   Data\n:\n Dyestuff\n\n      AIC       BIC    logLik  deviance\n \n333.3271\n  \n337.5307\n \n-163.6635\n  \n327.3271\n\n\nRandom effects\n:\n\n Groups   Name        Variance Std.Dev.\n Batch    \n(\nIntercept\n)\n \n1388\n     \n37.26\n\n Residual             \n2451\n     \n49.51\n\n Number of obs\n:\n \n30\n,\n groups\n:\n Batch\n,\n \n6\n\n\nFixed effects\n:\n\n            Estimate Std. Error t value\n\n(\nIntercept\n)\n  \n1527.50\n      \n17.69\n   \n86.33\n\n\n\n\n\n\nThese \nDyestuff\n data are available through \nRCall\n but to run the doctests we use a stored copy of the dataframe.\n\n\njulia\n \nusing\n \nDataFrames\n,\n \nMixedModels\n\n\n\njulia\n \nds\n\n\n30\nx2\n \nDataFrames\n.\nDataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \nYield\n  \n\u2502\n \nBatch\n \n\u2502\n\n\n\u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\n\n\n\u2502\n \n1\n   \n\u2502\n \n1545.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n1440.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n1440.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n1520.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n1580.0\n \n\u2502\n \nA\n   \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n1540.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n7\n   \n\u2502\n \n1555.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u2502\n \n8\n   \n\u2502\n \n1490.0\n \n\u2502\n \nB\n   \n\u2502\n\n\n\u22ee\n\n\n\u2502\n \n22\n  \n\u2502\n \n1630.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n23\n  \n\u2502\n \n1515.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n24\n  \n\u2502\n \n1635.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n25\n  \n\u2502\n \n1625.0\n \n\u2502\n \nE\n   \n\u2502\n\n\n\u2502\n \n26\n  \n\u2502\n \n1520.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n27\n  \n\u2502\n \n1455.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n28\n  \n\u2502\n \n1450.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n29\n  \n\u2502\n \n1480.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\u2502\n \n30\n  \n\u2502\n \n1445.0\n \n\u2502\n \nF\n   \n\u2502\n\n\n\n\n\n\nlmm\n defaults to maximum likelihood estimation whereas \nlmer\n in \nR\n defaults to REML estimation.\n\n\njulia\n \nm\n \n=\n \nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n))\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nlogLik\n:\n \n-\n163.663530\n,\n \ndeviance\n:\n \n327.327060\n,\n \nAIC\n:\n \n333.327060\n,\n \nBIC\n:\n \n337.530652\n\n\n\nVariance\n \ncomponents\n:\n\n           \nVariance\n  \nStd\n.\nDev\n.\n\n \nBatch\n    \n1388.3332\n \n37.260344\n\n \nResidual\n \n2451.2500\n \n49.510100\n\n \nNumber\n \nof\n \nobs\n:\n \n30\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n6\n\n\n  \nFixed\n-\neffects\n \nparameters\n:\n\n             \nEstimate\n \nStd\n.\nError\n \nz\n \nvalue\n\n\n(\nIntercept\n)\n    \n1527.5\n   \n17.6946\n  \n86.326\n\n\n\n\n\n\nIn general the model should be fit through an explicit call to the \nfit!\n function, which may take a second argument indicating a verbose fit.\n\n\njulia\n \nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n),\n \ntrue\n);\n\n\nf_1\n:\n \n327.76702\n,\n \n[\n1.0\n]\n\n\nf_2\n:\n \n331.03619\n,\n \n[\n1.75\n]\n\n\nf_3\n:\n \n330.64583\n,\n \n[\n0.25\n]\n\n\nf_4\n:\n \n327.69511\n,\n \n[\n0.97619\n]\n\n\nf_5\n:\n \n327.56631\n,\n \n[\n0.928569\n]\n\n\nf_6\n:\n \n327.3826\n,\n \n[\n0.833327\n]\n\n\nf_7\n:\n \n327.35315\n,\n \n[\n0.807188\n]\n\n\nf_8\n:\n \n327.34663\n,\n \n[\n0.799688\n]\n\n\nf_9\n:\n \n327.341\n,\n \n[\n0.792188\n]\n\n\nf_10\n:\n \n327.33253\n,\n \n[\n0.777188\n]\n\n\nf_11\n:\n \n327.32733\n,\n \n[\n0.747188\n]\n\n\nf_12\n:\n \n327.32862\n,\n \n[\n0.739688\n]\n\n\nf_13\n:\n \n327.32706\n,\n \n[\n0.752777\n]\n\n\nf_14\n:\n \n327.32707\n,\n \n[\n0.753527\n]\n\n\nf_15\n:\n \n327.32706\n,\n \n[\n0.752584\n]\n\n\nf_16\n:\n \n327.32706\n,\n \n[\n0.752509\n]\n\n\nf_17\n:\n \n327.32706\n,\n \n[\n0.752591\n]\n\n\nf_18\n:\n \n327.32706\n,\n \n[\n0.752581\n]\n\n\nFTOL_REACHED\n\n\n\n\n\n\nThe numeric representation of the model has type\n\n\njulia\n \ntypeof\n(\nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n)))\n\n\nMixedModels\n.\nLinearMixedModel\n{\nFloat64\n}\n\n\n\n\n\n\nThose familiar with the \nlme4\n package for \nR\n will see the usual suspects.\n\n\njulia\n \nm\n \n=\n \nfit!\n(\nlmm\n(\nYield\n \n~\n \n1\n \n+\n \n(\n1\n \n|\n \nBatch\n),\n \nds\n));\n\n\n\njulia\n \nfixef\n(\nm\n)\n  \n# estimates of the fixed-effects parameters\n\n\n1\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n1527.5\n\n\n\njulia\n \ncoef\n(\nm\n)\n  \n# another name for fixef\n\n\n1\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n1527.5\n\n\n\njulia\n \nranef\n(\nm\n)\n\n\n1\n-\nelement\n \nArray\n{\nArray\n{\nFloat64\n,\n2\n},\n1\n}:\n\n \n1\nx6\n \nArray\n{\nFloat64\n,\n2\n}:\n\n \n-\n16.6282\n  \n0.369516\n  \n26.9747\n  \n-\n21.8014\n  \n53.5798\n  \n-\n42.4943\n\n\n\njulia\n \nranef\n(\nm\n,\n \ntrue\n)\n  \n# on the u scale\n\n\n1\n-\nelement\n \nArray\n{\nArray\n{\nFloat64\n,\n2\n},\n1\n}:\n\n \n1\nx6\n \nArray\n{\nFloat64\n,\n2\n}:\n\n \n-\n22.0949\n  \n0.490999\n  \n35.8429\n  \n-\n28.9689\n  \n71.1948\n  \n-\n56.4648\n\n\n\njulia\n \ndeviance\n(\nm\n)\n\n\n327.3270598811394\n\n\n\njulia\n \nobjective\n(\nm\n)\n\n\n327.3270598811394\n\n\n\n\n\n\nWe prefer \nobjective\n to \ndeviance\n because the value returned is \n-2loglikelihood(m)\n, without the correction for the null deviance. It is not clear how the null deviance should be defined for these models.\n\n\n\n\nMore substantial examples\n\n\nFitting a model to the \nDyestuff\n data is trivial.  The \nInstEval\n data in the \nlme4\n package is more of a challenge in that there are nearly 75,000 evaluations by 2972 students on a total of 1128 instructors.\n\n\njulia\n \nhead\n(\ninst\n)\n\n\n6x7\n \nDataFrames\n.DataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \ns\n   \n\u2502\n \nd\n      \n\u2502\n \nstudage\n \n\u2502\n \nlectage\n \n\u2502\n \nservice\n \n\u2502\n \ndept\n \n\u2502\n \ny\n \n\u2502\n\n\n\u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2525\n\n\n\u2502\n \n1\n   \n\u2502\n \n1\n \n\u2502\n \n1002\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n0\n     \n\u2502\n \n2\n  \n\u2502\n \n5\n \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n1\n \n\u2502\n \n1050\n \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n1\n     \n\u2502\n \n6\n  \n\u2502\n \n2\n \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n1\n \n\u2502\n \n1582\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n0\n     \n\u2502\n \n2\n  \n\u2502\n \n5\n \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n1\n \n\u2502\n \n2050\n \n\u2502\n \n2\n     \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n3\n  \n\u2502\n \n3\n \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n2\n \n\u2502\n \n115\n  \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n0\n     \n\u2502\n \n5\n  \n\u2502\n \n2\n \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n2\n \n\u2502\n \n756\n  \n\u2502\n \n2\n     \n\u2502\n \n1\n     \n\u2502\n \n0\n     \n\u2502\n \n5\n  \n\u2502\n \n4\n \n\u2502\n\n\n\njulia\n \nm2\n \n=\n \nfit\n!(\nlmm\n(\ny\n \n~\n \n1\n \n+\n \ndept\n*\nservice\n \n+\n \n(\n1\n|\ns\n)\n \n+\n \n(\n1\n|\nd\n),\n \ninst\n))\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nlogLik\n:\n \n-118792\n.776708\n,\n \ndeviance\n:\n \n237585\n.553415\n,\n \nAIC\n:\n \n237647\n.553415\n,\n \nBIC\n:\n \n237932\n.876339\n\n\n\nVariance\n \ncomponents\n:\n\n            \nVariance\n   \nStd\n.Dev\n.\n\n \ns\n        \n0\n.105417971\n \n0\n.32468134\n\n \nd\n        \n0\n.258416394\n \n0\n.50834673\n\n \nResidual\n \n1\n.384727771\n \n1\n.17674456\n\n \nNumber\n \nof\n \nobs\n:\n \n73421\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n2972\n,\n \n1128\n\n\n  \nFixed-effects\n \nparameters\n:\n\n                           \nEstimate\n \nStd\n.Error\n   \nz\n \nvalue\n\n\n(\nIntercept\n)\n                 \n3\n.22961\n  \n0\n.064053\n   \n50\n.4209\n\n\ndept\n \n-\n \n5\n                   \n0\n.129536\n  \n0\n.101294\n   \n1\n.27882\n\n\ndept\n \n-\n \n10\n                 \n-0\n.176751\n \n0\n.0881352\n  \n-2\n.00545\n\n\ndept\n \n-\n \n12\n                 \n0\n.0517102\n \n0\n.0817524\n  \n0\n.632522\n\n\ndept\n \n-\n \n6\n                  \n0\n.0347319\n  \n0\n.085621\n  \n0\n.405647\n\n\ndept\n \n-\n \n7\n                    \n0\n.14594\n \n0\n.0997984\n   \n1\n.46235\n\n\ndept\n \n-\n \n4\n                   \n0\n.151689\n \n0\n.0816897\n   \n1\n.85689\n\n\ndept\n \n-\n \n8\n                   \n0\n.104206\n  \n0\n.118751\n  \n0\n.877517\n\n\ndept\n \n-\n \n9\n                  \n0\n.0440401\n \n0\n.0962985\n  \n0\n.457329\n\n\ndept\n \n-\n \n14\n                 \n0\n.0517546\n \n0\n.0986029\n  \n0\n.524879\n\n\ndept\n \n-\n \n1\n                  \n0\n.0466719\n  \n0\n.101942\n  \n0\n.457828\n\n\ndept\n \n-\n \n3\n                  \n0\n.0563461\n \n0\n.0977925\n   \n0\n.57618\n\n\ndept\n \n-\n \n11\n                 \n0\n.0596536\n  \n0\n.100233\n   \n0\n.59515\n\n\ndept\n \n-\n \n2\n                 \n0\n.00556281\n  \n0\n.110867\n \n0\n.0501757\n\n\nservice\n \n-\n \n1\n                \n0\n.252025\n \n0\n.0686507\n   \n3\n.67112\n\n\ndept\n \n-\n \n5\n \n \nservice\n \n-\n \n1\n    \n-0\n.180757\n  \n0\n.123179\n  \n-1\n.46744\n\n\ndept\n \n-\n \n10\n \n \nservice\n \n-\n \n1\n   \n0\n.0186492\n  \n0\n.110017\n  \n0\n.169512\n\n\ndept\n \n-\n \n12\n \n \nservice\n \n-\n \n1\n   \n-0\n.282269\n \n0\n.0792937\n  \n-3\n.55979\n\n\ndept\n \n-\n \n6\n \n \nservice\n \n-\n \n1\n    \n-0\n.494464\n \n0\n.0790278\n  \n-6\n.25683\n\n\ndept\n \n-\n \n7\n \n \nservice\n \n-\n \n1\n    \n-0\n.392054\n  \n0\n.110313\n  \n-3\n.55403\n\n\ndept\n \n-\n \n4\n \n \nservice\n \n-\n \n1\n    \n-0\n.278547\n \n0\n.0823727\n  \n-3\n.38154\n\n\ndept\n \n-\n \n8\n \n \nservice\n \n-\n \n1\n    \n-0\n.189526\n  \n0\n.111449\n  \n-1\n.70056\n\n\ndept\n \n-\n \n9\n \n \nservice\n \n-\n \n1\n    \n-0\n.499868\n \n0\n.0885423\n  \n-5\n.64553\n\n\ndept\n \n-\n \n14\n \n \nservice\n \n-\n \n1\n   \n-0\n.497162\n \n0\n.0917162\n  \n-5\n.42065\n\n\ndept\n \n-\n \n1\n \n \nservice\n \n-\n \n1\n     \n-0\n.24042\n \n0\n.0982071\n   \n-2\n.4481\n\n\ndept\n \n-\n \n3\n \n \nservice\n \n-\n \n1\n    \n-0\n.223013\n \n0\n.0890548\n  \n-2\n.50422\n\n\ndept\n \n-\n \n11\n \n \nservice\n \n-\n \n1\n   \n-0\n.516997\n \n0\n.0809077\n  \n-6\n.38997\n\n\ndept\n \n-\n \n2\n \n \nservice\n \n-\n \n1\n    \n-0\n.384773\n  \n0\n.091843\n  \n-4\n.18946\n\n\n\n\n\n\nModels with vector-valued random effects can be fit\n\n\njulia\n \nslp\n\n\n180\nx3\n \nDataFrames\n.\nDataFrame\n\n\n\u2502\n \nRow\n \n\u2502\n \nReaction\n \n\u2502\n \nDays\n \n\u2502\n \nSubject\n \n\u2502\n\n\n\u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\n\n\n\u2502\n \n1\n   \n\u2502\n \n249.56\n   \n\u2502\n \n0\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n2\n   \n\u2502\n \n258.705\n  \n\u2502\n \n1\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n3\n   \n\u2502\n \n250.801\n  \n\u2502\n \n2\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n4\n   \n\u2502\n \n321.44\n   \n\u2502\n \n3\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n5\n   \n\u2502\n \n356.852\n  \n\u2502\n \n4\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n6\n   \n\u2502\n \n414.69\n   \n\u2502\n \n5\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n7\n   \n\u2502\n \n382.204\n  \n\u2502\n \n6\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u2502\n \n8\n   \n\u2502\n \n290.149\n  \n\u2502\n \n7\n    \n\u2502\n \n1\n       \n\u2502\n\n\n\u22ee\n\n\n\u2502\n \n172\n \n\u2502\n \n273.474\n  \n\u2502\n \n1\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n173\n \n\u2502\n \n297.597\n  \n\u2502\n \n2\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n174\n \n\u2502\n \n310.632\n  \n\u2502\n \n3\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n175\n \n\u2502\n \n287.173\n  \n\u2502\n \n4\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n176\n \n\u2502\n \n329.608\n  \n\u2502\n \n5\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n177\n \n\u2502\n \n334.482\n  \n\u2502\n \n6\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n178\n \n\u2502\n \n343.22\n   \n\u2502\n \n7\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n179\n \n\u2502\n \n369.142\n  \n\u2502\n \n8\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\u2502\n \n180\n \n\u2502\n \n364.124\n  \n\u2502\n \n9\n    \n\u2502\n \n18\n      \n\u2502\n\n\n\njulia\n \nfm3\n \n=\n \nfit!\n(\nlmm\n(\nReaction\n \n~\n \n1\n \n+\n \nDays\n \n+\n \n(\n1\n+\nDays\n|\nSubject\n),\n \nslp\n))\n\n\nLinear\n \nmixed\n \nmodel\n \nfit\n \nby\n \nmaximum\n \nlikelihood\n\n \nlogLik\n:\n \n-\n875.969672\n,\n \ndeviance\n:\n \n1751.939344\n,\n \nAIC\n:\n \n1763.939344\n,\n \nBIC\n:\n \n1783.097086\n\n\n\nVariance\n \ncomponents\n:\n\n           \nVariance\n  \nStd\n.\nDev\n.\n   \nCorr\n.\n\n \nSubject\n  \n565.51066\n \n23.780468\n\n           \n32.68212\n  \n5.716828\n  \n0.08\n\n \nResidual\n \n654.94145\n \n25.591824\n\n \nNumber\n \nof\n \nobs\n:\n \n180\n;\n \nlevels\n \nof\n \ngrouping\n \nfactors\n:\n \n18\n\n\n  \nFixed\n-\neffects\n \nparameters\n:\n\n             \nEstimate\n \nStd\n.\nError\n \nz\n \nvalue\n\n\n(\nIntercept\n)\n   \n251.405\n   \n6.63226\n \n37.9064\n\n\nDays\n          \n10.4673\n   \n1.50224\n \n6.96781", 
            "title": "Fitting"
        }, 
        {
            "location": "/man/fitting/#fitting-linear-mixed-effects-models", 
            "text": "The  lmm  function is similar to the  lmer  function in the  lme4  package for  R .  The first two arguments for in the  R  version are  formula  and  data .  The principle method for the  Julia  version takes these arguments.", 
            "title": "Fitting linear mixed-effects models"
        }, 
        {
            "location": "/man/fitting/#a-simple-example", 
            "text": "The simplest example of a mixed-effects model that we use in the  lme4 package for R  is a model fit to the  Dyestuff  data.   str ( Dyestuff )  data.frame :     30  obs. of   2  variables : \n  $  Batch :  Factor w /   6  levels  A , B , C , D , .. :   1   1   1   1   1   2   2   2   2   2   ... \n  $  Yield :  num   1545   1440   1440   1520   1580   ...    ( fm1  -  lmer ( Yield  ~   1 | Batch ,  Dyestuff ,  REML = FALSE )) \nLinear mixed model fit by maximum likelihood  [ lmerMod ] \nFormula :  Yield  ~   1   |  Batch\n   Data :  Dyestuff\n\n      AIC       BIC    logLik  deviance\n  333.3271    337.5307   -163.6635    327.3271 \n\nRandom effects : \n Groups   Name        Variance Std.Dev.\n Batch     ( Intercept )   1388       37.26 \n Residual              2451       49.51 \n Number of obs :   30 ,  groups :  Batch ,   6 \n\nFixed effects : \n            Estimate Std. Error t value ( Intercept )    1527.50        17.69     86.33   These  Dyestuff  data are available through  RCall  but to run the doctests we use a stored copy of the dataframe.  julia   using   DataFrames ,   MixedModels  julia   ds  30 x2   DataFrames . DataFrame  \u2502   Row   \u2502   Yield    \u2502   Batch   \u2502  \u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525  \u2502   1     \u2502   1545.0   \u2502   A     \u2502  \u2502   2     \u2502   1440.0   \u2502   A     \u2502  \u2502   3     \u2502   1440.0   \u2502   A     \u2502  \u2502   4     \u2502   1520.0   \u2502   A     \u2502  \u2502   5     \u2502   1580.0   \u2502   A     \u2502  \u2502   6     \u2502   1540.0   \u2502   B     \u2502  \u2502   7     \u2502   1555.0   \u2502   B     \u2502  \u2502   8     \u2502   1490.0   \u2502   B     \u2502  \u22ee  \u2502   22    \u2502   1630.0   \u2502   E     \u2502  \u2502   23    \u2502   1515.0   \u2502   E     \u2502  \u2502   24    \u2502   1635.0   \u2502   E     \u2502  \u2502   25    \u2502   1625.0   \u2502   E     \u2502  \u2502   26    \u2502   1520.0   \u2502   F     \u2502  \u2502   27    \u2502   1455.0   \u2502   F     \u2502  \u2502   28    \u2502   1450.0   \u2502   F     \u2502  \u2502   29    \u2502   1480.0   \u2502   F     \u2502  \u2502   30    \u2502   1445.0   \u2502   F     \u2502   lmm  defaults to maximum likelihood estimation whereas  lmer  in  R  defaults to REML estimation.  julia   m   =   fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds ))  Linear   mixed   model   fit   by   maximum   likelihood \n  logLik :   - 163.663530 ,   deviance :   327.327060 ,   AIC :   333.327060 ,   BIC :   337.530652  Variance   components : \n            Variance    Std . Dev . \n  Batch      1388.3332   37.260344 \n  Residual   2451.2500   49.510100 \n  Number   of   obs :   30 ;   levels   of   grouping   factors :   6 \n\n   Fixed - effects   parameters : \n              Estimate   Std . Error   z   value  ( Intercept )      1527.5     17.6946    86.326   In general the model should be fit through an explicit call to the  fit!  function, which may take a second argument indicating a verbose fit.  julia   fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds ),   true );  f_1 :   327.76702 ,   [ 1.0 ]  f_2 :   331.03619 ,   [ 1.75 ]  f_3 :   330.64583 ,   [ 0.25 ]  f_4 :   327.69511 ,   [ 0.97619 ]  f_5 :   327.56631 ,   [ 0.928569 ]  f_6 :   327.3826 ,   [ 0.833327 ]  f_7 :   327.35315 ,   [ 0.807188 ]  f_8 :   327.34663 ,   [ 0.799688 ]  f_9 :   327.341 ,   [ 0.792188 ]  f_10 :   327.33253 ,   [ 0.777188 ]  f_11 :   327.32733 ,   [ 0.747188 ]  f_12 :   327.32862 ,   [ 0.739688 ]  f_13 :   327.32706 ,   [ 0.752777 ]  f_14 :   327.32707 ,   [ 0.753527 ]  f_15 :   327.32706 ,   [ 0.752584 ]  f_16 :   327.32706 ,   [ 0.752509 ]  f_17 :   327.32706 ,   [ 0.752591 ]  f_18 :   327.32706 ,   [ 0.752581 ]  FTOL_REACHED   The numeric representation of the model has type  julia   typeof ( fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds )))  MixedModels . LinearMixedModel { Float64 }   Those familiar with the  lme4  package for  R  will see the usual suspects.  julia   m   =   fit! ( lmm ( Yield   ~   1   +   ( 1   |   Batch ),   ds ));  julia   fixef ( m )    # estimates of the fixed-effects parameters  1 - element   Array { Float64 , 1 }: \n  1527.5  julia   coef ( m )    # another name for fixef  1 - element   Array { Float64 , 1 }: \n  1527.5  julia   ranef ( m )  1 - element   Array { Array { Float64 , 2 }, 1 }: \n  1 x6   Array { Float64 , 2 }: \n  - 16.6282    0.369516    26.9747    - 21.8014    53.5798    - 42.4943  julia   ranef ( m ,   true )    # on the u scale  1 - element   Array { Array { Float64 , 2 }, 1 }: \n  1 x6   Array { Float64 , 2 }: \n  - 22.0949    0.490999    35.8429    - 28.9689    71.1948    - 56.4648  julia   deviance ( m )  327.3270598811394  julia   objective ( m )  327.3270598811394   We prefer  objective  to  deviance  because the value returned is  -2loglikelihood(m) , without the correction for the null deviance. It is not clear how the null deviance should be defined for these models.", 
            "title": "A simple example"
        }, 
        {
            "location": "/man/fitting/#more-substantial-examples", 
            "text": "Fitting a model to the  Dyestuff  data is trivial.  The  InstEval  data in the  lme4  package is more of a challenge in that there are nearly 75,000 evaluations by 2972 students on a total of 1128 instructors.  julia   head ( inst )  6x7   DataFrames .DataFrame  \u2502   Row   \u2502   s     \u2502   d        \u2502   studage   \u2502   lectage   \u2502   service   \u2502   dept   \u2502   y   \u2502  \u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2525  \u2502   1     \u2502   1   \u2502   1002   \u2502   2       \u2502   2       \u2502   0       \u2502   2    \u2502   5   \u2502  \u2502   2     \u2502   1   \u2502   1050   \u2502   2       \u2502   1       \u2502   1       \u2502   6    \u2502   2   \u2502  \u2502   3     \u2502   1   \u2502   1582   \u2502   2       \u2502   2       \u2502   0       \u2502   2    \u2502   5   \u2502  \u2502   4     \u2502   1   \u2502   2050   \u2502   2       \u2502   2       \u2502   1       \u2502   3    \u2502   3   \u2502  \u2502   5     \u2502   2   \u2502   115    \u2502   2       \u2502   1       \u2502   0       \u2502   5    \u2502   2   \u2502  \u2502   6     \u2502   2   \u2502   756    \u2502   2       \u2502   1       \u2502   0       \u2502   5    \u2502   4   \u2502  julia   m2   =   fit !( lmm ( y   ~   1   +   dept * service   +   ( 1 | s )   +   ( 1 | d ),   inst ))  Linear   mixed   model   fit   by   maximum   likelihood \n  logLik :   -118792 .776708 ,   deviance :   237585 .553415 ,   AIC :   237647 .553415 ,   BIC :   237932 .876339  Variance   components : \n             Variance     Std .Dev . \n  s          0 .105417971   0 .32468134 \n  d          0 .258416394   0 .50834673 \n  Residual   1 .384727771   1 .17674456 \n  Number   of   obs :   73421 ;   levels   of   grouping   factors :   2972 ,   1128 \n\n   Fixed-effects   parameters : \n                            Estimate   Std .Error     z   value  ( Intercept )                   3 .22961    0 .064053     50 .4209  dept   -   5                     0 .129536    0 .101294     1 .27882  dept   -   10                   -0 .176751   0 .0881352    -2 .00545  dept   -   12                   0 .0517102   0 .0817524    0 .632522  dept   -   6                    0 .0347319    0 .085621    0 .405647  dept   -   7                      0 .14594   0 .0997984     1 .46235  dept   -   4                     0 .151689   0 .0816897     1 .85689  dept   -   8                     0 .104206    0 .118751    0 .877517  dept   -   9                    0 .0440401   0 .0962985    0 .457329  dept   -   14                   0 .0517546   0 .0986029    0 .524879  dept   -   1                    0 .0466719    0 .101942    0 .457828  dept   -   3                    0 .0563461   0 .0977925     0 .57618  dept   -   11                   0 .0596536    0 .100233     0 .59515  dept   -   2                   0 .00556281    0 .110867   0 .0501757  service   -   1                  0 .252025   0 .0686507     3 .67112  dept   -   5     service   -   1      -0 .180757    0 .123179    -1 .46744  dept   -   10     service   -   1     0 .0186492    0 .110017    0 .169512  dept   -   12     service   -   1     -0 .282269   0 .0792937    -3 .55979  dept   -   6     service   -   1      -0 .494464   0 .0790278    -6 .25683  dept   -   7     service   -   1      -0 .392054    0 .110313    -3 .55403  dept   -   4     service   -   1      -0 .278547   0 .0823727    -3 .38154  dept   -   8     service   -   1      -0 .189526    0 .111449    -1 .70056  dept   -   9     service   -   1      -0 .499868   0 .0885423    -5 .64553  dept   -   14     service   -   1     -0 .497162   0 .0917162    -5 .42065  dept   -   1     service   -   1       -0 .24042   0 .0982071     -2 .4481  dept   -   3     service   -   1      -0 .223013   0 .0890548    -2 .50422  dept   -   11     service   -   1     -0 .516997   0 .0809077    -6 .38997  dept   -   2     service   -   1      -0 .384773    0 .091843    -4 .18946   Models with vector-valued random effects can be fit  julia   slp  180 x3   DataFrames . DataFrame  \u2502   Row   \u2502   Reaction   \u2502   Days   \u2502   Subject   \u2502  \u251d\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525  \u2502   1     \u2502   249.56     \u2502   0      \u2502   1         \u2502  \u2502   2     \u2502   258.705    \u2502   1      \u2502   1         \u2502  \u2502   3     \u2502   250.801    \u2502   2      \u2502   1         \u2502  \u2502   4     \u2502   321.44     \u2502   3      \u2502   1         \u2502  \u2502   5     \u2502   356.852    \u2502   4      \u2502   1         \u2502  \u2502   6     \u2502   414.69     \u2502   5      \u2502   1         \u2502  \u2502   7     \u2502   382.204    \u2502   6      \u2502   1         \u2502  \u2502   8     \u2502   290.149    \u2502   7      \u2502   1         \u2502  \u22ee  \u2502   172   \u2502   273.474    \u2502   1      \u2502   18        \u2502  \u2502   173   \u2502   297.597    \u2502   2      \u2502   18        \u2502  \u2502   174   \u2502   310.632    \u2502   3      \u2502   18        \u2502  \u2502   175   \u2502   287.173    \u2502   4      \u2502   18        \u2502  \u2502   176   \u2502   329.608    \u2502   5      \u2502   18        \u2502  \u2502   177   \u2502   334.482    \u2502   6      \u2502   18        \u2502  \u2502   178   \u2502   343.22     \u2502   7      \u2502   18        \u2502  \u2502   179   \u2502   369.142    \u2502   8      \u2502   18        \u2502  \u2502   180   \u2502   364.124    \u2502   9      \u2502   18        \u2502  julia   fm3   =   fit! ( lmm ( Reaction   ~   1   +   Days   +   ( 1 + Days | Subject ),   slp ))  Linear   mixed   model   fit   by   maximum   likelihood \n  logLik :   - 875.969672 ,   deviance :   1751.939344 ,   AIC :   1763.939344 ,   BIC :   1783.097086  Variance   components : \n            Variance    Std . Dev .     Corr . \n  Subject    565.51066   23.780468 \n            32.68212    5.716828    0.08 \n  Residual   654.94145   25.591824 \n  Number   of   obs :   180 ;   levels   of   grouping   factors :   18 \n\n   Fixed - effects   parameters : \n              Estimate   Std . Error   z   value  ( Intercept )     251.405     6.63226   37.9064  Days            10.4673     1.50224   6.96781", 
            "title": "More substantial examples"
        }, 
        {
            "location": "/lib/public/", 
            "text": "Public Documentation\n\n\nDocumentation for \nMixedModels.jl\n's public interface.\n\n\nSee \nInternal Documentation\n for internal package docs covering all submodules.\n\n\n\n\nContents\n\n\n\n\nPublic Documentation\n\n\nContents\n\n\nIndex\n\n\nMixedModels\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nBase.LinAlg.cond\n\n\nMixedModels.LaplaceDeviance\n\n\nMixedModels.LinearMixedModel\n\n\nMixedModels.ScalarReMat\n\n\nMixedModels.VarCorr\n\n\nMixedModels.VectorReMat\n\n\nMixedModels.bootstrap\n\n\nMixedModels.fixef\n\n\nMixedModels.lmm\n\n\nMixedModels.lowerbd\n\n\nMixedModels.objective\n\n\nMixedModels.pirls!\n\n\nMixedModels.pwrss\n\n\nMixedModels.ranef\n\n\nMixedModels.refit!\n\n\nMixedModels.remat\n\n\nMixedModels.sdest\n\n\nMixedModels.simulate!\n\n\nMixedModels.varest\n\n\nStatsBase.coef\n\n\nStatsBase.coeftable\n\n\nStatsBase.deviance\n\n\nStatsBase.df\n\n\nStatsBase.fit!\n\n\nStatsBase.fitted\n\n\nStatsBase.model_response\n\n\nStatsBase.nobs\n\n\nStatsBase.vcov\n\n\n\n\n\n\nMixedModels\n\n\n#\n\n\nMixedModels.LinearMixedModel\n \n \nType\n.\n\n\n\n\nLinearMixedModel\n\n\n\n\n\nLinear mixed-effects model representation\n\n\nMembers:\n\n\n\n\nmf\n: the model frame, mostly used to get the \nterms\n component for labelling fixed effects\n\n\nwttrms\n: a length \nnt\n vector of weighted model matrices. The last two elements are \nX\n and \ny\n.\n\n\ntrms\n: a vector of unweighted model matrices.  If \nisempty(sqrtwts)\n the same object as \nwttrms\n\n\n\u039b\n: a length \nnt - 2\n vector of lower triangular matrices\n\n\nsqrtwts\n: the \nDiagonal\n matrix of the square roots of the case weights.  Allowed to be size 0\n\n\nA\n: an \nnt \u00d7 nt\n symmetric matrix of matrices representing \nhcat(Z,X,y)'hcat(Z,X,y)\n\n\nR\n: a \nnt \u00d7 nt\n matrix of matrices - the upper Cholesky factor of \n\u039b'A\u039b+I\n\n\nopt\n: an \nOptSummary\n object\n\n\n\n\n#\n\n\nMixedModels.ScalarReMat\n \n \nType\n.\n\n\n\n\nScalarReMat\n\n\n\n\n\nThe representation of the model matrix for a scalar random-effects term\n\n\nMembers:\n\n\n\n\nf\n: the grouping factor as a \nPooledDataVector\n\n\nz\n: the raw random-effects model matrix as a \nVector\n\n\nfnm\n: the name of the grouping factor as a \nSymbol\n\n\ncnms\n: a \nVector\n of column names\n\n\n\n\n#\n\n\nMixedModels.VarCorr\n \n \nType\n.\n\n\n\n\nVarCorr\n\n\n\n\n\nAn encapsulation of information on the fitted random-effects variance-covariance matrices.\n\n\nMembers:\n\n\n\n\n\u039b\n: the vector of lower triangular matrices from the \nMixedModel\n\n\nfnms\n: a \nVector{ASCIIString}\n of grouping factor names\n\n\ncnms\n: a \nVector{Vector{ASCIIString}}\n of column names\n\n\ns\n: the estimate of \u03c3, the standard deviation of the per-observation noise\n\n\n\n\nThe main purpose is to isolate the logic in the show method.\n\n\n#\n\n\nMixedModels.VectorReMat\n \n \nType\n.\n\n\n\n\nVectorReMat\n\n\n\n\n\nThe representation of the model matrix for a vector-valued random-effects term\n\n\nMembers:\n\n\n\n\nf\n: the grouping factor as a \nPooledDataVector\n\n\nz\n: the transposed raw random-effects model matrix\n\n\nfnm\n: the name of the grouping factor as a \nSymbol\n\n\ncnms\n: a \nVector\n of column names (row names after transposition) of \nz\n\n\n\n\n#\n\n\nMixedModels.bootstrap\n \n \nFunction\n.\n\n\n\n\nbootstrap(m, N, saveresults)\n\n\n\n\n\nSimulate \nN\n response vectors from \nm\n, refitting the model.  The function saveresults is called after each refit.\n\n\nTo save space \nm.trms[end]\n, which is the response vector, is overwritten by each simulation.  The original response is restored before returning.\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n that has been fit.\n\n\nN\n: the number of bootstrap samples to simulate\n\n\nsavresults\n: a function with arguments \ni\n and \nm\n called after each bootstrap simulation.    As the name indicates, this function should save the results of interest.\n\n\n\n\n#\n\n\nStatsBase.coef\n \n \nMethod\n.\n\n\n\n\nnothing\n\n#\n\n\nStatsBase.coeftable\n \n \nMethod\n.\n\n\n\n\nnothing\n\n#\n\n\nBase.LinAlg.cond\n \n \nMethod\n.\n\n\n\n\ncond(m::MixedModel)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nMixedModel\n\n\n\n\nReturns:   A \nVector\n of the condition numbers of the blocks of \nm.\u039b\n\n\n#\n\n\nStatsBase.df\n \n \nMethod\n.\n\n\n\n\ndf(m)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n\n\n\n\nReturns:  Number of parameters in the model.\n\n\ndf(obj::StatisticalModel)\n\n\nReturns the number of degrees of freedom consumed in the model, including when applicable the intercept and the distribution's dispersion parameter.\n\n\n#\n\n\nStatsBase.deviance\n \n \nMethod\n.\n\n\n\n\ndeviance(obj::StatisticalModel)\n\n\nReturns the deviance of the model relative to a reference, which is usually when applicable the saturated model. It is equal, \nup to a constant\n, to \n-2 log L\n, with \nL\n the likelihood of the model.\n\n\n#\n\n\nStatsBase.fit!\n \n \nMethod\n.\n\n\n\n\nfit!(m[, verbose = false]; optimizer = :LN_BOBYQA)\n\n\n\n\n\nOptimize the objective of a \nLinearMixedModel\n using an \nNLopt\n optimizer.\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n\n\nverbose\n: \nBool\n indicating if information on iterations should be printed, Defaults to \nfalse\n\n\n\n\nNamed Args:\n\n\n\n\noptimizer\n: \nSymbol\n form of the name of a derivative-free optimizer in \nNLopt\n that allows for   box constraints.  Defaults to \n:LN_BOBYQA\n\n\n\n\n#\n\n\nStatsBase.fitted\n \n \nMethod\n.\n\n\n\n\nnothing\n\n#\n\n\nMixedModels.fixef\n \n \nFunction\n.\n\n\n\n\nfixef(m)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n\n\n\n\nReturns:   A \nVector\n of estimates of the fixed-effects parameters of \nm\n\n\n#\n\n\nMixedModels.LaplaceDeviance\n \n \nFunction\n.\n\n\n\n\nLaplaceDeviance(m)\n\n\n\n\n\nLaplace approximation to the deviance of a GLMM.  For a distribution that does not have a scale factor this is defined as the squared length of the conditional modes, \nu\n, plus the determinant of \n\u039b'Z'Z\u039b + 1\n, plus the sum of the squared deviance residuals.\n\n\nArgs:\n\n\n\n\nm\n: a \nGeneralizedLinearMixedModel\n\n\n\n\nReturns:   the Laplace approximation to the deviance of \nm\n\n\n#\n\n\nMixedModels.lmm\n \n \nFunction\n.\n\n\n\n\nlmm(m::MixedModel)\n\n\n\n\n\nExtract the \nLinearMixedModel\n from a \nMixedModel\n.  If \nm\n is itself a \nLinearMixedModel\n this simply returns \nm\n. If \nm\n is a \nGeneralizedLinearMixedModel\n this returns its \nLMM\n member.\n\n\nArgs:\n\n\n\n\nm\n: a \nMixedModel\n\n\n\n\nReturns:   A \nLinearMixedModel\n, either \nm\n itself or the \nLMM\n member of \nm\n\n\nlmm(form, frm)\nlmm(form, frm; weights = wts)\n\n\n\n\n\nArgs:\n\n\n\n\nform\n: a \nDataFrames:Formula\n containing fixed-effects and random-effects terms\n\n\nfrm\n: a \nDataFrame\n in which to evaluate \nform\n\n\nweights\n: an optional vector of prior weights in the model.  Defaults to unit weights.\n\n\n\n\nReturns:   A \nLinearMixedModel\n.\n\n\nNotes:   The return value is ready to be \nfit!\n but has not yet been fit.\n\n\n#\n\n\nMixedModels.lowerbd\n \n \nFunction\n.\n\n\n\n\nlowerbd(m)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nMixedModel\n\n\n\n\nReturns:   A \nVector\n of lower bounds on the covariance parameter vector \nm[:\u03b8]\n\n\nlower bounds on the parameters (elements in the lower triangle)\n\n\n#\n\n\nStatsBase.model_response\n \n \nMethod\n.\n\n\n\n\nextract the response (as a reference)\n\n\n#\n\n\nStatsBase.nobs\n \n \nMethod\n.\n\n\n\n\nnobs(obj::StatisticalModel)\n\n\nReturns the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.\n\n\n#\n\n\nMixedModels.objective\n \n \nFunction\n.\n\n\n\n\nobjective(m)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n object\n\n\n\n\nReturns:   Negative twice the log-likelihood of model \nm\n\n\n#\n\n\nMixedModels.pwrss\n \n \nFunction\n.\n\n\n\n\npwrss(m::LinearMixedModel)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n\n\n\n\nReturns:   The penalized residual sum-of-squares, a scalar.\n\n\n#\n\n\nMixedModels.pirls!\n \n \nFunction\n.\n\n\n\n\npirls!(m)\n\n\n\n\n\nUse Penalized Iteratively Reweighted Least Squares (PIRLS) to determine the conditional modes of the random effects\n\n\nArgs:\n\n\n\n\nm\n: a \nGeneralizedLinearMixedModel\n\n\n\n\nReturns:   the updated model \nm\n\n\nNote:   On entry the values of \nm.u\u2080\n and \nm.devold\n should correspond.   One safe approach is to zero out \nm.u\u2080\n and evaluate devold from fixed-effects only.\n\n\n#\n\n\nMixedModels.ranef\n \n \nFunction\n.\n\n\n\n\nranef(m)\nranef(m, uscale)\n\n\n\n\n\nConditional modes of the random effects in model \nm\n\n\nArgs:\n\n\n\n\nm\n: a fitted \nMixedModel\n object\n\n\nuscale\n: a \nBool\n indicating conditional modes are on the \nu\n scale or the \nb\n scale.  Defaults to \nfalse\n\n\n\n\nReturns:   A \nVector\n of matrices of the conditional modes of the random effects on the indicated scale.   For a scalar random-effects term the matrix is \n1 \u00d7 k\n where \nk\n is the number of levels of the grouping factor.   For a vector-valued random-effects term the matrix is \nl \u00d7 k\n where \nl\n is the dimension of each random effect.\n\n\n#\n\n\nMixedModels.refit!\n \n \nFunction\n.\n\n\n\n\nrefit!(m, y)\n\n\n\n\n\nRefit the model \nm\n with response \ny\n\n\nArgs:\n\n\n\n\nm\n: a \nMixedModel{T}\n\n\ny\n: a \nVector{T}\n of length \nn\n, the number of observations in \nm\n\n\n\n\nReturns:   \nm\n after refitting\n\n\n#\n\n\nMixedModels.remat\n \n \nFunction\n.\n\n\n\n\n remat(e, df)\n\n\n\n\n\nA factory for \nReMat\n objects\n\n\nArgs:\n\n\n\n\ne\n: an \nExpr\n which should be of the form \n:(e1 | e2)\n where \ne1\n is a valid rhs of a \nFormula\n and \npool(e2)\n can be evaluated.\n\n\ndf\n: a \nDataFrame\n in which to evaluate \ne1\n and \ne2\n\n\n\n\nReturns:   a \nScalarReMat\n or a \nVectorReMat\n, as appropriate.\n\n\n#\n\n\nMixedModels.sdest\n \n \nFunction\n.\n\n\n\n\nsdest(m)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n object\n\n\n\n\nReturns:   The scalar, \ns\n, the estimate of \u03c3, the standard deviation of the per-observation noise\n\n\n#\n\n\nMixedModels.simulate!\n \n \nFunction\n.\n\n\n\n\nsimulate!(m; \u03b2, \u03c3, \u03b8)\n\n\n\n\n\nSimulate a response vector from model \nm\n, and refit \nm\n.\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n.\n\n\n\u03b2\n: the fixed-effects parameter vector to use; defaults to \ncoef(m)\n\n\n\u03c3\n: the standard deviation of the per-observation random noise term to use; defaults to \nsdest(m)\n\n\n\u03b8\n: the covariance parameter vector to use; defaults to \nm[:\u03b8]\n\n\n\n\nReturns:   \nm\n after having refit it to the simulated response vector\n\n\n#\n\n\nMixedModels.varest\n \n \nFunction\n.\n\n\n\n\nvarest(m::LinearMixedModel)\n\n\n\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n\n\n\n\nReturns:  The scalar, s\u00b2, the estimate of \u03c3\u00b2, the variance of the conditional distribution of Y given B\n\n\n#\n\n\nStatsBase.vcov\n \n \nFunction\n.\n\n\n\n\n vcov(m)\n\n\n\n\n\nEstimated covariance matrix of the fixed-effects estimator\n\n\nArgs:\n\n\n\n\nm\n: a \nLinearMixedModel\n\n\n\n\nReturns   a \np \u00d7 p\n \nMatrix", 
            "title": "Public"
        }, 
        {
            "location": "/lib/public/#public-documentation", 
            "text": "Documentation for  MixedModels.jl 's public interface.  See  Internal Documentation  for internal package docs covering all submodules.", 
            "title": "Public Documentation"
        }, 
        {
            "location": "/lib/public/#contents", 
            "text": "Public Documentation  Contents  Index  MixedModels", 
            "title": "Contents"
        }, 
        {
            "location": "/lib/public/#index", 
            "text": "Base.LinAlg.cond  MixedModels.LaplaceDeviance  MixedModels.LinearMixedModel  MixedModels.ScalarReMat  MixedModels.VarCorr  MixedModels.VectorReMat  MixedModels.bootstrap  MixedModels.fixef  MixedModels.lmm  MixedModels.lowerbd  MixedModels.objective  MixedModels.pirls!  MixedModels.pwrss  MixedModels.ranef  MixedModels.refit!  MixedModels.remat  MixedModels.sdest  MixedModels.simulate!  MixedModels.varest  StatsBase.coef  StatsBase.coeftable  StatsBase.deviance  StatsBase.df  StatsBase.fit!  StatsBase.fitted  StatsBase.model_response  StatsBase.nobs  StatsBase.vcov", 
            "title": "Index"
        }, 
        {
            "location": "/lib/public/#mixedmodels", 
            "text": "#  MixedModels.LinearMixedModel     Type .   LinearMixedModel  Linear mixed-effects model representation  Members:   mf : the model frame, mostly used to get the  terms  component for labelling fixed effects  wttrms : a length  nt  vector of weighted model matrices. The last two elements are  X  and  y .  trms : a vector of unweighted model matrices.  If  isempty(sqrtwts)  the same object as  wttrms  \u039b : a length  nt - 2  vector of lower triangular matrices  sqrtwts : the  Diagonal  matrix of the square roots of the case weights.  Allowed to be size 0  A : an  nt \u00d7 nt  symmetric matrix of matrices representing  hcat(Z,X,y)'hcat(Z,X,y)  R : a  nt \u00d7 nt  matrix of matrices - the upper Cholesky factor of  \u039b'A\u039b+I  opt : an  OptSummary  object   #  MixedModels.ScalarReMat     Type .   ScalarReMat  The representation of the model matrix for a scalar random-effects term  Members:   f : the grouping factor as a  PooledDataVector  z : the raw random-effects model matrix as a  Vector  fnm : the name of the grouping factor as a  Symbol  cnms : a  Vector  of column names   #  MixedModels.VarCorr     Type .   VarCorr  An encapsulation of information on the fitted random-effects variance-covariance matrices.  Members:   \u039b : the vector of lower triangular matrices from the  MixedModel  fnms : a  Vector{ASCIIString}  of grouping factor names  cnms : a  Vector{Vector{ASCIIString}}  of column names  s : the estimate of \u03c3, the standard deviation of the per-observation noise   The main purpose is to isolate the logic in the show method.  #  MixedModels.VectorReMat     Type .   VectorReMat  The representation of the model matrix for a vector-valued random-effects term  Members:   f : the grouping factor as a  PooledDataVector  z : the transposed raw random-effects model matrix  fnm : the name of the grouping factor as a  Symbol  cnms : a  Vector  of column names (row names after transposition) of  z   #  MixedModels.bootstrap     Function .   bootstrap(m, N, saveresults)  Simulate  N  response vectors from  m , refitting the model.  The function saveresults is called after each refit.  To save space  m.trms[end] , which is the response vector, is overwritten by each simulation.  The original response is restored before returning.  Args:   m : a  LinearMixedModel  that has been fit.  N : the number of bootstrap samples to simulate  savresults : a function with arguments  i  and  m  called after each bootstrap simulation.    As the name indicates, this function should save the results of interest.   #  StatsBase.coef     Method .   nothing #  StatsBase.coeftable     Method .   nothing #  Base.LinAlg.cond     Method .   cond(m::MixedModel)  Args:   m : a  MixedModel   Returns:   A  Vector  of the condition numbers of the blocks of  m.\u039b  #  StatsBase.df     Method .   df(m)  Args:   m : a  LinearMixedModel   Returns:  Number of parameters in the model.  df(obj::StatisticalModel)  Returns the number of degrees of freedom consumed in the model, including when applicable the intercept and the distribution's dispersion parameter.  #  StatsBase.deviance     Method .   deviance(obj::StatisticalModel)  Returns the deviance of the model relative to a reference, which is usually when applicable the saturated model. It is equal,  up to a constant , to  -2 log L , with  L  the likelihood of the model.  #  StatsBase.fit!     Method .   fit!(m[, verbose = false]; optimizer = :LN_BOBYQA)  Optimize the objective of a  LinearMixedModel  using an  NLopt  optimizer.  Args:   m : a  LinearMixedModel  verbose :  Bool  indicating if information on iterations should be printed, Defaults to  false   Named Args:   optimizer :  Symbol  form of the name of a derivative-free optimizer in  NLopt  that allows for   box constraints.  Defaults to  :LN_BOBYQA   #  StatsBase.fitted     Method .   nothing #  MixedModels.fixef     Function .   fixef(m)  Args:   m : a  LinearMixedModel   Returns:   A  Vector  of estimates of the fixed-effects parameters of  m  #  MixedModels.LaplaceDeviance     Function .   LaplaceDeviance(m)  Laplace approximation to the deviance of a GLMM.  For a distribution that does not have a scale factor this is defined as the squared length of the conditional modes,  u , plus the determinant of  \u039b'Z'Z\u039b + 1 , plus the sum of the squared deviance residuals.  Args:   m : a  GeneralizedLinearMixedModel   Returns:   the Laplace approximation to the deviance of  m  #  MixedModels.lmm     Function .   lmm(m::MixedModel)  Extract the  LinearMixedModel  from a  MixedModel .  If  m  is itself a  LinearMixedModel  this simply returns  m . If  m  is a  GeneralizedLinearMixedModel  this returns its  LMM  member.  Args:   m : a  MixedModel   Returns:   A  LinearMixedModel , either  m  itself or the  LMM  member of  m  lmm(form, frm)\nlmm(form, frm; weights = wts)  Args:   form : a  DataFrames:Formula  containing fixed-effects and random-effects terms  frm : a  DataFrame  in which to evaluate  form  weights : an optional vector of prior weights in the model.  Defaults to unit weights.   Returns:   A  LinearMixedModel .  Notes:   The return value is ready to be  fit!  but has not yet been fit.  #  MixedModels.lowerbd     Function .   lowerbd(m)  Args:   m : a  MixedModel   Returns:   A  Vector  of lower bounds on the covariance parameter vector  m[:\u03b8]  lower bounds on the parameters (elements in the lower triangle)  #  StatsBase.model_response     Method .   extract the response (as a reference)  #  StatsBase.nobs     Method .   nobs(obj::StatisticalModel)  Returns the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.  #  MixedModels.objective     Function .   objective(m)  Args:   m : a  LinearMixedModel  object   Returns:   Negative twice the log-likelihood of model  m  #  MixedModels.pwrss     Function .   pwrss(m::LinearMixedModel)  Args:   m : a  LinearMixedModel   Returns:   The penalized residual sum-of-squares, a scalar.  #  MixedModels.pirls!     Function .   pirls!(m)  Use Penalized Iteratively Reweighted Least Squares (PIRLS) to determine the conditional modes of the random effects  Args:   m : a  GeneralizedLinearMixedModel   Returns:   the updated model  m  Note:   On entry the values of  m.u\u2080  and  m.devold  should correspond.   One safe approach is to zero out  m.u\u2080  and evaluate devold from fixed-effects only.  #  MixedModels.ranef     Function .   ranef(m)\nranef(m, uscale)  Conditional modes of the random effects in model  m  Args:   m : a fitted  MixedModel  object  uscale : a  Bool  indicating conditional modes are on the  u  scale or the  b  scale.  Defaults to  false   Returns:   A  Vector  of matrices of the conditional modes of the random effects on the indicated scale.   For a scalar random-effects term the matrix is  1 \u00d7 k  where  k  is the number of levels of the grouping factor.   For a vector-valued random-effects term the matrix is  l \u00d7 k  where  l  is the dimension of each random effect.  #  MixedModels.refit!     Function .   refit!(m, y)  Refit the model  m  with response  y  Args:   m : a  MixedModel{T}  y : a  Vector{T}  of length  n , the number of observations in  m   Returns:    m  after refitting  #  MixedModels.remat     Function .    remat(e, df)  A factory for  ReMat  objects  Args:   e : an  Expr  which should be of the form  :(e1 | e2)  where  e1  is a valid rhs of a  Formula  and  pool(e2)  can be evaluated.  df : a  DataFrame  in which to evaluate  e1  and  e2   Returns:   a  ScalarReMat  or a  VectorReMat , as appropriate.  #  MixedModels.sdest     Function .   sdest(m)  Args:   m : a  LinearMixedModel  object   Returns:   The scalar,  s , the estimate of \u03c3, the standard deviation of the per-observation noise  #  MixedModels.simulate!     Function .   simulate!(m; \u03b2, \u03c3, \u03b8)  Simulate a response vector from model  m , and refit  m .  Args:   m : a  LinearMixedModel .  \u03b2 : the fixed-effects parameter vector to use; defaults to  coef(m)  \u03c3 : the standard deviation of the per-observation random noise term to use; defaults to  sdest(m)  \u03b8 : the covariance parameter vector to use; defaults to  m[:\u03b8]   Returns:    m  after having refit it to the simulated response vector  #  MixedModels.varest     Function .   varest(m::LinearMixedModel)  Args:   m : a  LinearMixedModel   Returns:  The scalar, s\u00b2, the estimate of \u03c3\u00b2, the variance of the conditional distribution of Y given B  #  StatsBase.vcov     Function .    vcov(m)  Estimated covariance matrix of the fixed-effects estimator  Args:   m : a  LinearMixedModel   Returns   a  p \u00d7 p   Matrix", 
            "title": "MixedModels"
        }, 
        {
            "location": "/lib/internals/", 
            "text": "Internal Documentation\n\n\n\n\nContents\n\n\n\n\nInternal Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nFunctions and methods\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nMixedModels.OptSummary\n\n\nMixedModels.ranef!\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nMixedModels.OptSummary\n \n \nType\n.\n\n\n\n\nOptSummary\n\n\n\n\n\nSummary of an \nNLopt\n optimization\n\n\nMembers:\n\n\n\n\ninitial\n: a copy of the initial parameter values in the optimization\n\n\nfinal\n: a copy of the final parameter values from the optimization\n\n\nfmin\n: the final value of the objective\n\n\nfeval\n: the number of function evaluations\n\n\noptimizer\n: the name of the optimizer used, as a \nSymbol\n\n\n\n\n\n\nFunctions and methods\n\n\n#\n\n\nMixedModels.ranef!\n \n \nFunction\n.\n\n\n\n\nranef!(v, m, uscale)\n\n\n\n\n\nOverwrite v with the conditional modes of the random effects for \nm\n\n\nArgs:\n\n\n\n\nv\n: a \nVector\n of matrices\n\n\nm\n: a \nMixedModel\n\n\nuscale\n: \nBool\n, return the random-effects on the spherical (i.e. \nu\n) scale?\n\n\n\n\nReturns:   \nv\n, overwritten with the conditional modes", 
            "title": "Internals"
        }, 
        {
            "location": "/lib/internals/#internal-documentation", 
            "text": "", 
            "title": "Internal Documentation"
        }, 
        {
            "location": "/lib/internals/#contents", 
            "text": "Internal Documentation  Contents  Index  Types  Functions and methods", 
            "title": "Contents"
        }, 
        {
            "location": "/lib/internals/#index", 
            "text": "MixedModels.OptSummary  MixedModels.ranef!", 
            "title": "Index"
        }, 
        {
            "location": "/lib/internals/#types", 
            "text": "#  MixedModels.OptSummary     Type .   OptSummary  Summary of an  NLopt  optimization  Members:   initial : a copy of the initial parameter values in the optimization  final : a copy of the final parameter values from the optimization  fmin : the final value of the objective  feval : the number of function evaluations  optimizer : the name of the optimizer used, as a  Symbol", 
            "title": "Types"
        }, 
        {
            "location": "/lib/internals/#functions-and-methods", 
            "text": "#  MixedModels.ranef!     Function .   ranef!(v, m, uscale)  Overwrite v with the conditional modes of the random effects for  m  Args:   v : a  Vector  of matrices  m : a  MixedModel  uscale :  Bool , return the random-effects on the spherical (i.e.  u ) scale?   Returns:    v , overwritten with the conditional modes", 
            "title": "Functions and methods"
        }
    ]
}